
  You can now view your Streamlit app in your browser.

  Network URL: http://140.114.89.225:8501
  External URL: http://140.114.89.225:8501

/home/nlplab/daisy/venv/lib/python3.8/site-packages/allennlp/tango/__init__.py:17: UserWarning: AllenNLP Tango is an experimental API and parts of it might change or disappear every time we release a new version.
  warnings.warn(
2021-11-12 14:31:29.553 Plugin allennlp_models available
2021-11-12 14:31:29.718 Plugin allennlp_server available
2021-11-12 14:31:29.720 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 14:31:29.720 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpj4k7p_2e
2021-11-12 14:31:31.846 dataset_reader.type = example_reader
2021-11-12 14:31:31.846 dataset_reader.max_instances = None
2021-11-12 14:31:31.846 dataset_reader.manual_distributed_sharding = False
2021-11-12 14:31:31.846 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 14:31:31.847 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:31:31.847 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:31:31.847 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 14:31:31.848 type = bert-base-cased
2021-11-12 14:31:31.848 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 14:31:31.848 dataset_reader.tokenizer.max_length = 128
2021-11-12 14:31:31.848 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 14:31:42.896 dataset_reader.text_token_indexers.type = ref
2021-11-12 14:31:42.898 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:31:42.899 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:31:42.899 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 14:31:42.900 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 14:31:42.900 type = bert-base-cased
2021-11-12 14:31:42.900 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 14:31:42.900 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 14:31:42.900 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 14:31:42.901 dataset_reader.to_index = 6
2021-11-12 14:31:42.901 dataset_reader.type = example_reader
2021-11-12 14:31:42.902 dataset_reader.max_instances = None
2021-11-12 14:31:42.902 dataset_reader.manual_distributed_sharding = False
2021-11-12 14:31:42.902 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 14:31:42.902 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:31:42.902 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:31:42.903 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 14:31:42.903 type = bert-base-cased
2021-11-12 14:31:42.903 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 14:31:42.903 dataset_reader.tokenizer.max_length = 128
2021-11-12 14:31:42.904 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 14:31:42.905 dataset_reader.text_token_indexers.type = ref
2021-11-12 14:31:42.906 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:31:42.906 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:31:42.906 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 14:31:42.906 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 14:31:42.907 type = bert-base-cased
2021-11-12 14:31:42.907 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 14:31:42.907 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 14:31:42.907 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 14:31:42.908 dataset_reader.to_index = 6
2021-11-12 14:31:42.908 type = from_instances
2021-11-12 14:31:42.908 Loading token dictionary from /tmp/tmpj4k7p_2e/vocabulary.
2021-11-12 14:31:42.909 model.type = sentence_level_classifier
2021-11-12 14:31:42.909 model.embedder.type = ref
2021-11-12 14:31:42.910 model.embedder.type = basic
2021-11-12 14:31:42.911 model.embedder.token_embedders.type = ref
2021-11-12 14:31:42.912 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 14:31:42.912 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 14:31:42.912 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 14:31:42.912 type = bert-base-cased
2021-11-12 14:31:42.913 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 14:31:42.913 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 14:31:42.913 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 14:31:42.913 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 14:31:42.913 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 14:31:42.913 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 14:31:42.913 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 14:31:42.913 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 14:31:42.913 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 14:31:42.914 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 14:31:42.914 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 14:31:46.179 model.encoder.type = bert_pooler
2021-11-12 14:31:46.179 model.encoder.type = bert_pooler
2021-11-12 14:31:46.180 model.encoder.pretrained_model = bert-base-cased
2021-11-12 14:31:46.180 type = bert-base-cased
2021-11-12 14:31:46.180 model.encoder.override_weights_file = None
2021-11-12 14:31:46.181 model.encoder.override_weights_strip_prefix = None
2021-11-12 14:31:46.181 model.encoder.load_weights = True
2021-11-12 14:31:46.181 model.encoder.requires_grad = True
2021-11-12 14:31:46.181 model.encoder.dropout = 0.0
2021-11-12 14:31:46.182 model.encoder.transformer_kwargs = None
2021-11-12 14:31:46.379 removing temporary unarchived model dir at /tmp/tmpj4k7p_2e
/home/nlplab/daisy/venv/lib/python3.8/site-packages/allennlp/models/archival.py:325: UserWarning: The model models/sent_level_bert_ce_6levels.tar.gz was trained on a newer version of AllenNLP (v2.8.0), but you're using version 2.7.0.
  warnings.warn(
2021-11-12 14:31:48.613 Plugin allennlp_models available
2021-11-12 14:31:48.622 Plugin allennlp_server available
2021-11-12 14:31:48.623 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 14:31:48.623 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpdq58a7du
2021-11-12 14:31:50.728 dataset_reader.type = example_reader
2021-11-12 14:31:50.729 dataset_reader.max_instances = None
2021-11-12 14:31:50.729 dataset_reader.manual_distributed_sharding = False
2021-11-12 14:31:50.729 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 14:31:50.729 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:31:50.729 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:31:50.729 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 14:31:50.729 type = bert-base-cased
2021-11-12 14:31:50.729 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 14:31:50.730 dataset_reader.tokenizer.max_length = 128
2021-11-12 14:31:50.730 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 14:31:50.731 dataset_reader.text_token_indexers.type = ref
2021-11-12 14:31:50.732 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:31:50.732 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:31:50.732 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 14:31:50.732 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 14:31:50.732 type = bert-base-cased
2021-11-12 14:31:50.732 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 14:31:50.732 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 14:31:50.732 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 14:31:50.733 dataset_reader.to_index = 6
2021-11-12 14:31:50.734 dataset_reader.type = example_reader
2021-11-12 14:31:50.734 dataset_reader.max_instances = None
2021-11-12 14:31:50.734 dataset_reader.manual_distributed_sharding = False
2021-11-12 14:31:50.734 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 14:31:50.734 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:31:50.734 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:31:50.734 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 14:31:50.734 type = bert-base-cased
2021-11-12 14:31:50.735 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 14:31:50.735 dataset_reader.tokenizer.max_length = 128
2021-11-12 14:31:50.735 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 14:31:50.736 dataset_reader.text_token_indexers.type = ref
2021-11-12 14:31:50.736 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:31:50.737 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:31:50.737 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 14:31:50.737 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 14:31:50.737 type = bert-base-cased
2021-11-12 14:31:50.737 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 14:31:50.737 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 14:31:50.737 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 14:31:50.738 dataset_reader.to_index = 6
2021-11-12 14:31:50.738 type = from_instances
2021-11-12 14:31:50.738 Loading token dictionary from /tmp/tmpdq58a7du/vocabulary.
2021-11-12 14:31:50.739 model.type = sentence_level_classifier
2021-11-12 14:31:50.739 model.embedder.type = ref
2021-11-12 14:31:50.740 model.embedder.type = basic
2021-11-12 14:31:50.740 model.embedder.token_embedders.type = ref
2021-11-12 14:31:50.741 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 14:31:50.741 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 14:31:50.741 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 14:31:50.741 type = bert-base-cased
2021-11-12 14:31:50.741 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 14:31:50.741 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 14:31:50.742 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 14:31:50.742 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 14:31:50.742 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 14:31:50.742 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 14:31:50.742 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 14:31:50.742 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 14:31:50.742 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 14:31:50.742 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 14:31:50.742 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 14:31:50.791 model.encoder.type = bert_pooler
2021-11-12 14:31:50.791 model.encoder.type = bert_pooler
2021-11-12 14:31:50.791 model.encoder.pretrained_model = bert-base-cased
2021-11-12 14:31:50.791 type = bert-base-cased
2021-11-12 14:31:50.792 model.encoder.override_weights_file = None
2021-11-12 14:31:50.792 model.encoder.override_weights_strip_prefix = None
2021-11-12 14:31:50.792 model.encoder.load_weights = True
2021-11-12 14:31:50.792 model.encoder.requires_grad = True
2021-11-12 14:31:50.792 model.encoder.dropout = 0.0
2021-11-12 14:31:50.792 model.encoder.transformer_kwargs = None
2021-11-12 14:31:50.984 removing temporary unarchived model dir at /tmp/tmpdq58a7du
2021-11-12 14:32:19.173 Plugin allennlp_models available
2021-11-12 14:32:19.181 Plugin allennlp_server available
2021-11-12 14:32:19.182 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 14:32:19.182 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpot0e_41f
2021-11-12 14:32:21.283 dataset_reader.type = example_reader
2021-11-12 14:32:21.283 dataset_reader.max_instances = None
2021-11-12 14:32:21.284 dataset_reader.manual_distributed_sharding = False
2021-11-12 14:32:21.284 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 14:32:21.284 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:32:21.285 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:32:21.285 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 14:32:21.285 type = bert-base-cased
2021-11-12 14:32:21.285 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 14:32:21.285 dataset_reader.tokenizer.max_length = 128
2021-11-12 14:32:21.285 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 14:32:21.286 dataset_reader.text_token_indexers.type = ref
2021-11-12 14:32:21.287 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:32:21.287 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:32:21.287 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 14:32:21.288 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 14:32:21.288 type = bert-base-cased
2021-11-12 14:32:21.288 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 14:32:21.288 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 14:32:21.288 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 14:32:21.289 dataset_reader.to_index = 6
2021-11-12 14:32:21.289 dataset_reader.type = example_reader
2021-11-12 14:32:21.289 dataset_reader.max_instances = None
2021-11-12 14:32:21.289 dataset_reader.manual_distributed_sharding = False
2021-11-12 14:32:21.290 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 14:32:21.290 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:32:21.290 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:32:21.290 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 14:32:21.290 type = bert-base-cased
2021-11-12 14:32:21.290 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 14:32:21.290 dataset_reader.tokenizer.max_length = 128
2021-11-12 14:32:21.290 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 14:32:21.291 dataset_reader.text_token_indexers.type = ref
2021-11-12 14:32:21.292 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:32:21.292 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:32:21.292 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 14:32:21.292 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 14:32:21.292 type = bert-base-cased
2021-11-12 14:32:21.292 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 14:32:21.293 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 14:32:21.293 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 14:32:21.293 dataset_reader.to_index = 6
2021-11-12 14:32:21.293 type = from_instances
2021-11-12 14:32:21.294 Loading token dictionary from /tmp/tmpot0e_41f/vocabulary.
2021-11-12 14:32:21.294 model.type = sentence_level_classifier
2021-11-12 14:32:21.294 model.embedder.type = ref
2021-11-12 14:32:21.296 model.embedder.type = basic
2021-11-12 14:32:21.296 model.embedder.token_embedders.type = ref
2021-11-12 14:32:21.297 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 14:32:21.297 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 14:32:21.297 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 14:32:21.297 type = bert-base-cased
2021-11-12 14:32:21.297 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 14:32:21.298 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 14:32:21.298 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 14:32:21.298 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 14:32:21.298 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 14:32:21.298 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 14:32:21.298 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 14:32:21.298 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 14:32:21.298 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 14:32:21.298 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 14:32:21.298 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 14:32:21.348 model.encoder.type = bert_pooler
2021-11-12 14:32:21.348 model.encoder.type = bert_pooler
2021-11-12 14:32:21.349 model.encoder.pretrained_model = bert-base-cased
2021-11-12 14:32:21.349 type = bert-base-cased
2021-11-12 14:32:21.349 model.encoder.override_weights_file = None
2021-11-12 14:32:21.349 model.encoder.override_weights_strip_prefix = None
2021-11-12 14:32:21.349 model.encoder.load_weights = True
2021-11-12 14:32:21.349 model.encoder.requires_grad = True
2021-11-12 14:32:21.349 model.encoder.dropout = 0.0
2021-11-12 14:32:21.349 model.encoder.transformer_kwargs = None
2021-11-12 14:32:21.539 removing temporary unarchived model dir at /tmp/tmpot0e_41f
2021-11-12 14:32:44.050 Plugin allennlp_models available
2021-11-12 14:32:44.059 Plugin allennlp_server available
2021-11-12 14:32:44.060 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 14:32:44.060 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpldel5j9g
2021-11-12 14:32:46.163 dataset_reader.type = example_reader
2021-11-12 14:32:46.164 dataset_reader.max_instances = None
2021-11-12 14:32:46.164 dataset_reader.manual_distributed_sharding = False
2021-11-12 14:32:46.164 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 14:32:46.164 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:32:46.164 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:32:46.165 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 14:32:46.166 type = bert-base-cased
2021-11-12 14:32:46.166 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 14:32:46.166 dataset_reader.tokenizer.max_length = 128
2021-11-12 14:32:46.167 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 14:32:46.169 dataset_reader.text_token_indexers.type = ref
2021-11-12 14:32:46.171 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:32:46.171 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:32:46.172 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 14:32:46.172 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 14:32:46.173 type = bert-base-cased
2021-11-12 14:32:46.173 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 14:32:46.173 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 14:32:46.174 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 14:32:46.175 dataset_reader.to_index = 6
2021-11-12 14:32:46.176 dataset_reader.type = example_reader
2021-11-12 14:32:46.176 dataset_reader.max_instances = None
2021-11-12 14:32:46.176 dataset_reader.manual_distributed_sharding = False
2021-11-12 14:32:46.177 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 14:32:46.177 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:32:46.178 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:32:46.178 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 14:32:46.178 type = bert-base-cased
2021-11-12 14:32:46.179 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 14:32:46.179 dataset_reader.tokenizer.max_length = 128
2021-11-12 14:32:46.179 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 14:32:46.181 dataset_reader.text_token_indexers.type = ref
2021-11-12 14:32:46.183 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:32:46.183 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:32:46.183 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 14:32:46.184 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 14:32:46.184 type = bert-base-cased
2021-11-12 14:32:46.184 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 14:32:46.185 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 14:32:46.185 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 14:32:46.187 dataset_reader.to_index = 6
2021-11-12 14:32:46.187 type = from_instances
2021-11-12 14:32:46.187 Loading token dictionary from /tmp/tmpldel5j9g/vocabulary.
2021-11-12 14:32:46.188 model.type = sentence_level_classifier
2021-11-12 14:32:46.189 model.embedder.type = ref
2021-11-12 14:32:46.190 model.embedder.type = basic
2021-11-12 14:32:46.191 model.embedder.token_embedders.type = ref
2021-11-12 14:32:46.194 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 14:32:46.194 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 14:32:46.194 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 14:32:46.195 type = bert-base-cased
2021-11-12 14:32:46.195 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 14:32:46.195 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 14:32:46.195 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 14:32:46.196 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 14:32:46.196 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 14:32:46.196 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 14:32:46.196 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 14:32:46.196 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 14:32:46.197 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 14:32:46.197 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 14:32:46.197 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 14:32:46.255 model.encoder.type = bert_pooler
2021-11-12 14:32:46.255 model.encoder.type = bert_pooler
2021-11-12 14:32:46.256 model.encoder.pretrained_model = bert-base-cased
2021-11-12 14:32:46.256 type = bert-base-cased
2021-11-12 14:32:46.256 model.encoder.override_weights_file = None
2021-11-12 14:32:46.256 model.encoder.override_weights_strip_prefix = None
2021-11-12 14:32:46.256 model.encoder.load_weights = True
2021-11-12 14:32:46.256 model.encoder.requires_grad = True
2021-11-12 14:32:46.256 model.encoder.dropout = 0.0
2021-11-12 14:32:46.256 model.encoder.transformer_kwargs = None
2021-11-12 14:32:46.445 removing temporary unarchived model dir at /tmp/tmpldel5j9g
2021-11-12 14:32:59.466 Plugin allennlp_models available
2021-11-12 14:32:59.476 Plugin allennlp_server available
2021-11-12 14:32:59.477 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 14:32:59.477 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmprk8inupp
2021-11-12 14:33:01.610 dataset_reader.type = example_reader
2021-11-12 14:33:01.611 dataset_reader.max_instances = None
2021-11-12 14:33:01.611 dataset_reader.manual_distributed_sharding = False
2021-11-12 14:33:01.612 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 14:33:01.612 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:33:01.613 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:33:01.613 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 14:33:01.614 type = bert-base-cased
2021-11-12 14:33:01.614 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 14:33:01.614 dataset_reader.tokenizer.max_length = 128
2021-11-12 14:33:01.614 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 14:33:01.616 dataset_reader.text_token_indexers.type = ref
2021-11-12 14:33:01.619 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:33:01.619 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:33:01.620 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 14:33:01.620 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 14:33:01.620 type = bert-base-cased
2021-11-12 14:33:01.621 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 14:33:01.621 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 14:33:01.621 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 14:33:01.622 dataset_reader.to_index = 6
2021-11-12 14:33:01.623 dataset_reader.type = example_reader
2021-11-12 14:33:01.623 dataset_reader.max_instances = None
2021-11-12 14:33:01.624 dataset_reader.manual_distributed_sharding = False
2021-11-12 14:33:01.624 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 14:33:01.624 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:33:01.625 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:33:01.625 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 14:33:01.625 type = bert-base-cased
2021-11-12 14:33:01.626 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 14:33:01.626 dataset_reader.tokenizer.max_length = 128
2021-11-12 14:33:01.626 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 14:33:01.628 dataset_reader.text_token_indexers.type = ref
2021-11-12 14:33:01.630 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:33:01.630 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:33:01.630 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 14:33:01.631 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 14:33:01.631 type = bert-base-cased
2021-11-12 14:33:01.631 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 14:33:01.631 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 14:33:01.632 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 14:33:01.633 dataset_reader.to_index = 6
2021-11-12 14:33:01.633 type = from_instances
2021-11-12 14:33:01.634 Loading token dictionary from /tmp/tmprk8inupp/vocabulary.
2021-11-12 14:33:01.634 model.type = sentence_level_classifier
2021-11-12 14:33:01.635 model.embedder.type = ref
2021-11-12 14:33:01.636 model.embedder.type = basic
2021-11-12 14:33:01.637 model.embedder.token_embedders.type = ref
2021-11-12 14:33:01.640 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 14:33:01.640 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 14:33:01.640 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 14:33:01.641 type = bert-base-cased
2021-11-12 14:33:01.641 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 14:33:01.641 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 14:33:01.641 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 14:33:01.641 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 14:33:01.642 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 14:33:01.642 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 14:33:01.642 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 14:33:01.642 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 14:33:01.643 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 14:33:01.643 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 14:33:01.643 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 14:33:01.695 model.encoder.type = bert_pooler
2021-11-12 14:33:01.695 model.encoder.type = bert_pooler
2021-11-12 14:33:01.695 model.encoder.pretrained_model = bert-base-cased
2021-11-12 14:33:01.696 type = bert-base-cased
2021-11-12 14:33:01.696 model.encoder.override_weights_file = None
2021-11-12 14:33:01.696 model.encoder.override_weights_strip_prefix = None
2021-11-12 14:33:01.696 model.encoder.load_weights = True
2021-11-12 14:33:01.696 model.encoder.requires_grad = True
2021-11-12 14:33:01.696 model.encoder.dropout = 0.0
2021-11-12 14:33:01.696 model.encoder.transformer_kwargs = None
2021-11-12 14:33:01.890 removing temporary unarchived model dir at /tmp/tmprk8inupp
2021-11-12 14:33:32.374 Plugin allennlp_models available
2021-11-12 14:33:32.382 Plugin allennlp_server available
2021-11-12 14:33:32.383 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 14:33:32.383 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpbayme48p
2021-11-12 14:33:34.484 dataset_reader.type = example_reader
2021-11-12 14:33:34.484 dataset_reader.max_instances = None
2021-11-12 14:33:34.484 dataset_reader.manual_distributed_sharding = False
2021-11-12 14:33:34.484 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 14:33:34.485 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:33:34.485 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:33:34.485 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 14:33:34.486 type = bert-base-cased
2021-11-12 14:33:34.486 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 14:33:34.486 dataset_reader.tokenizer.max_length = 128
2021-11-12 14:33:34.486 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 14:33:34.487 dataset_reader.text_token_indexers.type = ref
2021-11-12 14:33:34.488 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:33:34.488 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:33:34.489 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 14:33:34.489 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 14:33:34.490 type = bert-base-cased
2021-11-12 14:33:34.490 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 14:33:34.490 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 14:33:34.490 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 14:33:34.492 dataset_reader.to_index = 6
2021-11-12 14:33:34.492 dataset_reader.type = example_reader
2021-11-12 14:33:34.493 dataset_reader.max_instances = None
2021-11-12 14:33:34.493 dataset_reader.manual_distributed_sharding = False
2021-11-12 14:33:34.493 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 14:33:34.494 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:33:34.494 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:33:34.495 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 14:33:34.495 type = bert-base-cased
2021-11-12 14:33:34.495 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 14:33:34.496 dataset_reader.tokenizer.max_length = 128
2021-11-12 14:33:34.496 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 14:33:34.498 dataset_reader.text_token_indexers.type = ref
2021-11-12 14:33:34.500 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:33:34.500 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:33:34.501 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 14:33:34.501 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 14:33:34.501 type = bert-base-cased
2021-11-12 14:33:34.502 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 14:33:34.502 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 14:33:34.502 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 14:33:34.503 dataset_reader.to_index = 6
2021-11-12 14:33:34.504 type = from_instances
2021-11-12 14:33:34.504 Loading token dictionary from /tmp/tmpbayme48p/vocabulary.
2021-11-12 14:33:34.505 model.type = sentence_level_classifier
2021-11-12 14:33:34.506 model.embedder.type = ref
2021-11-12 14:33:34.507 model.embedder.type = basic
2021-11-12 14:33:34.508 model.embedder.token_embedders.type = ref
2021-11-12 14:33:34.511 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 14:33:34.511 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 14:33:34.512 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 14:33:34.512 type = bert-base-cased
2021-11-12 14:33:34.512 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 14:33:34.513 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 14:33:34.513 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 14:33:34.513 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 14:33:34.513 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 14:33:34.513 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 14:33:34.514 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 14:33:34.514 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 14:33:34.514 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 14:33:34.514 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 14:33:34.514 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 14:33:34.563 model.encoder.type = bert_pooler
2021-11-12 14:33:34.563 model.encoder.type = bert_pooler
2021-11-12 14:33:34.564 model.encoder.pretrained_model = bert-base-cased
2021-11-12 14:33:34.564 type = bert-base-cased
2021-11-12 14:33:34.564 model.encoder.override_weights_file = None
2021-11-12 14:33:34.564 model.encoder.override_weights_strip_prefix = None
2021-11-12 14:33:34.564 model.encoder.load_weights = True
2021-11-12 14:33:34.564 model.encoder.requires_grad = True
2021-11-12 14:33:34.565 model.encoder.dropout = 0.0
2021-11-12 14:33:34.565 model.encoder.transformer_kwargs = None
2021-11-12 14:33:34.754 removing temporary unarchived model dir at /tmp/tmpbayme48p
2021-11-12 14:33:46.554 Plugin allennlp_models available
2021-11-12 14:33:46.559 Plugin allennlp_server available
2021-11-12 14:33:46.560 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 14:33:46.560 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpac8e4fje
2021-11-12 14:33:48.656 dataset_reader.type = example_reader
2021-11-12 14:33:48.657 dataset_reader.max_instances = None
2021-11-12 14:33:48.657 dataset_reader.manual_distributed_sharding = False
2021-11-12 14:33:48.657 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 14:33:48.657 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:33:48.658 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:33:48.658 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 14:33:48.658 type = bert-base-cased
2021-11-12 14:33:48.658 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 14:33:48.658 dataset_reader.tokenizer.max_length = 128
2021-11-12 14:33:48.658 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 14:33:48.659 dataset_reader.text_token_indexers.type = ref
2021-11-12 14:33:48.660 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:33:48.660 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:33:48.661 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 14:33:48.661 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 14:33:48.661 type = bert-base-cased
2021-11-12 14:33:48.661 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 14:33:48.661 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 14:33:48.661 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 14:33:48.662 dataset_reader.to_index = 6
2021-11-12 14:33:48.662 dataset_reader.type = example_reader
2021-11-12 14:33:48.663 dataset_reader.max_instances = None
2021-11-12 14:33:48.663 dataset_reader.manual_distributed_sharding = False
2021-11-12 14:33:48.663 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 14:33:48.663 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:33:48.663 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:33:48.663 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 14:33:48.663 type = bert-base-cased
2021-11-12 14:33:48.663 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 14:33:48.663 dataset_reader.tokenizer.max_length = 128
2021-11-12 14:33:48.663 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 14:33:48.664 dataset_reader.text_token_indexers.type = ref
2021-11-12 14:33:48.665 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:33:48.665 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:33:48.665 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 14:33:48.666 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 14:33:48.666 type = bert-base-cased
2021-11-12 14:33:48.666 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 14:33:48.666 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 14:33:48.666 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 14:33:48.667 dataset_reader.to_index = 6
2021-11-12 14:33:48.667 type = from_instances
2021-11-12 14:33:48.667 Loading token dictionary from /tmp/tmpac8e4fje/vocabulary.
2021-11-12 14:33:48.668 model.type = sentence_level_classifier
2021-11-12 14:33:48.668 model.embedder.type = ref
2021-11-12 14:33:48.669 model.embedder.type = basic
2021-11-12 14:33:48.669 model.embedder.token_embedders.type = ref
2021-11-12 14:33:48.671 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 14:33:48.671 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 14:33:48.671 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 14:33:48.672 type = bert-base-cased
2021-11-12 14:33:48.672 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 14:33:48.672 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 14:33:48.672 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 14:33:48.672 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 14:33:48.672 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 14:33:48.672 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 14:33:48.672 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 14:33:48.672 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 14:33:48.672 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 14:33:48.672 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 14:33:48.672 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 14:33:48.723 model.encoder.type = bert_pooler
2021-11-12 14:33:48.723 model.encoder.type = bert_pooler
2021-11-12 14:33:48.723 model.encoder.pretrained_model = bert-base-cased
2021-11-12 14:33:48.723 type = bert-base-cased
2021-11-12 14:33:48.724 model.encoder.override_weights_file = None
2021-11-12 14:33:48.724 model.encoder.override_weights_strip_prefix = None
2021-11-12 14:33:48.724 model.encoder.load_weights = True
2021-11-12 14:33:48.724 model.encoder.requires_grad = True
2021-11-12 14:33:48.724 model.encoder.dropout = 0.0
2021-11-12 14:33:48.724 model.encoder.transformer_kwargs = None
2021-11-12 14:33:48.913 removing temporary unarchived model dir at /tmp/tmpac8e4fje
2021-11-12 14:33:56.670 Plugin allennlp_models available
2021-11-12 14:33:56.679 Plugin allennlp_server available
2021-11-12 14:33:56.680 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 14:33:56.680 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpfbcu8v51
2021-11-12 14:33:58.810 dataset_reader.type = example_reader
2021-11-12 14:33:58.810 dataset_reader.max_instances = None
2021-11-12 14:33:58.810 dataset_reader.manual_distributed_sharding = False
2021-11-12 14:33:58.810 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 14:33:58.810 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:33:58.810 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:33:58.811 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 14:33:58.811 type = bert-base-cased
2021-11-12 14:33:58.811 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 14:33:58.811 dataset_reader.tokenizer.max_length = 128
2021-11-12 14:33:58.811 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 14:33:58.812 dataset_reader.text_token_indexers.type = ref
2021-11-12 14:33:58.813 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:33:58.813 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:33:58.814 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 14:33:58.814 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 14:33:58.814 type = bert-base-cased
2021-11-12 14:33:58.814 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 14:33:58.814 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 14:33:58.814 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 14:33:58.815 dataset_reader.to_index = 6
2021-11-12 14:33:58.815 dataset_reader.type = example_reader
2021-11-12 14:33:58.815 dataset_reader.max_instances = None
2021-11-12 14:33:58.815 dataset_reader.manual_distributed_sharding = False
2021-11-12 14:33:58.815 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 14:33:58.815 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:33:58.816 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:33:58.816 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 14:33:58.816 type = bert-base-cased
2021-11-12 14:33:58.816 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 14:33:58.816 dataset_reader.tokenizer.max_length = 128
2021-11-12 14:33:58.816 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 14:33:58.817 dataset_reader.text_token_indexers.type = ref
2021-11-12 14:33:58.818 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:33:58.818 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:33:58.818 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 14:33:58.818 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 14:33:58.818 type = bert-base-cased
2021-11-12 14:33:58.818 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 14:33:58.818 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 14:33:58.818 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 14:33:58.819 dataset_reader.to_index = 6
2021-11-12 14:33:58.820 type = from_instances
2021-11-12 14:33:58.820 Loading token dictionary from /tmp/tmpfbcu8v51/vocabulary.
2021-11-12 14:33:58.820 model.type = sentence_level_classifier
2021-11-12 14:33:58.820 model.embedder.type = ref
2021-11-12 14:33:58.821 model.embedder.type = basic
2021-11-12 14:33:58.821 model.embedder.token_embedders.type = ref
2021-11-12 14:33:58.823 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 14:33:58.823 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 14:33:58.823 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 14:33:58.823 type = bert-base-cased
2021-11-12 14:33:58.823 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 14:33:58.823 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 14:33:58.823 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 14:33:58.824 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 14:33:58.824 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 14:33:58.824 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 14:33:58.824 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 14:33:58.824 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 14:33:58.824 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 14:33:58.824 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 14:33:58.824 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 14:33:58.872 model.encoder.type = bert_pooler
2021-11-12 14:33:58.873 model.encoder.type = bert_pooler
2021-11-12 14:33:58.873 model.encoder.pretrained_model = bert-base-cased
2021-11-12 14:33:58.873 type = bert-base-cased
2021-11-12 14:33:58.874 model.encoder.override_weights_file = None
2021-11-12 14:33:58.874 model.encoder.override_weights_strip_prefix = None
2021-11-12 14:33:58.874 model.encoder.load_weights = True
2021-11-12 14:33:58.874 model.encoder.requires_grad = True
2021-11-12 14:33:58.874 model.encoder.dropout = 0.0
2021-11-12 14:33:58.874 model.encoder.transformer_kwargs = None
2021-11-12 14:33:59.064 removing temporary unarchived model dir at /tmp/tmpfbcu8v51
2021-11-12 14:34:13.501 Plugin allennlp_models available
2021-11-12 14:34:13.506 Plugin allennlp_server available
2021-11-12 14:34:13.507 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 14:34:13.507 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpfzgupykb
2021-11-12 14:34:15.601 dataset_reader.type = example_reader
2021-11-12 14:34:15.602 dataset_reader.max_instances = None
2021-11-12 14:34:15.602 dataset_reader.manual_distributed_sharding = False
2021-11-12 14:34:15.602 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 14:34:15.602 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:34:15.602 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:34:15.602 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 14:34:15.603 type = bert-base-cased
2021-11-12 14:34:15.603 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 14:34:15.603 dataset_reader.tokenizer.max_length = 128
2021-11-12 14:34:15.603 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 14:34:15.604 dataset_reader.text_token_indexers.type = ref
2021-11-12 14:34:15.605 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:34:15.605 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:34:15.606 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 14:34:15.606 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 14:34:15.606 type = bert-base-cased
2021-11-12 14:34:15.606 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 14:34:15.606 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 14:34:15.606 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 14:34:15.607 dataset_reader.to_index = 6
2021-11-12 14:34:15.607 dataset_reader.type = example_reader
2021-11-12 14:34:15.607 dataset_reader.max_instances = None
2021-11-12 14:34:15.607 dataset_reader.manual_distributed_sharding = False
2021-11-12 14:34:15.608 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 14:34:15.608 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:34:15.608 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:34:15.608 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 14:34:15.608 type = bert-base-cased
2021-11-12 14:34:15.608 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 14:34:15.608 dataset_reader.tokenizer.max_length = 128
2021-11-12 14:34:15.608 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 14:34:15.609 dataset_reader.text_token_indexers.type = ref
2021-11-12 14:34:15.610 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:34:15.610 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:34:15.610 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 14:34:15.610 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 14:34:15.610 type = bert-base-cased
2021-11-12 14:34:15.610 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 14:34:15.610 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 14:34:15.610 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 14:34:15.611 dataset_reader.to_index = 6
2021-11-12 14:34:15.612 type = from_instances
2021-11-12 14:34:15.612 Loading token dictionary from /tmp/tmpfzgupykb/vocabulary.
2021-11-12 14:34:15.612 model.type = sentence_level_classifier
2021-11-12 14:34:15.613 model.embedder.type = ref
2021-11-12 14:34:15.613 model.embedder.type = basic
2021-11-12 14:34:15.614 model.embedder.token_embedders.type = ref
2021-11-12 14:34:15.615 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 14:34:15.615 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 14:34:15.616 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 14:34:15.616 type = bert-base-cased
2021-11-12 14:34:15.616 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 14:34:15.616 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 14:34:15.616 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 14:34:15.616 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 14:34:15.616 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 14:34:15.616 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 14:34:15.616 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 14:34:15.616 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 14:34:15.616 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 14:34:15.616 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 14:34:15.616 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 14:34:15.664 model.encoder.type = bert_pooler
2021-11-12 14:34:15.665 model.encoder.type = bert_pooler
2021-11-12 14:34:15.665 model.encoder.pretrained_model = bert-base-cased
2021-11-12 14:34:15.665 type = bert-base-cased
2021-11-12 14:34:15.665 model.encoder.override_weights_file = None
2021-11-12 14:34:15.665 model.encoder.override_weights_strip_prefix = None
2021-11-12 14:34:15.666 model.encoder.load_weights = True
2021-11-12 14:34:15.666 model.encoder.requires_grad = True
2021-11-12 14:34:15.666 model.encoder.dropout = 0.0
2021-11-12 14:34:15.666 model.encoder.transformer_kwargs = None
2021-11-12 14:34:15.848 removing temporary unarchived model dir at /tmp/tmpfzgupykb
2021-11-12 14:34:33.670 Plugin allennlp_models available
2021-11-12 14:34:33.676 Plugin allennlp_server available
2021-11-12 14:34:33.677 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 14:34:33.677 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmppf26i4l4
2021-11-12 14:34:35.771 dataset_reader.type = example_reader
2021-11-12 14:34:35.771 dataset_reader.max_instances = None
2021-11-12 14:34:35.771 dataset_reader.manual_distributed_sharding = False
2021-11-12 14:34:35.772 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 14:34:35.772 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:34:35.772 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:34:35.773 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 14:34:35.773 type = bert-base-cased
2021-11-12 14:34:35.773 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 14:34:35.773 dataset_reader.tokenizer.max_length = 128
2021-11-12 14:34:35.773 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 14:34:35.774 dataset_reader.text_token_indexers.type = ref
2021-11-12 14:34:35.775 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:34:35.775 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:34:35.775 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 14:34:35.776 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 14:34:35.776 type = bert-base-cased
2021-11-12 14:34:35.776 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 14:34:35.776 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 14:34:35.776 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 14:34:35.777 dataset_reader.to_index = 6
2021-11-12 14:34:35.777 dataset_reader.type = example_reader
2021-11-12 14:34:35.777 dataset_reader.max_instances = None
2021-11-12 14:34:35.777 dataset_reader.manual_distributed_sharding = False
2021-11-12 14:34:35.777 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 14:34:35.777 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:34:35.777 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:34:35.777 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 14:34:35.778 type = bert-base-cased
2021-11-12 14:34:35.778 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 14:34:35.778 dataset_reader.tokenizer.max_length = 128
2021-11-12 14:34:35.778 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 14:34:35.779 dataset_reader.text_token_indexers.type = ref
2021-11-12 14:34:35.779 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:34:35.779 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:34:35.780 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 14:34:35.780 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 14:34:35.780 type = bert-base-cased
2021-11-12 14:34:35.780 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 14:34:35.780 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 14:34:35.780 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 14:34:35.781 dataset_reader.to_index = 6
2021-11-12 14:34:35.781 type = from_instances
2021-11-12 14:34:35.781 Loading token dictionary from /tmp/tmppf26i4l4/vocabulary.
2021-11-12 14:34:35.782 model.type = sentence_level_classifier
2021-11-12 14:34:35.782 model.embedder.type = ref
2021-11-12 14:34:35.783 model.embedder.type = basic
2021-11-12 14:34:35.783 model.embedder.token_embedders.type = ref
2021-11-12 14:34:35.785 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 14:34:35.785 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 14:34:35.785 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 14:34:35.786 type = bert-base-cased
2021-11-12 14:34:35.786 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 14:34:35.786 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 14:34:35.786 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 14:34:35.786 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 14:34:35.786 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 14:34:35.786 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 14:34:35.786 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 14:34:35.786 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 14:34:35.786 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 14:34:35.786 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 14:34:35.786 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 14:34:35.834 model.encoder.type = bert_pooler
2021-11-12 14:34:35.834 model.encoder.type = bert_pooler
2021-11-12 14:34:35.834 model.encoder.pretrained_model = bert-base-cased
2021-11-12 14:34:35.834 type = bert-base-cased
2021-11-12 14:34:35.834 model.encoder.override_weights_file = None
2021-11-12 14:34:35.834 model.encoder.override_weights_strip_prefix = None
2021-11-12 14:34:35.834 model.encoder.load_weights = True
2021-11-12 14:34:35.835 model.encoder.requires_grad = True
2021-11-12 14:34:35.835 model.encoder.dropout = 0.0
2021-11-12 14:34:35.835 model.encoder.transformer_kwargs = None
2021-11-12 14:34:36.025 removing temporary unarchived model dir at /tmp/tmppf26i4l4
2021-11-12 14:43:00.263 Plugin allennlp_models available
2021-11-12 14:43:00.272 Plugin allennlp_server available
2021-11-12 14:43:00.273 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 14:43:00.273 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpqlb3i4p2
2021-11-12 14:43:02.375 dataset_reader.type = example_reader
2021-11-12 14:43:02.375 dataset_reader.max_instances = None
2021-11-12 14:43:02.375 dataset_reader.manual_distributed_sharding = False
2021-11-12 14:43:02.375 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 14:43:02.375 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:43:02.375 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:43:02.376 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 14:43:02.376 type = bert-base-cased
2021-11-12 14:43:02.376 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 14:43:02.376 dataset_reader.tokenizer.max_length = 128
2021-11-12 14:43:02.376 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 14:43:02.377 dataset_reader.text_token_indexers.type = ref
2021-11-12 14:43:02.378 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:43:02.379 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:43:02.379 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 14:43:02.379 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 14:43:02.379 type = bert-base-cased
2021-11-12 14:43:02.380 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 14:43:02.380 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 14:43:02.380 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 14:43:02.381 dataset_reader.to_index = 6
2021-11-12 14:43:02.381 dataset_reader.type = example_reader
2021-11-12 14:43:02.381 dataset_reader.max_instances = None
2021-11-12 14:43:02.381 dataset_reader.manual_distributed_sharding = False
2021-11-12 14:43:02.381 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 14:43:02.381 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:43:02.382 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:43:02.382 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 14:43:02.382 type = bert-base-cased
2021-11-12 14:43:02.382 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 14:43:02.382 dataset_reader.tokenizer.max_length = 128
2021-11-12 14:43:02.382 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 14:43:02.383 dataset_reader.text_token_indexers.type = ref
2021-11-12 14:43:02.384 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:43:02.384 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:43:02.384 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 14:43:02.384 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 14:43:02.384 type = bert-base-cased
2021-11-12 14:43:02.385 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 14:43:02.385 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 14:43:02.385 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 14:43:02.385 dataset_reader.to_index = 6
2021-11-12 14:43:02.386 type = from_instances
2021-11-12 14:43:02.386 Loading token dictionary from /tmp/tmpqlb3i4p2/vocabulary.
2021-11-12 14:43:02.386 model.type = sentence_level_classifier
2021-11-12 14:43:02.387 model.embedder.type = ref
2021-11-12 14:43:02.387 model.embedder.type = basic
2021-11-12 14:43:02.388 model.embedder.token_embedders.type = ref
2021-11-12 14:43:02.389 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 14:43:02.389 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 14:43:02.389 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 14:43:02.389 type = bert-base-cased
2021-11-12 14:43:02.390 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 14:43:02.390 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 14:43:02.390 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 14:43:02.390 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 14:43:02.390 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 14:43:02.390 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 14:43:02.390 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 14:43:02.390 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 14:43:02.390 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 14:43:02.390 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 14:43:02.390 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 14:43:02.438 model.encoder.type = bert_pooler
2021-11-12 14:43:02.438 model.encoder.type = bert_pooler
2021-11-12 14:43:02.438 model.encoder.pretrained_model = bert-base-cased
2021-11-12 14:43:02.438 type = bert-base-cased
2021-11-12 14:43:02.439 model.encoder.override_weights_file = None
2021-11-12 14:43:02.439 model.encoder.override_weights_strip_prefix = None
2021-11-12 14:43:02.439 model.encoder.load_weights = True
2021-11-12 14:43:02.439 model.encoder.requires_grad = True
2021-11-12 14:43:02.439 model.encoder.dropout = 0.0
2021-11-12 14:43:02.439 model.encoder.transformer_kwargs = None
2021-11-12 14:43:02.631 removing temporary unarchived model dir at /tmp/tmpqlb3i4p2
2021-11-12 14:57:13.170 Plugin allennlp_models available
2021-11-12 14:57:13.177 Plugin allennlp_server available
2021-11-12 14:57:13.178 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 14:57:13.178 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmphyvcjjb7
2021-11-12 14:57:15.269 dataset_reader.type = example_reader
2021-11-12 14:57:15.269 dataset_reader.max_instances = None
2021-11-12 14:57:15.269 dataset_reader.manual_distributed_sharding = False
2021-11-12 14:57:15.270 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 14:57:15.270 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:57:15.270 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:57:15.271 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 14:57:15.271 type = bert-base-cased
2021-11-12 14:57:15.271 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 14:57:15.271 dataset_reader.tokenizer.max_length = 128
2021-11-12 14:57:15.272 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 14:57:15.273 dataset_reader.text_token_indexers.type = ref
2021-11-12 14:57:15.273 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:57:15.274 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:57:15.274 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 14:57:15.274 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 14:57:15.274 type = bert-base-cased
2021-11-12 14:57:15.274 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 14:57:15.275 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 14:57:15.275 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 14:57:15.275 dataset_reader.to_index = 6
2021-11-12 14:57:15.276 dataset_reader.type = example_reader
2021-11-12 14:57:15.276 dataset_reader.max_instances = None
2021-11-12 14:57:15.276 dataset_reader.manual_distributed_sharding = False
2021-11-12 14:57:15.276 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 14:57:15.276 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:57:15.277 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:57:15.277 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 14:57:15.277 type = bert-base-cased
2021-11-12 14:57:15.277 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 14:57:15.277 dataset_reader.tokenizer.max_length = 128
2021-11-12 14:57:15.278 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 14:57:15.278 dataset_reader.text_token_indexers.type = ref
2021-11-12 14:57:15.279 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:57:15.279 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:57:15.279 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 14:57:15.280 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 14:57:15.280 type = bert-base-cased
2021-11-12 14:57:15.280 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 14:57:15.280 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 14:57:15.280 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 14:57:15.281 dataset_reader.to_index = 6
2021-11-12 14:57:15.281 type = from_instances
2021-11-12 14:57:15.281 Loading token dictionary from /tmp/tmphyvcjjb7/vocabulary.
2021-11-12 14:57:15.282 model.type = sentence_level_classifier
2021-11-12 14:57:15.282 model.embedder.type = ref
2021-11-12 14:57:15.283 model.embedder.type = basic
2021-11-12 14:57:15.283 model.embedder.token_embedders.type = ref
2021-11-12 14:57:15.285 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 14:57:15.285 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 14:57:15.285 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 14:57:15.286 type = bert-base-cased
2021-11-12 14:57:15.286 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 14:57:15.286 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 14:57:15.286 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 14:57:15.286 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 14:57:15.286 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 14:57:15.286 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 14:57:15.286 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 14:57:15.286 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 14:57:15.286 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 14:57:15.286 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 14:57:15.286 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 14:57:15.334 model.encoder.type = bert_pooler
2021-11-12 14:57:15.334 model.encoder.type = bert_pooler
2021-11-12 14:57:15.335 model.encoder.pretrained_model = bert-base-cased
2021-11-12 14:57:15.335 type = bert-base-cased
2021-11-12 14:57:15.335 model.encoder.override_weights_file = None
2021-11-12 14:57:15.335 model.encoder.override_weights_strip_prefix = None
2021-11-12 14:57:15.335 model.encoder.load_weights = True
2021-11-12 14:57:15.335 model.encoder.requires_grad = True
2021-11-12 14:57:15.335 model.encoder.dropout = 0.0
2021-11-12 14:57:15.335 model.encoder.transformer_kwargs = None
2021-11-12 14:57:15.525 removing temporary unarchived model dir at /tmp/tmphyvcjjb7
2021-11-12 14:57:30.986 Plugin allennlp_models available
2021-11-12 14:57:30.994 Plugin allennlp_server available
2021-11-12 14:57:30.995 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 14:57:30.996 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpv9mhtbi4
2021-11-12 14:57:33.107 dataset_reader.type = example_reader
2021-11-12 14:57:33.107 dataset_reader.max_instances = None
2021-11-12 14:57:33.107 dataset_reader.manual_distributed_sharding = False
2021-11-12 14:57:33.108 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 14:57:33.108 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:57:33.108 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:57:33.108 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 14:57:33.109 type = bert-base-cased
2021-11-12 14:57:33.109 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 14:57:33.109 dataset_reader.tokenizer.max_length = 128
2021-11-12 14:57:33.109 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 14:57:33.110 dataset_reader.text_token_indexers.type = ref
2021-11-12 14:57:33.111 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:57:33.111 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:57:33.112 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 14:57:33.112 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 14:57:33.112 type = bert-base-cased
2021-11-12 14:57:33.112 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 14:57:33.112 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 14:57:33.112 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 14:57:33.113 dataset_reader.to_index = 6
2021-11-12 14:57:33.113 dataset_reader.type = example_reader
2021-11-12 14:57:33.113 dataset_reader.max_instances = None
2021-11-12 14:57:33.113 dataset_reader.manual_distributed_sharding = False
2021-11-12 14:57:33.113 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 14:57:33.113 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:57:33.114 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:57:33.114 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 14:57:33.114 type = bert-base-cased
2021-11-12 14:57:33.114 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 14:57:33.114 dataset_reader.tokenizer.max_length = 128
2021-11-12 14:57:33.114 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 14:57:33.115 dataset_reader.text_token_indexers.type = ref
2021-11-12 14:57:33.115 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:57:33.116 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:57:33.116 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 14:57:33.116 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 14:57:33.116 type = bert-base-cased
2021-11-12 14:57:33.116 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 14:57:33.116 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 14:57:33.116 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 14:57:33.117 dataset_reader.to_index = 6
2021-11-12 14:57:33.117 type = from_instances
2021-11-12 14:57:33.117 Loading token dictionary from /tmp/tmpv9mhtbi4/vocabulary.
2021-11-12 14:57:33.118 model.type = sentence_level_classifier
2021-11-12 14:57:33.118 model.embedder.type = ref
2021-11-12 14:57:33.119 model.embedder.type = basic
2021-11-12 14:57:33.119 model.embedder.token_embedders.type = ref
2021-11-12 14:57:33.121 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 14:57:33.121 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 14:57:33.121 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 14:57:33.121 type = bert-base-cased
2021-11-12 14:57:33.121 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 14:57:33.121 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 14:57:33.121 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 14:57:33.121 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 14:57:33.121 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 14:57:33.122 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 14:57:33.122 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 14:57:33.122 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 14:57:33.122 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 14:57:33.122 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 14:57:33.122 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 14:57:33.171 model.encoder.type = bert_pooler
2021-11-12 14:57:33.171 model.encoder.type = bert_pooler
2021-11-12 14:57:33.172 model.encoder.pretrained_model = bert-base-cased
2021-11-12 14:57:33.172 type = bert-base-cased
2021-11-12 14:57:33.172 model.encoder.override_weights_file = None
2021-11-12 14:57:33.173 model.encoder.override_weights_strip_prefix = None
2021-11-12 14:57:33.173 model.encoder.load_weights = True
2021-11-12 14:57:33.173 model.encoder.requires_grad = True
2021-11-12 14:57:33.173 model.encoder.dropout = 0.0
2021-11-12 14:57:33.173 model.encoder.transformer_kwargs = None
2021-11-12 14:57:33.364 removing temporary unarchived model dir at /tmp/tmpv9mhtbi4
2021-11-12 14:58:09.066 Plugin allennlp_models available
2021-11-12 14:58:09.077 Plugin allennlp_server available
2021-11-12 14:58:09.078 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 14:58:09.078 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpxysznb4y
2021-11-12 14:58:11.182 dataset_reader.type = example_reader
2021-11-12 14:58:11.182 dataset_reader.max_instances = None
2021-11-12 14:58:11.183 dataset_reader.manual_distributed_sharding = False
2021-11-12 14:58:11.183 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 14:58:11.183 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:58:11.184 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:58:11.184 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 14:58:11.184 type = bert-base-cased
2021-11-12 14:58:11.184 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 14:58:11.184 dataset_reader.tokenizer.max_length = 128
2021-11-12 14:58:11.184 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 14:58:11.185 dataset_reader.text_token_indexers.type = ref
2021-11-12 14:58:11.186 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:58:11.186 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:58:11.186 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 14:58:11.187 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 14:58:11.187 type = bert-base-cased
2021-11-12 14:58:11.187 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 14:58:11.187 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 14:58:11.187 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 14:58:11.188 dataset_reader.to_index = 6
2021-11-12 14:58:11.188 dataset_reader.type = example_reader
2021-11-12 14:58:11.188 dataset_reader.max_instances = None
2021-11-12 14:58:11.188 dataset_reader.manual_distributed_sharding = False
2021-11-12 14:58:11.188 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 14:58:11.188 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:58:11.189 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:58:11.189 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 14:58:11.189 type = bert-base-cased
2021-11-12 14:58:11.189 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 14:58:11.189 dataset_reader.tokenizer.max_length = 128
2021-11-12 14:58:11.189 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 14:58:11.190 dataset_reader.text_token_indexers.type = ref
2021-11-12 14:58:11.191 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:58:11.191 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:58:11.191 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 14:58:11.191 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 14:58:11.191 type = bert-base-cased
2021-11-12 14:58:11.191 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 14:58:11.192 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 14:58:11.192 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 14:58:11.192 dataset_reader.to_index = 6
2021-11-12 14:58:11.193 type = from_instances
2021-11-12 14:58:11.193 Loading token dictionary from /tmp/tmpxysznb4y/vocabulary.
2021-11-12 14:58:11.193 model.type = sentence_level_classifier
2021-11-12 14:58:11.193 model.embedder.type = ref
2021-11-12 14:58:11.194 model.embedder.type = basic
2021-11-12 14:58:11.194 model.embedder.token_embedders.type = ref
2021-11-12 14:58:11.196 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 14:58:11.196 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 14:58:11.196 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 14:58:11.197 type = bert-base-cased
2021-11-12 14:58:11.197 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 14:58:11.197 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 14:58:11.197 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 14:58:11.197 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 14:58:11.197 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 14:58:11.197 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 14:58:11.197 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 14:58:11.197 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 14:58:11.197 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 14:58:11.197 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 14:58:11.197 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 14:58:11.236 model.encoder.type = bert_pooler
2021-11-12 14:58:11.236 model.encoder.type = bert_pooler
2021-11-12 14:58:11.237 model.encoder.pretrained_model = bert-base-cased
2021-11-12 14:58:11.237 type = bert-base-cased
2021-11-12 14:58:11.237 model.encoder.override_weights_file = None
2021-11-12 14:58:11.237 model.encoder.override_weights_strip_prefix = None
2021-11-12 14:58:11.237 model.encoder.load_weights = True
2021-11-12 14:58:11.237 model.encoder.requires_grad = True
2021-11-12 14:58:11.237 model.encoder.dropout = 0.0
2021-11-12 14:58:11.238 model.encoder.transformer_kwargs = None
2021-11-12 14:58:11.429 removing temporary unarchived model dir at /tmp/tmpxysznb4y
2021-11-12 14:58:43.784 Plugin allennlp_models available
2021-11-12 14:58:43.793 Plugin allennlp_server available
2021-11-12 14:58:43.794 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 14:58:43.795 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp1qnkyrfx
2021-11-12 14:58:45.901 dataset_reader.type = example_reader
2021-11-12 14:58:45.902 dataset_reader.max_instances = None
2021-11-12 14:58:45.902 dataset_reader.manual_distributed_sharding = False
2021-11-12 14:58:45.902 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 14:58:45.902 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:58:45.902 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:58:45.902 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 14:58:45.902 type = bert-base-cased
2021-11-12 14:58:45.902 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 14:58:45.903 dataset_reader.tokenizer.max_length = 128
2021-11-12 14:58:45.903 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 14:58:45.904 dataset_reader.text_token_indexers.type = ref
2021-11-12 14:58:45.905 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:58:45.905 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:58:45.905 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 14:58:45.906 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 14:58:45.906 type = bert-base-cased
2021-11-12 14:58:45.906 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 14:58:45.906 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 14:58:45.906 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 14:58:45.906 dataset_reader.to_index = 6
2021-11-12 14:58:45.907 dataset_reader.type = example_reader
2021-11-12 14:58:45.907 dataset_reader.max_instances = None
2021-11-12 14:58:45.907 dataset_reader.manual_distributed_sharding = False
2021-11-12 14:58:45.907 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 14:58:45.907 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:58:45.908 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:58:45.908 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 14:58:45.908 type = bert-base-cased
2021-11-12 14:58:45.908 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 14:58:45.908 dataset_reader.tokenizer.max_length = 128
2021-11-12 14:58:45.908 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 14:58:45.910 dataset_reader.text_token_indexers.type = ref
2021-11-12 14:58:45.911 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:58:45.912 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:58:45.912 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 14:58:45.913 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 14:58:45.913 type = bert-base-cased
2021-11-12 14:58:45.913 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 14:58:45.913 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 14:58:45.914 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 14:58:45.915 dataset_reader.to_index = 6
2021-11-12 14:58:45.916 type = from_instances
2021-11-12 14:58:45.916 Loading token dictionary from /tmp/tmp1qnkyrfx/vocabulary.
2021-11-12 14:58:45.917 model.type = sentence_level_classifier
2021-11-12 14:58:45.918 model.embedder.type = ref
2021-11-12 14:58:45.919 model.embedder.type = basic
2021-11-12 14:58:45.920 model.embedder.token_embedders.type = ref
2021-11-12 14:58:45.923 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 14:58:45.924 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 14:58:45.924 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 14:58:45.925 type = bert-base-cased
2021-11-12 14:58:45.925 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 14:58:45.925 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 14:58:45.925 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 14:58:45.925 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 14:58:45.926 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 14:58:45.926 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 14:58:45.926 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 14:58:45.926 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 14:58:45.926 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 14:58:45.926 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 14:58:45.927 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 14:58:45.975 model.encoder.type = bert_pooler
2021-11-12 14:58:45.975 model.encoder.type = bert_pooler
2021-11-12 14:58:45.975 model.encoder.pretrained_model = bert-base-cased
2021-11-12 14:58:45.975 type = bert-base-cased
2021-11-12 14:58:45.976 model.encoder.override_weights_file = None
2021-11-12 14:58:45.976 model.encoder.override_weights_strip_prefix = None
2021-11-12 14:58:45.976 model.encoder.load_weights = True
2021-11-12 14:58:45.976 model.encoder.requires_grad = True
2021-11-12 14:58:45.976 model.encoder.dropout = 0.0
2021-11-12 14:58:45.976 model.encoder.transformer_kwargs = None
2021-11-12 14:58:46.194 removing temporary unarchived model dir at /tmp/tmp1qnkyrfx
2021-11-12 14:59:26.550 Plugin allennlp_models available
2021-11-12 14:59:26.557 Plugin allennlp_server available
2021-11-12 14:59:26.558 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 14:59:26.558 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp6g6qsnch
2021-11-12 14:59:28.670 dataset_reader.type = example_reader
2021-11-12 14:59:28.670 dataset_reader.max_instances = None
2021-11-12 14:59:28.670 dataset_reader.manual_distributed_sharding = False
2021-11-12 14:59:28.670 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 14:59:28.670 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:59:28.671 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:59:28.671 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 14:59:28.671 type = bert-base-cased
2021-11-12 14:59:28.672 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 14:59:28.672 dataset_reader.tokenizer.max_length = 128
2021-11-12 14:59:28.672 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 14:59:28.673 dataset_reader.text_token_indexers.type = ref
2021-11-12 14:59:28.674 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:59:28.674 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:59:28.674 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 14:59:28.675 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 14:59:28.675 type = bert-base-cased
2021-11-12 14:59:28.675 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 14:59:28.675 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 14:59:28.675 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 14:59:28.675 dataset_reader.to_index = 6
2021-11-12 14:59:28.676 dataset_reader.type = example_reader
2021-11-12 14:59:28.676 dataset_reader.max_instances = None
2021-11-12 14:59:28.676 dataset_reader.manual_distributed_sharding = False
2021-11-12 14:59:28.676 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 14:59:28.676 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:59:28.676 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:59:28.676 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 14:59:28.677 type = bert-base-cased
2021-11-12 14:59:28.677 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 14:59:28.677 dataset_reader.tokenizer.max_length = 128
2021-11-12 14:59:28.677 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 14:59:28.678 dataset_reader.text_token_indexers.type = ref
2021-11-12 14:59:28.678 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:59:28.679 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:59:28.679 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 14:59:28.679 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 14:59:28.679 type = bert-base-cased
2021-11-12 14:59:28.679 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 14:59:28.679 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 14:59:28.679 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 14:59:28.680 dataset_reader.to_index = 6
2021-11-12 14:59:28.680 type = from_instances
2021-11-12 14:59:28.680 Loading token dictionary from /tmp/tmp6g6qsnch/vocabulary.
2021-11-12 14:59:28.681 model.type = sentence_level_classifier
2021-11-12 14:59:28.682 model.embedder.type = ref
2021-11-12 14:59:28.684 model.embedder.type = basic
2021-11-12 14:59:28.684 model.embedder.token_embedders.type = ref
2021-11-12 14:59:28.688 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 14:59:28.689 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 14:59:28.690 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 14:59:28.690 type = bert-base-cased
2021-11-12 14:59:28.690 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 14:59:28.691 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 14:59:28.691 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 14:59:28.691 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 14:59:28.691 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 14:59:28.692 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 14:59:28.692 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 14:59:28.692 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 14:59:28.692 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 14:59:28.692 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 14:59:28.693 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 14:59:28.741 model.encoder.type = bert_pooler
2021-11-12 14:59:28.742 model.encoder.type = bert_pooler
2021-11-12 14:59:28.742 model.encoder.pretrained_model = bert-base-cased
2021-11-12 14:59:28.742 type = bert-base-cased
2021-11-12 14:59:28.742 model.encoder.override_weights_file = None
2021-11-12 14:59:28.743 model.encoder.override_weights_strip_prefix = None
2021-11-12 14:59:28.743 model.encoder.load_weights = True
2021-11-12 14:59:28.743 model.encoder.requires_grad = True
2021-11-12 14:59:28.743 model.encoder.dropout = 0.0
2021-11-12 14:59:28.743 model.encoder.transformer_kwargs = None
2021-11-12 14:59:28.933 removing temporary unarchived model dir at /tmp/tmp6g6qsnch
2021-11-12 14:59:33.308 Plugin allennlp_models available
2021-11-12 14:59:33.318 Plugin allennlp_server available
2021-11-12 14:59:33.319 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 14:59:33.319 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp3pkwd3g6
2021-11-12 14:59:35.426 dataset_reader.type = example_reader
2021-11-12 14:59:35.426 dataset_reader.max_instances = None
2021-11-12 14:59:35.427 dataset_reader.manual_distributed_sharding = False
2021-11-12 14:59:35.427 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 14:59:35.427 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:59:35.427 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:59:35.427 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 14:59:35.427 type = bert-base-cased
2021-11-12 14:59:35.428 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 14:59:35.428 dataset_reader.tokenizer.max_length = 128
2021-11-12 14:59:35.428 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 14:59:35.429 dataset_reader.text_token_indexers.type = ref
2021-11-12 14:59:35.430 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:59:35.430 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:59:35.430 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 14:59:35.431 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 14:59:35.431 type = bert-base-cased
2021-11-12 14:59:35.431 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 14:59:35.431 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 14:59:35.431 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 14:59:35.432 dataset_reader.to_index = 6
2021-11-12 14:59:35.432 dataset_reader.type = example_reader
2021-11-12 14:59:35.432 dataset_reader.max_instances = None
2021-11-12 14:59:35.432 dataset_reader.manual_distributed_sharding = False
2021-11-12 14:59:35.433 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 14:59:35.433 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:59:35.433 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:59:35.433 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 14:59:35.433 type = bert-base-cased
2021-11-12 14:59:35.433 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 14:59:35.433 dataset_reader.tokenizer.max_length = 128
2021-11-12 14:59:35.433 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 14:59:35.434 dataset_reader.text_token_indexers.type = ref
2021-11-12 14:59:35.435 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:59:35.435 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:59:35.435 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 14:59:35.436 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 14:59:35.436 type = bert-base-cased
2021-11-12 14:59:35.436 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 14:59:35.436 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 14:59:35.436 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 14:59:35.437 dataset_reader.to_index = 6
2021-11-12 14:59:35.437 type = from_instances
2021-11-12 14:59:35.437 Loading token dictionary from /tmp/tmp3pkwd3g6/vocabulary.
2021-11-12 14:59:35.438 model.type = sentence_level_classifier
2021-11-12 14:59:35.438 model.embedder.type = ref
2021-11-12 14:59:35.438 model.embedder.type = basic
2021-11-12 14:59:35.439 model.embedder.token_embedders.type = ref
2021-11-12 14:59:35.440 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 14:59:35.440 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 14:59:35.441 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 14:59:35.441 type = bert-base-cased
2021-11-12 14:59:35.441 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 14:59:35.441 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 14:59:35.441 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 14:59:35.441 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 14:59:35.441 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 14:59:35.441 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 14:59:35.441 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 14:59:35.441 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 14:59:35.442 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 14:59:35.442 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 14:59:35.442 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 14:59:35.489 model.encoder.type = bert_pooler
2021-11-12 14:59:35.489 model.encoder.type = bert_pooler
2021-11-12 14:59:35.489 model.encoder.pretrained_model = bert-base-cased
2021-11-12 14:59:35.490 type = bert-base-cased
2021-11-12 14:59:35.490 model.encoder.override_weights_file = None
2021-11-12 14:59:35.490 model.encoder.override_weights_strip_prefix = None
2021-11-12 14:59:35.491 model.encoder.load_weights = True
2021-11-12 14:59:35.491 model.encoder.requires_grad = True
2021-11-12 14:59:35.491 model.encoder.dropout = 0.0
2021-11-12 14:59:35.491 model.encoder.transformer_kwargs = None
2021-11-12 14:59:35.680 removing temporary unarchived model dir at /tmp/tmp3pkwd3g6
2021-11-12 14:59:49.675 Plugin allennlp_models available
2021-11-12 14:59:49.681 Plugin allennlp_server available
2021-11-12 14:59:49.682 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 14:59:49.682 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpcl9h2x_1
2021-11-12 14:59:51.784 dataset_reader.type = example_reader
2021-11-12 14:59:51.784 dataset_reader.max_instances = None
2021-11-12 14:59:51.785 dataset_reader.manual_distributed_sharding = False
2021-11-12 14:59:51.785 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 14:59:51.785 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:59:51.786 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:59:51.786 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 14:59:51.786 type = bert-base-cased
2021-11-12 14:59:51.786 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 14:59:51.787 dataset_reader.tokenizer.max_length = 128
2021-11-12 14:59:51.787 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 14:59:51.788 dataset_reader.text_token_indexers.type = ref
2021-11-12 14:59:51.789 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:59:51.790 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:59:51.790 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 14:59:51.790 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 14:59:51.790 type = bert-base-cased
2021-11-12 14:59:51.790 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 14:59:51.791 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 14:59:51.791 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 14:59:51.792 dataset_reader.to_index = 6
2021-11-12 14:59:51.792 dataset_reader.type = example_reader
2021-11-12 14:59:51.792 dataset_reader.max_instances = None
2021-11-12 14:59:51.792 dataset_reader.manual_distributed_sharding = False
2021-11-12 14:59:51.793 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 14:59:51.793 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:59:51.793 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 14:59:51.794 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 14:59:51.795 type = bert-base-cased
2021-11-12 14:59:51.795 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 14:59:51.795 dataset_reader.tokenizer.max_length = 128
2021-11-12 14:59:51.795 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 14:59:51.797 dataset_reader.text_token_indexers.type = ref
2021-11-12 14:59:51.799 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:59:51.799 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 14:59:51.800 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 14:59:51.800 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 14:59:51.801 type = bert-base-cased
2021-11-12 14:59:51.801 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 14:59:51.801 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 14:59:51.801 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 14:59:51.803 dataset_reader.to_index = 6
2021-11-12 14:59:51.804 type = from_instances
2021-11-12 14:59:51.804 Loading token dictionary from /tmp/tmpcl9h2x_1/vocabulary.
2021-11-12 14:59:51.805 model.type = sentence_level_classifier
2021-11-12 14:59:51.806 model.embedder.type = ref
2021-11-12 14:59:51.807 model.embedder.type = basic
2021-11-12 14:59:51.808 model.embedder.token_embedders.type = ref
2021-11-12 14:59:51.811 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 14:59:51.812 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 14:59:51.812 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 14:59:51.813 type = bert-base-cased
2021-11-12 14:59:51.813 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 14:59:51.813 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 14:59:51.813 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 14:59:51.814 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 14:59:51.814 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 14:59:51.814 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 14:59:51.814 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 14:59:51.814 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 14:59:51.814 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 14:59:51.815 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 14:59:51.815 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 14:59:51.862 model.encoder.type = bert_pooler
2021-11-12 14:59:51.863 model.encoder.type = bert_pooler
2021-11-12 14:59:51.863 model.encoder.pretrained_model = bert-base-cased
2021-11-12 14:59:51.863 type = bert-base-cased
2021-11-12 14:59:51.864 model.encoder.override_weights_file = None
2021-11-12 14:59:51.864 model.encoder.override_weights_strip_prefix = None
2021-11-12 14:59:51.864 model.encoder.load_weights = True
2021-11-12 14:59:51.864 model.encoder.requires_grad = True
2021-11-12 14:59:51.864 model.encoder.dropout = 0.0
2021-11-12 14:59:51.865 model.encoder.transformer_kwargs = None
2021-11-12 14:59:52.088 removing temporary unarchived model dir at /tmp/tmpcl9h2x_1
2021-11-12 15:00:03.351 Plugin allennlp_models available
2021-11-12 15:00:03.361 Plugin allennlp_server available
2021-11-12 15:00:03.361 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:00:03.362 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpmcavlf6p
2021-11-12 15:00:05.461 dataset_reader.type = example_reader
2021-11-12 15:00:05.461 dataset_reader.max_instances = None
2021-11-12 15:00:05.461 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:00:05.461 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:00:05.461 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:00:05.461 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:00:05.462 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:00:05.462 type = bert-base-cased
2021-11-12 15:00:05.462 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:00:05.462 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:00:05.462 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:00:05.464 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:00:05.465 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:00:05.465 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:00:05.465 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:00:05.465 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:00:05.465 type = bert-base-cased
2021-11-12 15:00:05.465 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:00:05.466 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:00:05.466 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:00:05.466 dataset_reader.to_index = 6
2021-11-12 15:00:05.466 dataset_reader.type = example_reader
2021-11-12 15:00:05.467 dataset_reader.max_instances = None
2021-11-12 15:00:05.467 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:00:05.467 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:00:05.467 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:00:05.467 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:00:05.467 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:00:05.467 type = bert-base-cased
2021-11-12 15:00:05.467 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:00:05.467 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:00:05.467 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:00:05.468 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:00:05.469 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:00:05.469 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:00:05.469 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:00:05.469 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:00:05.469 type = bert-base-cased
2021-11-12 15:00:05.469 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:00:05.469 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:00:05.469 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:00:05.470 dataset_reader.to_index = 6
2021-11-12 15:00:05.470 type = from_instances
2021-11-12 15:00:05.470 Loading token dictionary from /tmp/tmpmcavlf6p/vocabulary.
2021-11-12 15:00:05.471 model.type = sentence_level_classifier
2021-11-12 15:00:05.471 model.embedder.type = ref
2021-11-12 15:00:05.472 model.embedder.type = basic
2021-11-12 15:00:05.472 model.embedder.token_embedders.type = ref
2021-11-12 15:00:05.474 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:00:05.474 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:00:05.474 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:00:05.474 type = bert-base-cased
2021-11-12 15:00:05.475 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:00:05.475 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:00:05.475 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:00:05.475 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:00:05.475 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:00:05.475 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:00:05.475 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:00:05.475 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:00:05.475 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:00:05.475 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:00:05.475 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:00:05.522 model.encoder.type = bert_pooler
2021-11-12 15:00:05.522 model.encoder.type = bert_pooler
2021-11-12 15:00:05.522 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:00:05.523 type = bert-base-cased
2021-11-12 15:00:05.523 model.encoder.override_weights_file = None
2021-11-12 15:00:05.523 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:00:05.523 model.encoder.load_weights = True
2021-11-12 15:00:05.523 model.encoder.requires_grad = True
2021-11-12 15:00:05.523 model.encoder.dropout = 0.0
2021-11-12 15:00:05.523 model.encoder.transformer_kwargs = None
2021-11-12 15:00:05.718 removing temporary unarchived model dir at /tmp/tmpmcavlf6p
2021-11-12 15:00:09.945 Plugin allennlp_models available
2021-11-12 15:00:09.952 Plugin allennlp_server available
2021-11-12 15:00:09.953 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:00:09.953 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp4a54qlcy
2021-11-12 15:00:12.046 dataset_reader.type = example_reader
2021-11-12 15:00:12.046 dataset_reader.max_instances = None
2021-11-12 15:00:12.047 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:00:12.047 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:00:12.047 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:00:12.047 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:00:12.048 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:00:12.048 type = bert-base-cased
2021-11-12 15:00:12.048 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:00:12.048 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:00:12.048 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:00:12.049 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:00:12.050 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:00:12.050 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:00:12.050 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:00:12.051 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:00:12.051 type = bert-base-cased
2021-11-12 15:00:12.051 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:00:12.051 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:00:12.051 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:00:12.052 dataset_reader.to_index = 6
2021-11-12 15:00:12.052 dataset_reader.type = example_reader
2021-11-12 15:00:12.052 dataset_reader.max_instances = None
2021-11-12 15:00:12.053 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:00:12.053 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:00:12.053 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:00:12.053 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:00:12.053 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:00:12.054 type = bert-base-cased
2021-11-12 15:00:12.054 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:00:12.054 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:00:12.054 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:00:12.055 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:00:12.055 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:00:12.056 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:00:12.056 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:00:12.056 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:00:12.056 type = bert-base-cased
2021-11-12 15:00:12.056 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:00:12.056 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:00:12.056 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:00:12.057 dataset_reader.to_index = 6
2021-11-12 15:00:12.057 type = from_instances
2021-11-12 15:00:12.057 Loading token dictionary from /tmp/tmp4a54qlcy/vocabulary.
2021-11-12 15:00:12.058 model.type = sentence_level_classifier
2021-11-12 15:00:12.058 model.embedder.type = ref
2021-11-12 15:00:12.059 model.embedder.type = basic
2021-11-12 15:00:12.059 model.embedder.token_embedders.type = ref
2021-11-12 15:00:12.061 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:00:12.061 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:00:12.061 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:00:12.061 type = bert-base-cased
2021-11-12 15:00:12.061 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:00:12.062 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:00:12.062 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:00:12.062 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:00:12.062 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:00:12.062 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:00:12.062 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:00:12.062 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:00:12.062 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:00:12.062 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:00:12.062 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:00:12.114 model.encoder.type = bert_pooler
2021-11-12 15:00:12.114 model.encoder.type = bert_pooler
2021-11-12 15:00:12.114 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:00:12.114 type = bert-base-cased
2021-11-12 15:00:12.115 model.encoder.override_weights_file = None
2021-11-12 15:00:12.115 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:00:12.115 model.encoder.load_weights = True
2021-11-12 15:00:12.115 model.encoder.requires_grad = True
2021-11-12 15:00:12.115 model.encoder.dropout = 0.0
2021-11-12 15:00:12.115 model.encoder.transformer_kwargs = None
2021-11-12 15:00:12.305 removing temporary unarchived model dir at /tmp/tmp4a54qlcy
2021-11-12 15:00:17.190 Plugin allennlp_models available
2021-11-12 15:00:17.196 Plugin allennlp_server available
2021-11-12 15:00:17.196 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:00:17.197 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp725m1248
2021-11-12 15:00:19.289 dataset_reader.type = example_reader
2021-11-12 15:00:19.290 dataset_reader.max_instances = None
2021-11-12 15:00:19.290 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:00:19.290 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:00:19.290 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:00:19.290 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:00:19.291 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:00:19.291 type = bert-base-cased
2021-11-12 15:00:19.291 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:00:19.291 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:00:19.291 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:00:19.292 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:00:19.293 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:00:19.293 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:00:19.294 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:00:19.294 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:00:19.294 type = bert-base-cased
2021-11-12 15:00:19.294 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:00:19.294 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:00:19.294 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:00:19.295 dataset_reader.to_index = 6
2021-11-12 15:00:19.295 dataset_reader.type = example_reader
2021-11-12 15:00:19.295 dataset_reader.max_instances = None
2021-11-12 15:00:19.295 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:00:19.295 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:00:19.295 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:00:19.296 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:00:19.296 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:00:19.296 type = bert-base-cased
2021-11-12 15:00:19.296 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:00:19.296 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:00:19.296 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:00:19.297 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:00:19.298 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:00:19.298 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:00:19.298 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:00:19.298 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:00:19.299 type = bert-base-cased
2021-11-12 15:00:19.299 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:00:19.299 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:00:19.299 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:00:19.299 dataset_reader.to_index = 6
2021-11-12 15:00:19.300 type = from_instances
2021-11-12 15:00:19.300 Loading token dictionary from /tmp/tmp725m1248/vocabulary.
2021-11-12 15:00:19.300 model.type = sentence_level_classifier
2021-11-12 15:00:19.301 model.embedder.type = ref
2021-11-12 15:00:19.301 model.embedder.type = basic
2021-11-12 15:00:19.302 model.embedder.token_embedders.type = ref
2021-11-12 15:00:19.303 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:00:19.303 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:00:19.304 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:00:19.304 type = bert-base-cased
2021-11-12 15:00:19.304 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:00:19.304 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:00:19.304 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:00:19.304 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:00:19.304 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:00:19.305 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:00:19.305 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:00:19.305 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:00:19.305 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:00:19.305 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:00:19.305 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:00:19.351 model.encoder.type = bert_pooler
2021-11-12 15:00:19.351 model.encoder.type = bert_pooler
2021-11-12 15:00:19.352 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:00:19.352 type = bert-base-cased
2021-11-12 15:00:19.352 model.encoder.override_weights_file = None
2021-11-12 15:00:19.352 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:00:19.352 model.encoder.load_weights = True
2021-11-12 15:00:19.353 model.encoder.requires_grad = True
2021-11-12 15:00:19.353 model.encoder.dropout = 0.0
2021-11-12 15:00:19.353 model.encoder.transformer_kwargs = None
2021-11-12 15:00:19.542 removing temporary unarchived model dir at /tmp/tmp725m1248
2021-11-12 15:00:27.465 Plugin allennlp_models available
2021-11-12 15:00:27.475 Plugin allennlp_server available
2021-11-12 15:00:27.476 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:00:27.476 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpy22k9xlg
2021-11-12 15:00:29.581 dataset_reader.type = example_reader
2021-11-12 15:00:29.581 dataset_reader.max_instances = None
2021-11-12 15:00:29.581 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:00:29.584 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:00:29.584 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:00:29.584 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:00:29.584 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:00:29.584 type = bert-base-cased
2021-11-12 15:00:29.584 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:00:29.585 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:00:29.585 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:00:29.586 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:00:29.587 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:00:29.587 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:00:29.587 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:00:29.588 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:00:29.588 type = bert-base-cased
2021-11-12 15:00:29.588 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:00:29.588 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:00:29.588 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:00:29.589 dataset_reader.to_index = 6
2021-11-12 15:00:29.589 dataset_reader.type = example_reader
2021-11-12 15:00:29.589 dataset_reader.max_instances = None
2021-11-12 15:00:29.589 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:00:29.589 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:00:29.590 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:00:29.590 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:00:29.590 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:00:29.590 type = bert-base-cased
2021-11-12 15:00:29.590 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:00:29.590 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:00:29.590 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:00:29.591 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:00:29.592 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:00:29.592 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:00:29.592 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:00:29.592 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:00:29.593 type = bert-base-cased
2021-11-12 15:00:29.593 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:00:29.593 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:00:29.593 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:00:29.594 dataset_reader.to_index = 6
2021-11-12 15:00:29.594 type = from_instances
2021-11-12 15:00:29.594 Loading token dictionary from /tmp/tmpy22k9xlg/vocabulary.
2021-11-12 15:00:29.595 model.type = sentence_level_classifier
2021-11-12 15:00:29.595 model.embedder.type = ref
2021-11-12 15:00:29.596 model.embedder.type = basic
2021-11-12 15:00:29.596 model.embedder.token_embedders.type = ref
2021-11-12 15:00:29.597 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:00:29.598 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:00:29.598 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:00:29.598 type = bert-base-cased
2021-11-12 15:00:29.598 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:00:29.598 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:00:29.599 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:00:29.599 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:00:29.599 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:00:29.599 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:00:29.599 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:00:29.599 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:00:29.599 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:00:29.599 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:00:29.599 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:00:29.648 model.encoder.type = bert_pooler
2021-11-12 15:00:29.648 model.encoder.type = bert_pooler
2021-11-12 15:00:29.648 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:00:29.648 type = bert-base-cased
2021-11-12 15:00:29.649 model.encoder.override_weights_file = None
2021-11-12 15:00:29.649 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:00:29.649 model.encoder.load_weights = True
2021-11-12 15:00:29.649 model.encoder.requires_grad = True
2021-11-12 15:00:29.649 model.encoder.dropout = 0.0
2021-11-12 15:00:29.649 model.encoder.transformer_kwargs = None
2021-11-12 15:00:29.839 removing temporary unarchived model dir at /tmp/tmpy22k9xlg
2021-11-12 15:00:42.870 Plugin allennlp_models available
2021-11-12 15:00:42.877 Plugin allennlp_server available
2021-11-12 15:00:42.877 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:00:42.878 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp1r8iqhhj
2021-11-12 15:00:44.968 dataset_reader.type = example_reader
2021-11-12 15:00:44.968 dataset_reader.max_instances = None
2021-11-12 15:00:44.968 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:00:44.969 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:00:44.969 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:00:44.969 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:00:44.970 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:00:44.970 type = bert-base-cased
2021-11-12 15:00:44.970 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:00:44.970 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:00:44.970 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:00:44.971 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:00:44.972 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:00:44.972 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:00:44.972 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:00:44.973 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:00:44.973 type = bert-base-cased
2021-11-12 15:00:44.973 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:00:44.973 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:00:44.974 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:00:44.974 dataset_reader.to_index = 6
2021-11-12 15:00:44.974 dataset_reader.type = example_reader
2021-11-12 15:00:44.975 dataset_reader.max_instances = None
2021-11-12 15:00:44.975 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:00:44.975 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:00:44.975 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:00:44.975 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:00:44.975 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:00:44.975 type = bert-base-cased
2021-11-12 15:00:44.975 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:00:44.975 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:00:44.976 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:00:44.976 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:00:44.977 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:00:44.977 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:00:44.977 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:00:44.977 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:00:44.978 type = bert-base-cased
2021-11-12 15:00:44.978 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:00:44.978 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:00:44.978 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:00:44.979 dataset_reader.to_index = 6
2021-11-12 15:00:44.979 type = from_instances
2021-11-12 15:00:44.979 Loading token dictionary from /tmp/tmp1r8iqhhj/vocabulary.
2021-11-12 15:00:44.980 model.type = sentence_level_classifier
2021-11-12 15:00:44.980 model.embedder.type = ref
2021-11-12 15:00:44.981 model.embedder.type = basic
2021-11-12 15:00:44.981 model.embedder.token_embedders.type = ref
2021-11-12 15:00:44.983 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:00:44.983 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:00:44.983 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:00:44.983 type = bert-base-cased
2021-11-12 15:00:44.983 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:00:44.984 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:00:44.984 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:00:44.984 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:00:44.984 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:00:44.984 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:00:44.984 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:00:44.984 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:00:44.984 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:00:44.984 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:00:44.984 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:00:45.033 model.encoder.type = bert_pooler
2021-11-12 15:00:45.033 model.encoder.type = bert_pooler
2021-11-12 15:00:45.034 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:00:45.034 type = bert-base-cased
2021-11-12 15:00:45.034 model.encoder.override_weights_file = None
2021-11-12 15:00:45.034 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:00:45.034 model.encoder.load_weights = True
2021-11-12 15:00:45.034 model.encoder.requires_grad = True
2021-11-12 15:00:45.035 model.encoder.dropout = 0.0
2021-11-12 15:00:45.035 model.encoder.transformer_kwargs = None
2021-11-12 15:00:45.221 removing temporary unarchived model dir at /tmp/tmp1r8iqhhj
2021-11-12 15:00:51.007 Plugin allennlp_models available
2021-11-12 15:00:51.015 Plugin allennlp_server available
2021-11-12 15:00:51.016 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:00:51.016 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpw7k4elm5
2021-11-12 15:00:53.146 dataset_reader.type = example_reader
2021-11-12 15:00:53.146 dataset_reader.max_instances = None
2021-11-12 15:00:53.147 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:00:53.147 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:00:53.147 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:00:53.147 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:00:53.147 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:00:53.148 type = bert-base-cased
2021-11-12 15:00:53.148 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:00:53.148 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:00:53.148 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:00:53.149 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:00:53.150 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:00:53.150 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:00:53.150 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:00:53.151 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:00:53.151 type = bert-base-cased
2021-11-12 15:00:53.151 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:00:53.151 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:00:53.151 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:00:53.151 dataset_reader.to_index = 6
2021-11-12 15:00:53.152 dataset_reader.type = example_reader
2021-11-12 15:00:53.152 dataset_reader.max_instances = None
2021-11-12 15:00:53.152 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:00:53.152 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:00:53.152 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:00:53.152 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:00:53.152 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:00:53.153 type = bert-base-cased
2021-11-12 15:00:53.153 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:00:53.153 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:00:53.153 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:00:53.154 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:00:53.154 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:00:53.154 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:00:53.155 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:00:53.155 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:00:53.155 type = bert-base-cased
2021-11-12 15:00:53.155 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:00:53.155 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:00:53.155 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:00:53.156 dataset_reader.to_index = 6
2021-11-12 15:00:53.156 type = from_instances
2021-11-12 15:00:53.156 Loading token dictionary from /tmp/tmpw7k4elm5/vocabulary.
2021-11-12 15:00:53.157 model.type = sentence_level_classifier
2021-11-12 15:00:53.157 model.embedder.type = ref
2021-11-12 15:00:53.158 model.embedder.type = basic
2021-11-12 15:00:53.158 model.embedder.token_embedders.type = ref
2021-11-12 15:00:53.160 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:00:53.160 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:00:53.160 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:00:53.160 type = bert-base-cased
2021-11-12 15:00:53.160 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:00:53.160 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:00:53.161 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:00:53.161 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:00:53.161 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:00:53.161 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:00:53.161 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:00:53.161 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:00:53.161 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:00:53.161 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:00:53.161 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:00:53.212 model.encoder.type = bert_pooler
2021-11-12 15:00:53.212 model.encoder.type = bert_pooler
2021-11-12 15:00:53.212 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:00:53.213 type = bert-base-cased
2021-11-12 15:00:53.213 model.encoder.override_weights_file = None
2021-11-12 15:00:53.213 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:00:53.213 model.encoder.load_weights = True
2021-11-12 15:00:53.213 model.encoder.requires_grad = True
2021-11-12 15:00:53.213 model.encoder.dropout = 0.0
2021-11-12 15:00:53.213 model.encoder.transformer_kwargs = None
2021-11-12 15:00:53.400 removing temporary unarchived model dir at /tmp/tmpw7k4elm5
2021-11-12 15:01:04.360 Plugin allennlp_models available
2021-11-12 15:01:04.368 Plugin allennlp_server available
2021-11-12 15:01:04.369 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:01:04.370 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmps9xc5qr4
2021-11-12 15:01:06.472 dataset_reader.type = example_reader
2021-11-12 15:01:06.472 dataset_reader.max_instances = None
2021-11-12 15:01:06.472 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:01:06.472 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:01:06.473 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:01:06.473 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:01:06.473 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:01:06.473 type = bert-base-cased
2021-11-12 15:01:06.473 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:01:06.473 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:01:06.473 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:01:06.475 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:01:06.476 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:01:06.476 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:01:06.476 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:01:06.477 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:01:06.477 type = bert-base-cased
2021-11-12 15:01:06.477 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:01:06.478 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:01:06.478 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:01:06.480 dataset_reader.to_index = 6
2021-11-12 15:01:06.480 dataset_reader.type = example_reader
2021-11-12 15:01:06.481 dataset_reader.max_instances = None
2021-11-12 15:01:06.481 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:01:06.481 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:01:06.482 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:01:06.482 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:01:06.483 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:01:06.483 type = bert-base-cased
2021-11-12 15:01:06.483 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:01:06.483 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:01:06.484 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:01:06.486 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:01:06.487 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:01:06.488 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:01:06.488 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:01:06.489 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:01:06.489 type = bert-base-cased
2021-11-12 15:01:06.489 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:01:06.489 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:01:06.490 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:01:06.491 dataset_reader.to_index = 6
2021-11-12 15:01:06.492 type = from_instances
2021-11-12 15:01:06.492 Loading token dictionary from /tmp/tmps9xc5qr4/vocabulary.
2021-11-12 15:01:06.493 model.type = sentence_level_classifier
2021-11-12 15:01:06.494 model.embedder.type = ref
2021-11-12 15:01:06.495 model.embedder.type = basic
2021-11-12 15:01:06.496 model.embedder.token_embedders.type = ref
2021-11-12 15:01:06.499 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:01:06.499 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:01:06.500 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:01:06.500 type = bert-base-cased
2021-11-12 15:01:06.500 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:01:06.501 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:01:06.501 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:01:06.501 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:01:06.501 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:01:06.501 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:01:06.502 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:01:06.502 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:01:06.502 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:01:06.502 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:01:06.503 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:01:06.552 model.encoder.type = bert_pooler
2021-11-12 15:01:06.553 model.encoder.type = bert_pooler
2021-11-12 15:01:06.553 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:01:06.553 type = bert-base-cased
2021-11-12 15:01:06.553 model.encoder.override_weights_file = None
2021-11-12 15:01:06.553 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:01:06.553 model.encoder.load_weights = True
2021-11-12 15:01:06.554 model.encoder.requires_grad = True
2021-11-12 15:01:06.554 model.encoder.dropout = 0.0
2021-11-12 15:01:06.554 model.encoder.transformer_kwargs = None
2021-11-12 15:01:06.741 removing temporary unarchived model dir at /tmp/tmps9xc5qr4
2021-11-12 15:02:11.155 Plugin allennlp_models available
2021-11-12 15:02:11.165 Plugin allennlp_server available
2021-11-12 15:02:11.166 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:02:11.166 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpnm7_c231
2021-11-12 15:02:13.263 dataset_reader.type = example_reader
2021-11-12 15:02:13.263 dataset_reader.max_instances = None
2021-11-12 15:02:13.263 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:02:13.264 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:02:13.264 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:02:13.264 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:02:13.264 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:02:13.264 type = bert-base-cased
2021-11-12 15:02:13.265 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:02:13.265 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:02:13.265 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:02:13.266 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:02:13.267 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:02:13.267 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:02:13.268 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:02:13.268 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:02:13.268 type = bert-base-cased
2021-11-12 15:02:13.268 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:02:13.268 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:02:13.268 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:02:13.269 dataset_reader.to_index = 6
2021-11-12 15:02:13.269 dataset_reader.type = example_reader
2021-11-12 15:02:13.269 dataset_reader.max_instances = None
2021-11-12 15:02:13.269 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:02:13.270 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:02:13.270 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:02:13.270 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:02:13.270 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:02:13.270 type = bert-base-cased
2021-11-12 15:02:13.270 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:02:13.270 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:02:13.270 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:02:13.271 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:02:13.272 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:02:13.272 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:02:13.272 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:02:13.272 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:02:13.273 type = bert-base-cased
2021-11-12 15:02:13.273 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:02:13.273 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:02:13.273 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:02:13.274 dataset_reader.to_index = 6
2021-11-12 15:02:13.274 type = from_instances
2021-11-12 15:02:13.274 Loading token dictionary from /tmp/tmpnm7_c231/vocabulary.
2021-11-12 15:02:13.275 model.type = sentence_level_classifier
2021-11-12 15:02:13.275 model.embedder.type = ref
2021-11-12 15:02:13.275 model.embedder.type = basic
2021-11-12 15:02:13.276 model.embedder.token_embedders.type = ref
2021-11-12 15:02:13.277 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:02:13.278 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:02:13.278 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:02:13.278 type = bert-base-cased
2021-11-12 15:02:13.278 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:02:13.279 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:02:13.279 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:02:13.279 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:02:13.279 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:02:13.279 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:02:13.279 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:02:13.279 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:02:13.279 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:02:13.279 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:02:13.279 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:02:13.329 model.encoder.type = bert_pooler
2021-11-12 15:02:13.329 model.encoder.type = bert_pooler
2021-11-12 15:02:13.329 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:02:13.329 type = bert-base-cased
2021-11-12 15:02:13.330 model.encoder.override_weights_file = None
2021-11-12 15:02:13.330 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:02:13.330 model.encoder.load_weights = True
2021-11-12 15:02:13.330 model.encoder.requires_grad = True
2021-11-12 15:02:13.330 model.encoder.dropout = 0.0
2021-11-12 15:02:13.330 model.encoder.transformer_kwargs = None
2021-11-12 15:02:13.520 removing temporary unarchived model dir at /tmp/tmpnm7_c231
2021-11-12 15:02:37.159 Plugin allennlp_models available
2021-11-12 15:02:37.166 Plugin allennlp_server available
2021-11-12 15:02:37.167 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:02:37.167 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp3xiwhdk7
2021-11-12 15:02:39.255 dataset_reader.type = example_reader
2021-11-12 15:02:39.256 dataset_reader.max_instances = None
2021-11-12 15:02:39.256 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:02:39.257 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:02:39.257 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:02:39.257 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:02:39.257 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:02:39.258 type = bert-base-cased
2021-11-12 15:02:39.258 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:02:39.258 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:02:39.258 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:02:39.260 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:02:39.261 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:02:39.261 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:02:39.261 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:02:39.261 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:02:39.262 type = bert-base-cased
2021-11-12 15:02:39.262 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:02:39.262 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:02:39.262 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:02:39.263 dataset_reader.to_index = 6
2021-11-12 15:02:39.263 dataset_reader.type = example_reader
2021-11-12 15:02:39.264 dataset_reader.max_instances = None
2021-11-12 15:02:39.264 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:02:39.264 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:02:39.264 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:02:39.265 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:02:39.265 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:02:39.265 type = bert-base-cased
2021-11-12 15:02:39.266 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:02:39.266 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:02:39.266 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:02:39.267 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:02:39.268 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:02:39.268 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:02:39.268 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:02:39.268 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:02:39.269 type = bert-base-cased
2021-11-12 15:02:39.269 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:02:39.269 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:02:39.269 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:02:39.270 dataset_reader.to_index = 6
2021-11-12 15:02:39.271 type = from_instances
2021-11-12 15:02:39.271 Loading token dictionary from /tmp/tmp3xiwhdk7/vocabulary.
2021-11-12 15:02:39.271 model.type = sentence_level_classifier
2021-11-12 15:02:39.272 model.embedder.type = ref
2021-11-12 15:02:39.273 model.embedder.type = basic
2021-11-12 15:02:39.273 model.embedder.token_embedders.type = ref
2021-11-12 15:02:39.275 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:02:39.275 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:02:39.275 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:02:39.276 type = bert-base-cased
2021-11-12 15:02:39.276 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:02:39.276 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:02:39.276 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:02:39.277 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:02:39.277 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:02:39.277 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:02:39.277 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:02:39.277 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:02:39.278 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:02:39.278 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:02:39.278 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:02:39.328 model.encoder.type = bert_pooler
2021-11-12 15:02:39.328 model.encoder.type = bert_pooler
2021-11-12 15:02:39.329 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:02:39.329 type = bert-base-cased
2021-11-12 15:02:39.330 model.encoder.override_weights_file = None
2021-11-12 15:02:39.330 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:02:39.330 model.encoder.load_weights = True
2021-11-12 15:02:39.330 model.encoder.requires_grad = True
2021-11-12 15:02:39.330 model.encoder.dropout = 0.0
2021-11-12 15:02:39.331 model.encoder.transformer_kwargs = None
2021-11-12 15:02:39.525 removing temporary unarchived model dir at /tmp/tmp3xiwhdk7
2021-11-12 15:02:53.055 Plugin allennlp_models available
2021-11-12 15:02:53.065 Plugin allennlp_server available
2021-11-12 15:02:53.066 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:02:53.066 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpw4tzerlv
2021-11-12 15:02:55.177 dataset_reader.type = example_reader
2021-11-12 15:02:55.177 dataset_reader.max_instances = None
2021-11-12 15:02:55.177 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:02:55.178 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:02:55.178 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:02:55.178 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:02:55.179 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:02:55.179 type = bert-base-cased
2021-11-12 15:02:55.179 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:02:55.179 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:02:55.179 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:02:55.180 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:02:55.181 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:02:55.181 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:02:55.181 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:02:55.181 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:02:55.182 type = bert-base-cased
2021-11-12 15:02:55.182 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:02:55.182 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:02:55.182 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:02:55.183 dataset_reader.to_index = 6
2021-11-12 15:02:55.183 dataset_reader.type = example_reader
2021-11-12 15:02:55.183 dataset_reader.max_instances = None
2021-11-12 15:02:55.183 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:02:55.183 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:02:55.183 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:02:55.183 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:02:55.183 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:02:55.184 type = bert-base-cased
2021-11-12 15:02:55.184 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:02:55.184 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:02:55.184 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:02:55.185 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:02:55.186 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:02:55.186 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:02:55.186 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:02:55.186 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:02:55.186 type = bert-base-cased
2021-11-12 15:02:55.186 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:02:55.186 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:02:55.186 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:02:55.187 dataset_reader.to_index = 6
2021-11-12 15:02:55.187 type = from_instances
2021-11-12 15:02:55.187 Loading token dictionary from /tmp/tmpw4tzerlv/vocabulary.
2021-11-12 15:02:55.188 model.type = sentence_level_classifier
2021-11-12 15:02:55.188 model.embedder.type = ref
2021-11-12 15:02:55.189 model.embedder.type = basic
2021-11-12 15:02:55.189 model.embedder.token_embedders.type = ref
2021-11-12 15:02:55.191 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:02:55.191 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:02:55.191 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:02:55.191 type = bert-base-cased
2021-11-12 15:02:55.191 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:02:55.191 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:02:55.191 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:02:55.191 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:02:55.191 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:02:55.191 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:02:55.191 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:02:55.191 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:02:55.192 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:02:55.192 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:02:55.192 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:02:55.240 model.encoder.type = bert_pooler
2021-11-12 15:02:55.240 model.encoder.type = bert_pooler
2021-11-12 15:02:55.240 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:02:55.240 type = bert-base-cased
2021-11-12 15:02:55.241 model.encoder.override_weights_file = None
2021-11-12 15:02:55.241 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:02:55.241 model.encoder.load_weights = True
2021-11-12 15:02:55.241 model.encoder.requires_grad = True
2021-11-12 15:02:55.241 model.encoder.dropout = 0.0
2021-11-12 15:02:55.241 model.encoder.transformer_kwargs = None
2021-11-12 15:02:55.431 removing temporary unarchived model dir at /tmp/tmpw4tzerlv
2021-11-12 15:03:08.229 Plugin allennlp_models available
2021-11-12 15:03:08.237 Plugin allennlp_server available
2021-11-12 15:03:08.238 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:03:08.238 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpw01nc40n
2021-11-12 15:03:10.340 dataset_reader.type = example_reader
2021-11-12 15:03:10.341 dataset_reader.max_instances = None
2021-11-12 15:03:10.341 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:03:10.341 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:03:10.341 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:03:10.341 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:03:10.342 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:03:10.342 type = bert-base-cased
2021-11-12 15:03:10.342 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:03:10.342 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:03:10.342 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:03:10.343 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:03:10.344 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:03:10.344 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:03:10.345 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:03:10.345 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:03:10.345 type = bert-base-cased
2021-11-12 15:03:10.345 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:03:10.345 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:03:10.345 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:03:10.346 dataset_reader.to_index = 6
2021-11-12 15:03:10.346 dataset_reader.type = example_reader
2021-11-12 15:03:10.346 dataset_reader.max_instances = None
2021-11-12 15:03:10.347 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:03:10.347 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:03:10.347 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:03:10.347 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:03:10.347 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:03:10.347 type = bert-base-cased
2021-11-12 15:03:10.347 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:03:10.347 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:03:10.347 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:03:10.348 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:03:10.349 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:03:10.349 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:03:10.349 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:03:10.349 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:03:10.349 type = bert-base-cased
2021-11-12 15:03:10.350 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:03:10.350 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:03:10.350 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:03:10.350 dataset_reader.to_index = 6
2021-11-12 15:03:10.351 type = from_instances
2021-11-12 15:03:10.351 Loading token dictionary from /tmp/tmpw01nc40n/vocabulary.
2021-11-12 15:03:10.351 model.type = sentence_level_classifier
2021-11-12 15:03:10.351 model.embedder.type = ref
2021-11-12 15:03:10.352 model.embedder.type = basic
2021-11-12 15:03:10.352 model.embedder.token_embedders.type = ref
2021-11-12 15:03:10.354 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:03:10.354 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:03:10.355 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:03:10.355 type = bert-base-cased
2021-11-12 15:03:10.355 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:03:10.355 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:03:10.355 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:03:10.355 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:03:10.356 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:03:10.356 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:03:10.356 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:03:10.356 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:03:10.356 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:03:10.356 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:03:10.356 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:03:10.403 model.encoder.type = bert_pooler
2021-11-12 15:03:10.403 model.encoder.type = bert_pooler
2021-11-12 15:03:10.404 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:03:10.404 type = bert-base-cased
2021-11-12 15:03:10.404 model.encoder.override_weights_file = None
2021-11-12 15:03:10.404 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:03:10.404 model.encoder.load_weights = True
2021-11-12 15:03:10.404 model.encoder.requires_grad = True
2021-11-12 15:03:10.405 model.encoder.dropout = 0.0
2021-11-12 15:03:10.405 model.encoder.transformer_kwargs = None
2021-11-12 15:03:10.606 removing temporary unarchived model dir at /tmp/tmpw01nc40n
2021-11-12 15:03:17.140 Plugin allennlp_models available
2021-11-12 15:03:17.148 Plugin allennlp_server available
2021-11-12 15:03:17.149 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:03:17.149 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp8664id5g
2021-11-12 15:03:19.261 dataset_reader.type = example_reader
2021-11-12 15:03:19.261 dataset_reader.max_instances = None
2021-11-12 15:03:19.261 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:03:19.261 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:03:19.261 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:03:19.262 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:03:19.262 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:03:19.262 type = bert-base-cased
2021-11-12 15:03:19.262 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:03:19.262 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:03:19.262 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:03:19.263 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:03:19.264 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:03:19.265 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:03:19.265 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:03:19.265 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:03:19.265 type = bert-base-cased
2021-11-12 15:03:19.266 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:03:19.266 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:03:19.266 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:03:19.267 dataset_reader.to_index = 6
2021-11-12 15:03:19.267 dataset_reader.type = example_reader
2021-11-12 15:03:19.267 dataset_reader.max_instances = None
2021-11-12 15:03:19.267 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:03:19.267 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:03:19.267 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:03:19.267 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:03:19.268 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:03:19.268 type = bert-base-cased
2021-11-12 15:03:19.268 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:03:19.268 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:03:19.268 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:03:19.269 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:03:19.270 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:03:19.270 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:03:19.270 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:03:19.270 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:03:19.270 type = bert-base-cased
2021-11-12 15:03:19.270 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:03:19.270 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:03:19.271 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:03:19.271 dataset_reader.to_index = 6
2021-11-12 15:03:19.272 type = from_instances
2021-11-12 15:03:19.272 Loading token dictionary from /tmp/tmp8664id5g/vocabulary.
2021-11-12 15:03:19.272 model.type = sentence_level_classifier
2021-11-12 15:03:19.272 model.embedder.type = ref
2021-11-12 15:03:19.273 model.embedder.type = basic
2021-11-12 15:03:19.273 model.embedder.token_embedders.type = ref
2021-11-12 15:03:19.275 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:03:19.275 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:03:19.275 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:03:19.276 type = bert-base-cased
2021-11-12 15:03:19.276 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:03:19.276 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:03:19.276 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:03:19.276 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:03:19.276 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:03:19.276 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:03:19.276 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:03:19.276 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:03:19.276 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:03:19.277 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:03:19.277 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:03:19.328 model.encoder.type = bert_pooler
2021-11-12 15:03:19.328 model.encoder.type = bert_pooler
2021-11-12 15:03:19.328 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:03:19.329 type = bert-base-cased
2021-11-12 15:03:19.329 model.encoder.override_weights_file = None
2021-11-12 15:03:19.329 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:03:19.329 model.encoder.load_weights = True
2021-11-12 15:03:19.329 model.encoder.requires_grad = True
2021-11-12 15:03:19.329 model.encoder.dropout = 0.0
2021-11-12 15:03:19.329 model.encoder.transformer_kwargs = None
2021-11-12 15:03:19.517 removing temporary unarchived model dir at /tmp/tmp8664id5g
2021-11-12 15:03:45.856 Plugin allennlp_models available
2021-11-12 15:03:45.866 Plugin allennlp_server available
2021-11-12 15:03:45.867 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:03:45.867 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp4wgcqrsv
2021-11-12 15:03:47.988 dataset_reader.type = example_reader
2021-11-12 15:03:47.988 dataset_reader.max_instances = None
2021-11-12 15:03:47.988 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:03:47.989 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:03:47.989 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:03:47.989 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:03:47.990 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:03:47.990 type = bert-base-cased
2021-11-12 15:03:47.990 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:03:47.990 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:03:47.990 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:03:47.991 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:03:47.992 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:03:47.992 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:03:47.993 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:03:47.993 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:03:47.993 type = bert-base-cased
2021-11-12 15:03:47.993 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:03:47.994 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:03:47.994 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:03:47.994 dataset_reader.to_index = 6
2021-11-12 15:03:47.995 dataset_reader.type = example_reader
2021-11-12 15:03:47.995 dataset_reader.max_instances = None
2021-11-12 15:03:47.995 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:03:47.995 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:03:47.995 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:03:47.995 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:03:47.996 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:03:47.996 type = bert-base-cased
2021-11-12 15:03:47.996 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:03:47.996 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:03:47.996 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:03:47.997 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:03:47.998 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:03:47.998 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:03:47.998 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:03:47.998 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:03:47.998 type = bert-base-cased
2021-11-12 15:03:47.998 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:03:47.999 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:03:47.999 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:03:48.000 dataset_reader.to_index = 6
2021-11-12 15:03:48.000 type = from_instances
2021-11-12 15:03:48.000 Loading token dictionary from /tmp/tmp4wgcqrsv/vocabulary.
2021-11-12 15:03:48.000 model.type = sentence_level_classifier
2021-11-12 15:03:48.001 model.embedder.type = ref
2021-11-12 15:03:48.001 model.embedder.type = basic
2021-11-12 15:03:48.002 model.embedder.token_embedders.type = ref
2021-11-12 15:03:48.002 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:03:48.003 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:03:48.003 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:03:48.003 type = bert-base-cased
2021-11-12 15:03:48.003 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:03:48.003 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:03:48.003 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:03:48.003 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:03:48.003 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:03:48.004 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:03:48.004 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:03:48.004 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:03:48.004 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:03:48.004 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:03:48.004 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:03:48.053 model.encoder.type = bert_pooler
2021-11-12 15:03:48.053 model.encoder.type = bert_pooler
2021-11-12 15:03:48.053 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:03:48.054 type = bert-base-cased
2021-11-12 15:03:48.054 model.encoder.override_weights_file = None
2021-11-12 15:03:48.054 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:03:48.054 model.encoder.load_weights = True
2021-11-12 15:03:48.054 model.encoder.requires_grad = True
2021-11-12 15:03:48.054 model.encoder.dropout = 0.0
2021-11-12 15:03:48.054 model.encoder.transformer_kwargs = None
2021-11-12 15:03:48.242 removing temporary unarchived model dir at /tmp/tmp4wgcqrsv
2021-11-12 15:04:20.380 Plugin allennlp_models available
2021-11-12 15:04:20.384 Plugin allennlp_server available
2021-11-12 15:04:20.385 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:04:20.385 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpxe7goqpq
2021-11-12 15:04:22.876 dataset_reader.type = example_reader
2021-11-12 15:04:22.876 dataset_reader.max_instances = None
2021-11-12 15:04:22.876 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:04:22.877 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:04:22.877 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:04:22.877 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:04:22.877 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:04:22.877 type = bert-base-cased
2021-11-12 15:04:22.877 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:04:22.877 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:04:22.877 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:04:22.878 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:04:22.879 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:04:22.880 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:04:22.880 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:04:22.880 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:04:22.880 type = bert-base-cased
2021-11-12 15:04:22.880 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:04:22.880 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:04:22.880 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:04:22.881 dataset_reader.to_index = 6
2021-11-12 15:04:22.881 dataset_reader.type = example_reader
2021-11-12 15:04:22.882 dataset_reader.max_instances = None
2021-11-12 15:04:22.882 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:04:22.882 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:04:22.882 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:04:22.882 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:04:22.882 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:04:22.883 type = bert-base-cased
2021-11-12 15:04:22.883 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:04:22.883 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:04:22.883 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:04:22.884 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:04:22.884 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:04:22.884 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:04:22.885 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:04:22.885 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:04:22.885 type = bert-base-cased
2021-11-12 15:04:22.885 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:04:22.885 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:04:22.885 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:04:22.886 dataset_reader.to_index = 6
2021-11-12 15:04:22.886 type = from_instances
2021-11-12 15:04:22.887 Loading token dictionary from /tmp/tmpxe7goqpq/vocabulary.
2021-11-12 15:04:22.887 model.type = sentence_level_classifier
2021-11-12 15:04:22.888 model.embedder.type = ref
2021-11-12 15:04:22.888 model.embedder.type = basic
2021-11-12 15:04:22.889 model.embedder.token_embedders.type = ref
2021-11-12 15:04:22.890 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:04:22.891 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:04:22.891 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:04:22.891 type = bert-base-cased
2021-11-12 15:04:22.891 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:04:22.891 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:04:22.891 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:04:22.892 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:04:22.892 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:04:22.892 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:04:22.892 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:04:22.892 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:04:22.892 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:04:22.892 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:04:22.892 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:04:22.931 model.encoder.type = bert_pooler
2021-11-12 15:04:22.931 model.encoder.type = bert_pooler
2021-11-12 15:04:22.931 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:04:22.932 type = bert-base-cased
2021-11-12 15:04:22.932 model.encoder.override_weights_file = None
2021-11-12 15:04:22.932 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:04:22.932 model.encoder.load_weights = True
2021-11-12 15:04:22.932 model.encoder.requires_grad = True
2021-11-12 15:04:22.932 model.encoder.dropout = 0.0
2021-11-12 15:04:22.933 model.encoder.transformer_kwargs = None
2021-11-12 15:04:23.133 removing temporary unarchived model dir at /tmp/tmpxe7goqpq
2021-11-12 15:05:05.919 Plugin allennlp_models available
2021-11-12 15:05:05.929 Plugin allennlp_server available
2021-11-12 15:05:05.930 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:05:05.931 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpoldhakkf
2021-11-12 15:05:08.061 dataset_reader.type = example_reader
2021-11-12 15:05:08.061 dataset_reader.max_instances = None
2021-11-12 15:05:08.061 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:05:08.061 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:05:08.061 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:05:08.062 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:05:08.062 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:05:08.062 type = bert-base-cased
2021-11-12 15:05:08.062 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:05:08.062 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:05:08.063 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:05:08.064 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:05:08.066 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:05:08.067 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:05:08.068 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:05:08.068 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:05:08.068 type = bert-base-cased
2021-11-12 15:05:08.069 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:05:08.069 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:05:08.069 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:05:08.071 dataset_reader.to_index = 6
2021-11-12 15:05:08.071 dataset_reader.type = example_reader
2021-11-12 15:05:08.072 dataset_reader.max_instances = None
2021-11-12 15:05:08.072 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:05:08.073 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:05:08.073 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:05:08.073 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:05:08.074 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:05:08.074 type = bert-base-cased
2021-11-12 15:05:08.074 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:05:08.075 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:05:08.075 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:05:08.077 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:05:08.079 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:05:08.079 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:05:08.079 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:05:08.080 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:05:08.080 type = bert-base-cased
2021-11-12 15:05:08.080 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:05:08.081 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:05:08.081 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:05:08.082 dataset_reader.to_index = 6
2021-11-12 15:05:08.083 type = from_instances
2021-11-12 15:05:08.083 Loading token dictionary from /tmp/tmpoldhakkf/vocabulary.
2021-11-12 15:05:08.084 model.type = sentence_level_classifier
2021-11-12 15:05:08.085 model.embedder.type = ref
2021-11-12 15:05:08.086 model.embedder.type = basic
2021-11-12 15:05:08.087 model.embedder.token_embedders.type = ref
2021-11-12 15:05:08.090 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:05:08.091 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:05:08.091 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:05:08.092 type = bert-base-cased
2021-11-12 15:05:08.092 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:05:08.092 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:05:08.092 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:05:08.092 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:05:08.092 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:05:08.093 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:05:08.093 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:05:08.093 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:05:08.093 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:05:08.093 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:05:08.094 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:05:08.144 model.encoder.type = bert_pooler
2021-11-12 15:05:08.144 model.encoder.type = bert_pooler
2021-11-12 15:05:08.144 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:05:08.144 type = bert-base-cased
2021-11-12 15:05:08.144 model.encoder.override_weights_file = None
2021-11-12 15:05:08.144 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:05:08.145 model.encoder.load_weights = True
2021-11-12 15:05:08.145 model.encoder.requires_grad = True
2021-11-12 15:05:08.145 model.encoder.dropout = 0.0
2021-11-12 15:05:08.145 model.encoder.transformer_kwargs = None
2021-11-12 15:05:08.336 removing temporary unarchived model dir at /tmp/tmpoldhakkf
2021-11-12 15:09:40.327 Plugin allennlp_models available
2021-11-12 15:09:40.338 Plugin allennlp_server available
2021-11-12 15:09:40.338 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:09:40.339 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp61fw2pmr
2021-11-12 15:09:42.445 dataset_reader.type = example_reader
2021-11-12 15:09:42.445 dataset_reader.max_instances = None
2021-11-12 15:09:42.445 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:09:42.445 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:09:42.446 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:09:42.446 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:09:42.446 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:09:42.447 type = bert-base-cased
2021-11-12 15:09:42.447 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:09:42.447 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:09:42.447 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:09:42.448 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:09:42.449 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:09:42.449 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:09:42.449 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:09:42.450 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:09:42.450 type = bert-base-cased
2021-11-12 15:09:42.450 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:09:42.450 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:09:42.450 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:09:42.451 dataset_reader.to_index = 6
2021-11-12 15:09:42.451 dataset_reader.type = example_reader
2021-11-12 15:09:42.451 dataset_reader.max_instances = None
2021-11-12 15:09:42.451 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:09:42.451 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:09:42.451 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:09:42.452 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:09:42.452 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:09:42.452 type = bert-base-cased
2021-11-12 15:09:42.452 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:09:42.452 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:09:42.452 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:09:42.453 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:09:42.454 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:09:42.454 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:09:42.454 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:09:42.454 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:09:42.455 type = bert-base-cased
2021-11-12 15:09:42.455 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:09:42.455 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:09:42.455 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:09:42.456 dataset_reader.to_index = 6
2021-11-12 15:09:42.456 type = from_instances
2021-11-12 15:09:42.456 Loading token dictionary from /tmp/tmp61fw2pmr/vocabulary.
2021-11-12 15:09:42.457 model.type = sentence_level_classifier
2021-11-12 15:09:42.457 model.embedder.type = ref
2021-11-12 15:09:42.458 model.embedder.type = basic
2021-11-12 15:09:42.458 model.embedder.token_embedders.type = ref
2021-11-12 15:09:42.460 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:09:42.460 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:09:42.460 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:09:42.460 type = bert-base-cased
2021-11-12 15:09:42.460 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:09:42.461 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:09:42.461 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:09:42.461 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:09:42.461 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:09:42.461 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:09:42.461 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:09:42.461 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:09:42.461 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:09:42.461 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:09:42.461 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:09:42.509 model.encoder.type = bert_pooler
2021-11-12 15:09:42.509 model.encoder.type = bert_pooler
2021-11-12 15:09:42.509 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:09:42.509 type = bert-base-cased
2021-11-12 15:09:42.510 model.encoder.override_weights_file = None
2021-11-12 15:09:42.510 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:09:42.510 model.encoder.load_weights = True
2021-11-12 15:09:42.510 model.encoder.requires_grad = True
2021-11-12 15:09:42.510 model.encoder.dropout = 0.0
2021-11-12 15:09:42.510 model.encoder.transformer_kwargs = None
2021-11-12 15:09:42.700 removing temporary unarchived model dir at /tmp/tmp61fw2pmr
2021-11-12 15:09:58.862 Plugin allennlp_models available
2021-11-12 15:09:58.869 Plugin allennlp_server available
2021-11-12 15:09:58.870 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:09:58.870 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpn189wxtn
2021-11-12 15:10:00.974 dataset_reader.type = example_reader
2021-11-12 15:10:00.974 dataset_reader.max_instances = None
2021-11-12 15:10:00.974 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:10:00.974 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:10:00.974 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:10:00.974 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:10:00.975 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:10:00.975 type = bert-base-cased
2021-11-12 15:10:00.975 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:10:00.975 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:10:00.975 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:10:00.976 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:10:00.977 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:10:00.978 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:10:00.978 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:10:00.978 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:10:00.978 type = bert-base-cased
2021-11-12 15:10:00.978 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:10:00.978 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:10:00.978 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:10:00.979 dataset_reader.to_index = 6
2021-11-12 15:10:00.979 dataset_reader.type = example_reader
2021-11-12 15:10:00.979 dataset_reader.max_instances = None
2021-11-12 15:10:00.980 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:10:00.980 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:10:00.980 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:10:00.980 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:10:00.980 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:10:00.980 type = bert-base-cased
2021-11-12 15:10:00.980 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:10:00.981 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:10:00.981 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:10:00.981 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:10:00.982 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:10:00.982 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:10:00.983 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:10:00.983 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:10:00.983 type = bert-base-cased
2021-11-12 15:10:00.983 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:10:00.983 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:10:00.983 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:10:00.984 dataset_reader.to_index = 6
2021-11-12 15:10:00.984 type = from_instances
2021-11-12 15:10:00.984 Loading token dictionary from /tmp/tmpn189wxtn/vocabulary.
2021-11-12 15:10:00.985 model.type = sentence_level_classifier
2021-11-12 15:10:00.985 model.embedder.type = ref
2021-11-12 15:10:00.986 model.embedder.type = basic
2021-11-12 15:10:00.986 model.embedder.token_embedders.type = ref
2021-11-12 15:10:00.988 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:10:00.988 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:10:00.988 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:10:00.988 type = bert-base-cased
2021-11-12 15:10:00.989 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:10:00.989 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:10:00.989 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:10:00.989 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:10:00.989 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:10:00.989 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:10:00.989 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:10:00.989 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:10:00.989 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:10:00.989 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:10:00.989 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:10:01.039 model.encoder.type = bert_pooler
2021-11-12 15:10:01.040 model.encoder.type = bert_pooler
2021-11-12 15:10:01.040 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:10:01.040 type = bert-base-cased
2021-11-12 15:10:01.040 model.encoder.override_weights_file = None
2021-11-12 15:10:01.040 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:10:01.040 model.encoder.load_weights = True
2021-11-12 15:10:01.040 model.encoder.requires_grad = True
2021-11-12 15:10:01.041 model.encoder.dropout = 0.0
2021-11-12 15:10:01.041 model.encoder.transformer_kwargs = None
2021-11-12 15:10:01.242 removing temporary unarchived model dir at /tmp/tmpn189wxtn
2021-11-12 15:10:13.932 Plugin allennlp_models available
2021-11-12 15:10:13.939 Plugin allennlp_server available
2021-11-12 15:10:13.940 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:10:13.940 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp_6y9zape
2021-11-12 15:10:16.033 dataset_reader.type = example_reader
2021-11-12 15:10:16.033 dataset_reader.max_instances = None
2021-11-12 15:10:16.034 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:10:16.034 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:10:16.034 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:10:16.034 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:10:16.035 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:10:16.035 type = bert-base-cased
2021-11-12 15:10:16.035 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:10:16.035 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:10:16.035 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:10:16.036 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:10:16.037 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:10:16.037 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:10:16.038 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:10:16.038 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:10:16.038 type = bert-base-cased
2021-11-12 15:10:16.038 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:10:16.038 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:10:16.038 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:10:16.039 dataset_reader.to_index = 6
2021-11-12 15:10:16.039 dataset_reader.type = example_reader
2021-11-12 15:10:16.040 dataset_reader.max_instances = None
2021-11-12 15:10:16.040 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:10:16.040 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:10:16.040 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:10:16.040 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:10:16.040 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:10:16.040 type = bert-base-cased
2021-11-12 15:10:16.040 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:10:16.040 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:10:16.040 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:10:16.041 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:10:16.042 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:10:16.042 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:10:16.042 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:10:16.042 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:10:16.042 type = bert-base-cased
2021-11-12 15:10:16.042 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:10:16.042 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:10:16.043 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:10:16.043 dataset_reader.to_index = 6
2021-11-12 15:10:16.044 type = from_instances
2021-11-12 15:10:16.044 Loading token dictionary from /tmp/tmp_6y9zape/vocabulary.
2021-11-12 15:10:16.044 model.type = sentence_level_classifier
2021-11-12 15:10:16.044 model.embedder.type = ref
2021-11-12 15:10:16.045 model.embedder.type = basic
2021-11-12 15:10:16.046 model.embedder.token_embedders.type = ref
2021-11-12 15:10:16.047 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:10:16.047 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:10:16.048 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:10:16.048 type = bert-base-cased
2021-11-12 15:10:16.048 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:10:16.048 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:10:16.048 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:10:16.048 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:10:16.048 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:10:16.048 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:10:16.048 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:10:16.048 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:10:16.048 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:10:16.048 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:10:16.049 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:10:16.101 model.encoder.type = bert_pooler
2021-11-12 15:10:16.101 model.encoder.type = bert_pooler
2021-11-12 15:10:16.102 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:10:16.102 type = bert-base-cased
2021-11-12 15:10:16.102 model.encoder.override_weights_file = None
2021-11-12 15:10:16.102 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:10:16.102 model.encoder.load_weights = True
2021-11-12 15:10:16.103 model.encoder.requires_grad = True
2021-11-12 15:10:16.103 model.encoder.dropout = 0.0
2021-11-12 15:10:16.103 model.encoder.transformer_kwargs = None
2021-11-12 15:10:16.292 removing temporary unarchived model dir at /tmp/tmp_6y9zape
2021-11-12 15:10:29.199 Plugin allennlp_models available
2021-11-12 15:10:29.208 Plugin allennlp_server available
2021-11-12 15:10:29.209 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:10:29.210 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpxs4xblst
2021-11-12 15:10:31.355 dataset_reader.type = example_reader
2021-11-12 15:10:31.355 dataset_reader.max_instances = None
2021-11-12 15:10:31.355 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:10:31.355 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:10:31.356 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:10:31.356 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:10:31.356 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:10:31.356 type = bert-base-cased
2021-11-12 15:10:31.356 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:10:31.356 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:10:31.357 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:10:31.358 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:10:31.359 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:10:31.359 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:10:31.359 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:10:31.359 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:10:31.360 type = bert-base-cased
2021-11-12 15:10:31.360 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:10:31.360 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:10:31.360 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:10:31.361 dataset_reader.to_index = 6
2021-11-12 15:10:31.361 dataset_reader.type = example_reader
2021-11-12 15:10:31.362 dataset_reader.max_instances = None
2021-11-12 15:10:31.362 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:10:31.363 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:10:31.363 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:10:31.364 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:10:31.364 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:10:31.364 type = bert-base-cased
2021-11-12 15:10:31.365 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:10:31.365 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:10:31.366 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:10:31.367 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:10:31.369 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:10:31.370 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:10:31.370 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:10:31.370 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:10:31.371 type = bert-base-cased
2021-11-12 15:10:31.371 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:10:31.371 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:10:31.371 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:10:31.373 dataset_reader.to_index = 6
2021-11-12 15:10:31.373 type = from_instances
2021-11-12 15:10:31.374 Loading token dictionary from /tmp/tmpxs4xblst/vocabulary.
2021-11-12 15:10:31.375 model.type = sentence_level_classifier
2021-11-12 15:10:31.375 model.embedder.type = ref
2021-11-12 15:10:31.377 model.embedder.type = basic
2021-11-12 15:10:31.377 model.embedder.token_embedders.type = ref
2021-11-12 15:10:31.381 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:10:31.381 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:10:31.382 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:10:31.382 type = bert-base-cased
2021-11-12 15:10:31.382 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:10:31.382 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:10:31.383 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:10:31.383 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:10:31.383 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:10:31.383 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:10:31.383 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:10:31.383 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:10:31.384 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:10:31.384 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:10:31.384 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:10:31.443 model.encoder.type = bert_pooler
2021-11-12 15:10:31.443 model.encoder.type = bert_pooler
2021-11-12 15:10:31.443 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:10:31.444 type = bert-base-cased
2021-11-12 15:10:31.444 model.encoder.override_weights_file = None
2021-11-12 15:10:31.444 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:10:31.444 model.encoder.load_weights = True
2021-11-12 15:10:31.444 model.encoder.requires_grad = True
2021-11-12 15:10:31.444 model.encoder.dropout = 0.0
2021-11-12 15:10:31.445 model.encoder.transformer_kwargs = None
2021-11-12 15:10:31.637 removing temporary unarchived model dir at /tmp/tmpxs4xblst
2021-11-12 15:10:49.637 Plugin allennlp_models available
2021-11-12 15:10:49.648 Plugin allennlp_server available
2021-11-12 15:10:49.649 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:10:49.649 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp7doswzrq
2021-11-12 15:10:51.753 dataset_reader.type = example_reader
2021-11-12 15:10:51.754 dataset_reader.max_instances = None
2021-11-12 15:10:51.754 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:10:51.754 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:10:51.754 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:10:51.754 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:10:51.754 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:10:51.754 type = bert-base-cased
2021-11-12 15:10:51.755 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:10:51.755 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:10:51.755 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:10:51.756 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:10:51.757 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:10:51.757 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:10:51.757 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:10:51.758 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:10:51.758 type = bert-base-cased
2021-11-12 15:10:51.758 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:10:51.758 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:10:51.758 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:10:51.759 dataset_reader.to_index = 6
2021-11-12 15:10:51.759 dataset_reader.type = example_reader
2021-11-12 15:10:51.759 dataset_reader.max_instances = None
2021-11-12 15:10:51.759 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:10:51.759 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:10:51.759 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:10:51.760 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:10:51.760 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:10:51.760 type = bert-base-cased
2021-11-12 15:10:51.760 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:10:51.760 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:10:51.760 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:10:51.761 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:10:51.761 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:10:51.761 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:10:51.762 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:10:51.762 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:10:51.762 type = bert-base-cased
2021-11-12 15:10:51.762 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:10:51.762 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:10:51.762 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:10:51.763 dataset_reader.to_index = 6
2021-11-12 15:10:51.763 type = from_instances
2021-11-12 15:10:51.763 Loading token dictionary from /tmp/tmp7doswzrq/vocabulary.
2021-11-12 15:10:51.764 model.type = sentence_level_classifier
2021-11-12 15:10:51.764 model.embedder.type = ref
2021-11-12 15:10:51.765 model.embedder.type = basic
2021-11-12 15:10:51.765 model.embedder.token_embedders.type = ref
2021-11-12 15:10:51.767 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:10:51.767 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:10:51.767 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:10:51.767 type = bert-base-cased
2021-11-12 15:10:51.768 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:10:51.768 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:10:51.768 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:10:51.768 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:10:51.768 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:10:51.768 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:10:51.768 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:10:51.768 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:10:51.768 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:10:51.768 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:10:51.768 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:10:51.817 model.encoder.type = bert_pooler
2021-11-12 15:10:51.818 model.encoder.type = bert_pooler
2021-11-12 15:10:51.818 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:10:51.818 type = bert-base-cased
2021-11-12 15:10:51.818 model.encoder.override_weights_file = None
2021-11-12 15:10:51.818 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:10:51.819 model.encoder.load_weights = True
2021-11-12 15:10:51.819 model.encoder.requires_grad = True
2021-11-12 15:10:51.819 model.encoder.dropout = 0.0
2021-11-12 15:10:51.819 model.encoder.transformer_kwargs = None
2021-11-12 15:10:52.008 removing temporary unarchived model dir at /tmp/tmp7doswzrq
2021-11-12 15:11:01.222 Plugin allennlp_models available
2021-11-12 15:11:01.230 Plugin allennlp_server available
2021-11-12 15:11:01.231 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:11:01.232 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpon117qim
2021-11-12 15:11:03.334 dataset_reader.type = example_reader
2021-11-12 15:11:03.335 dataset_reader.max_instances = None
2021-11-12 15:11:03.335 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:11:03.335 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:11:03.335 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:11:03.335 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:11:03.336 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:11:03.336 type = bert-base-cased
2021-11-12 15:11:03.336 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:11:03.336 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:11:03.336 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:11:03.337 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:11:03.338 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:11:03.338 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:11:03.338 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:11:03.339 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:11:03.339 type = bert-base-cased
2021-11-12 15:11:03.339 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:11:03.339 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:11:03.339 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:11:03.340 dataset_reader.to_index = 6
2021-11-12 15:11:03.340 dataset_reader.type = example_reader
2021-11-12 15:11:03.340 dataset_reader.max_instances = None
2021-11-12 15:11:03.340 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:11:03.340 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:11:03.341 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:11:03.341 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:11:03.341 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:11:03.341 type = bert-base-cased
2021-11-12 15:11:03.341 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:11:03.341 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:11:03.341 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:11:03.342 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:11:03.343 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:11:03.343 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:11:03.343 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:11:03.344 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:11:03.344 type = bert-base-cased
2021-11-12 15:11:03.344 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:11:03.344 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:11:03.344 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:11:03.345 dataset_reader.to_index = 6
2021-11-12 15:11:03.345 type = from_instances
2021-11-12 15:11:03.345 Loading token dictionary from /tmp/tmpon117qim/vocabulary.
2021-11-12 15:11:03.346 model.type = sentence_level_classifier
2021-11-12 15:11:03.346 model.embedder.type = ref
2021-11-12 15:11:03.346 model.embedder.type = basic
2021-11-12 15:11:03.347 model.embedder.token_embedders.type = ref
2021-11-12 15:11:03.348 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:11:03.349 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:11:03.349 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:11:03.349 type = bert-base-cased
2021-11-12 15:11:03.349 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:11:03.349 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:11:03.349 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:11:03.349 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:11:03.350 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:11:03.350 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:11:03.350 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:11:03.350 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:11:03.350 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:11:03.350 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:11:03.350 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:11:03.397 model.encoder.type = bert_pooler
2021-11-12 15:11:03.397 model.encoder.type = bert_pooler
2021-11-12 15:11:03.398 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:11:03.398 type = bert-base-cased
2021-11-12 15:11:03.398 model.encoder.override_weights_file = None
2021-11-12 15:11:03.398 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:11:03.398 model.encoder.load_weights = True
2021-11-12 15:11:03.398 model.encoder.requires_grad = True
2021-11-12 15:11:03.398 model.encoder.dropout = 0.0
2021-11-12 15:11:03.398 model.encoder.transformer_kwargs = None
2021-11-12 15:11:03.588 removing temporary unarchived model dir at /tmp/tmpon117qim
2021-11-12 15:11:10.032 Plugin allennlp_models available
2021-11-12 15:11:10.040 Plugin allennlp_server available
2021-11-12 15:11:10.041 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:11:10.042 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpo9suw487
2021-11-12 15:11:12.153 dataset_reader.type = example_reader
2021-11-12 15:11:12.154 dataset_reader.max_instances = None
2021-11-12 15:11:12.154 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:11:12.154 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:11:12.154 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:11:12.154 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:11:12.154 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:11:12.155 type = bert-base-cased
2021-11-12 15:11:12.155 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:11:12.155 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:11:12.155 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:11:12.156 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:11:12.157 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:11:12.157 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:11:12.157 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:11:12.158 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:11:12.158 type = bert-base-cased
2021-11-12 15:11:12.158 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:11:12.158 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:11:12.158 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:11:12.159 dataset_reader.to_index = 6
2021-11-12 15:11:12.159 dataset_reader.type = example_reader
2021-11-12 15:11:12.160 dataset_reader.max_instances = None
2021-11-12 15:11:12.160 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:11:12.160 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:11:12.160 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:11:12.160 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:11:12.161 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:11:12.161 type = bert-base-cased
2021-11-12 15:11:12.161 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:11:12.162 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:11:12.162 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:11:12.164 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:11:12.166 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:11:12.166 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:11:12.167 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:11:12.167 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:11:12.167 type = bert-base-cased
2021-11-12 15:11:12.168 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:11:12.168 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:11:12.168 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:11:12.170 dataset_reader.to_index = 6
2021-11-12 15:11:12.170 type = from_instances
2021-11-12 15:11:12.171 Loading token dictionary from /tmp/tmpo9suw487/vocabulary.
2021-11-12 15:11:12.171 model.type = sentence_level_classifier
2021-11-12 15:11:12.172 model.embedder.type = ref
2021-11-12 15:11:12.174 model.embedder.type = basic
2021-11-12 15:11:12.174 model.embedder.token_embedders.type = ref
2021-11-12 15:11:12.178 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:11:12.178 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:11:12.179 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:11:12.179 type = bert-base-cased
2021-11-12 15:11:12.179 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:11:12.180 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:11:12.180 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:11:12.180 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:11:12.180 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:11:12.181 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:11:12.181 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:11:12.181 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:11:12.181 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:11:12.181 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:11:12.181 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:11:12.232 model.encoder.type = bert_pooler
2021-11-12 15:11:12.232 model.encoder.type = bert_pooler
2021-11-12 15:11:12.232 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:11:12.232 type = bert-base-cased
2021-11-12 15:11:12.233 model.encoder.override_weights_file = None
2021-11-12 15:11:12.233 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:11:12.233 model.encoder.load_weights = True
2021-11-12 15:11:12.233 model.encoder.requires_grad = True
2021-11-12 15:11:12.233 model.encoder.dropout = 0.0
2021-11-12 15:11:12.233 model.encoder.transformer_kwargs = None
2021-11-12 15:11:12.431 removing temporary unarchived model dir at /tmp/tmpo9suw487
2021-11-12 15:11:31.042 Plugin allennlp_models available
2021-11-12 15:11:31.051 Plugin allennlp_server available
2021-11-12 15:11:31.051 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:11:31.052 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp24d0ryhp
2021-11-12 15:11:33.158 dataset_reader.type = example_reader
2021-11-12 15:11:33.159 dataset_reader.max_instances = None
2021-11-12 15:11:33.159 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:11:33.159 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:11:33.159 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:11:33.159 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:11:33.159 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:11:33.159 type = bert-base-cased
2021-11-12 15:11:33.159 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:11:33.159 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:11:33.160 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:11:33.161 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:11:33.162 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:11:33.162 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:11:33.162 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:11:33.162 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:11:33.163 type = bert-base-cased
2021-11-12 15:11:33.163 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:11:33.163 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:11:33.163 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:11:33.164 dataset_reader.to_index = 6
2021-11-12 15:11:33.164 dataset_reader.type = example_reader
2021-11-12 15:11:33.164 dataset_reader.max_instances = None
2021-11-12 15:11:33.164 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:11:33.164 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:11:33.164 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:11:33.164 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:11:33.165 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:11:33.165 type = bert-base-cased
2021-11-12 15:11:33.165 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:11:33.165 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:11:33.165 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:11:33.166 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:11:33.166 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:11:33.167 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:11:33.167 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:11:33.167 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:11:33.167 type = bert-base-cased
2021-11-12 15:11:33.168 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:11:33.168 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:11:33.168 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:11:33.169 dataset_reader.to_index = 6
2021-11-12 15:11:33.170 type = from_instances
2021-11-12 15:11:33.170 Loading token dictionary from /tmp/tmp24d0ryhp/vocabulary.
2021-11-12 15:11:33.171 model.type = sentence_level_classifier
2021-11-12 15:11:33.172 model.embedder.type = ref
2021-11-12 15:11:33.173 model.embedder.type = basic
2021-11-12 15:11:33.174 model.embedder.token_embedders.type = ref
2021-11-12 15:11:33.178 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:11:33.178 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:11:33.179 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:11:33.179 type = bert-base-cased
2021-11-12 15:11:33.180 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:11:33.180 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:11:33.180 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:11:33.180 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:11:33.181 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:11:33.181 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:11:33.181 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:11:33.181 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:11:33.181 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:11:33.182 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:11:33.182 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:11:33.234 model.encoder.type = bert_pooler
2021-11-12 15:11:33.234 model.encoder.type = bert_pooler
2021-11-12 15:11:33.235 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:11:33.235 type = bert-base-cased
2021-11-12 15:11:33.235 model.encoder.override_weights_file = None
2021-11-12 15:11:33.235 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:11:33.235 model.encoder.load_weights = True
2021-11-12 15:11:33.235 model.encoder.requires_grad = True
2021-11-12 15:11:33.236 model.encoder.dropout = 0.0
2021-11-12 15:11:33.236 model.encoder.transformer_kwargs = None
2021-11-12 15:11:33.429 removing temporary unarchived model dir at /tmp/tmp24d0ryhp
2021-11-12 15:12:11.835 Plugin allennlp_models available
2021-11-12 15:12:11.844 Plugin allennlp_server available
2021-11-12 15:12:11.845 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:12:11.846 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpkl4xaza3
2021-11-12 15:12:13.942 dataset_reader.type = example_reader
2021-11-12 15:12:13.942 dataset_reader.max_instances = None
2021-11-12 15:12:13.942 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:12:13.942 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:12:13.943 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:12:13.943 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:12:13.943 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:12:13.943 type = bert-base-cased
2021-11-12 15:12:13.944 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:12:13.944 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:12:13.944 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:12:13.945 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:12:13.946 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:12:13.946 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:12:13.946 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:12:13.946 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:12:13.947 type = bert-base-cased
2021-11-12 15:12:13.947 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:12:13.947 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:12:13.947 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:12:13.948 dataset_reader.to_index = 6
2021-11-12 15:12:13.948 dataset_reader.type = example_reader
2021-11-12 15:12:13.948 dataset_reader.max_instances = None
2021-11-12 15:12:13.948 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:12:13.948 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:12:13.948 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:12:13.949 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:12:13.949 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:12:13.949 type = bert-base-cased
2021-11-12 15:12:13.949 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:12:13.949 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:12:13.949 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:12:13.950 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:12:13.951 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:12:13.951 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:12:13.951 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:12:13.951 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:12:13.952 type = bert-base-cased
2021-11-12 15:12:13.952 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:12:13.952 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:12:13.952 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:12:13.953 dataset_reader.to_index = 6
2021-11-12 15:12:13.953 type = from_instances
2021-11-12 15:12:13.953 Loading token dictionary from /tmp/tmpkl4xaza3/vocabulary.
2021-11-12 15:12:13.954 model.type = sentence_level_classifier
2021-11-12 15:12:13.954 model.embedder.type = ref
2021-11-12 15:12:13.954 model.embedder.type = basic
2021-11-12 15:12:13.955 model.embedder.token_embedders.type = ref
2021-11-12 15:12:13.957 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:12:13.957 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:12:13.957 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:12:13.957 type = bert-base-cased
2021-11-12 15:12:13.957 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:12:13.957 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:12:13.958 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:12:13.958 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:12:13.958 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:12:13.958 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:12:13.958 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:12:13.958 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:12:13.958 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:12:13.958 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:12:13.958 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:12:14.007 model.encoder.type = bert_pooler
2021-11-12 15:12:14.008 model.encoder.type = bert_pooler
2021-11-12 15:12:14.008 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:12:14.008 type = bert-base-cased
2021-11-12 15:12:14.008 model.encoder.override_weights_file = None
2021-11-12 15:12:14.008 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:12:14.008 model.encoder.load_weights = True
2021-11-12 15:12:14.008 model.encoder.requires_grad = True
2021-11-12 15:12:14.009 model.encoder.dropout = 0.0
2021-11-12 15:12:14.009 model.encoder.transformer_kwargs = None
2021-11-12 15:12:14.199 removing temporary unarchived model dir at /tmp/tmpkl4xaza3
2021-11-12 15:12:46.181 Plugin allennlp_models available
2021-11-12 15:12:46.188 Plugin allennlp_server available
2021-11-12 15:12:46.189 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:12:46.189 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpnaoqeg_q
2021-11-12 15:12:48.284 dataset_reader.type = example_reader
2021-11-12 15:12:48.284 dataset_reader.max_instances = None
2021-11-12 15:12:48.284 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:12:48.292 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:12:48.293 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:12:48.293 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:12:48.293 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:12:48.293 type = bert-base-cased
2021-11-12 15:12:48.294 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:12:48.294 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:12:48.294 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:12:48.295 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:12:48.296 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:12:48.296 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:12:48.296 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:12:48.297 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:12:48.297 type = bert-base-cased
2021-11-12 15:12:48.297 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:12:48.297 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:12:48.297 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:12:48.298 dataset_reader.to_index = 6
2021-11-12 15:12:48.298 dataset_reader.type = example_reader
2021-11-12 15:12:48.298 dataset_reader.max_instances = None
2021-11-12 15:12:48.299 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:12:48.299 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:12:48.299 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:12:48.299 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:12:48.299 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:12:48.299 type = bert-base-cased
2021-11-12 15:12:48.300 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:12:48.300 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:12:48.300 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:12:48.301 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:12:48.301 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:12:48.302 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:12:48.302 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:12:48.302 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:12:48.302 type = bert-base-cased
2021-11-12 15:12:48.302 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:12:48.302 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:12:48.303 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:12:48.303 dataset_reader.to_index = 6
2021-11-12 15:12:48.304 type = from_instances
2021-11-12 15:12:48.304 Loading token dictionary from /tmp/tmpnaoqeg_q/vocabulary.
2021-11-12 15:12:48.304 model.type = sentence_level_classifier
2021-11-12 15:12:48.304 model.embedder.type = ref
2021-11-12 15:12:48.305 model.embedder.type = basic
2021-11-12 15:12:48.305 model.embedder.token_embedders.type = ref
2021-11-12 15:12:48.307 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:12:48.307 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:12:48.307 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:12:48.308 type = bert-base-cased
2021-11-12 15:12:48.308 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:12:48.308 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:12:48.308 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:12:48.308 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:12:48.308 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:12:48.308 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:12:48.308 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:12:48.308 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:12:48.308 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:12:48.308 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:12:48.308 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:12:48.357 model.encoder.type = bert_pooler
2021-11-12 15:12:48.358 model.encoder.type = bert_pooler
2021-11-12 15:12:48.358 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:12:48.359 type = bert-base-cased
2021-11-12 15:12:48.359 model.encoder.override_weights_file = None
2021-11-12 15:12:48.359 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:12:48.359 model.encoder.load_weights = True
2021-11-12 15:12:48.359 model.encoder.requires_grad = True
2021-11-12 15:12:48.359 model.encoder.dropout = 0.0
2021-11-12 15:12:48.359 model.encoder.transformer_kwargs = None
2021-11-12 15:12:48.549 removing temporary unarchived model dir at /tmp/tmpnaoqeg_q
2021-11-12 15:13:00.610 Plugin allennlp_models available
2021-11-12 15:13:00.616 Plugin allennlp_server available
2021-11-12 15:13:00.616 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:13:00.617 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpht_g4uo7
2021-11-12 15:13:02.727 dataset_reader.type = example_reader
2021-11-12 15:13:02.727 dataset_reader.max_instances = None
2021-11-12 15:13:02.727 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:13:02.727 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:13:02.728 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:13:02.728 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:13:02.728 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:13:02.728 type = bert-base-cased
2021-11-12 15:13:02.728 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:13:02.729 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:13:02.729 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:13:02.730 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:13:02.731 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:13:02.731 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:13:02.731 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:13:02.731 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:13:02.731 type = bert-base-cased
2021-11-12 15:13:02.731 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:13:02.731 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:13:02.732 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:13:02.732 dataset_reader.to_index = 6
2021-11-12 15:13:02.733 dataset_reader.type = example_reader
2021-11-12 15:13:02.733 dataset_reader.max_instances = None
2021-11-12 15:13:02.733 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:13:02.733 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:13:02.733 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:13:02.734 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:13:02.734 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:13:02.734 type = bert-base-cased
2021-11-12 15:13:02.734 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:13:02.734 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:13:02.734 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:13:02.735 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:13:02.735 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:13:02.735 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:13:02.736 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:13:02.736 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:13:02.736 type = bert-base-cased
2021-11-12 15:13:02.736 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:13:02.736 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:13:02.736 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:13:02.737 dataset_reader.to_index = 6
2021-11-12 15:13:02.737 type = from_instances
2021-11-12 15:13:02.738 Loading token dictionary from /tmp/tmpht_g4uo7/vocabulary.
2021-11-12 15:13:02.738 model.type = sentence_level_classifier
2021-11-12 15:13:02.738 model.embedder.type = ref
2021-11-12 15:13:02.739 model.embedder.type = basic
2021-11-12 15:13:02.739 model.embedder.token_embedders.type = ref
2021-11-12 15:13:02.741 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:13:02.741 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:13:02.741 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:13:02.741 type = bert-base-cased
2021-11-12 15:13:02.742 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:13:02.742 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:13:02.742 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:13:02.742 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:13:02.742 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:13:02.742 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:13:02.743 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:13:02.743 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:13:02.743 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:13:02.743 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:13:02.743 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:13:02.790 model.encoder.type = bert_pooler
2021-11-12 15:13:02.790 model.encoder.type = bert_pooler
2021-11-12 15:13:02.790 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:13:02.791 type = bert-base-cased
2021-11-12 15:13:02.791 model.encoder.override_weights_file = None
2021-11-12 15:13:02.791 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:13:02.791 model.encoder.load_weights = True
2021-11-12 15:13:02.791 model.encoder.requires_grad = True
2021-11-12 15:13:02.791 model.encoder.dropout = 0.0
2021-11-12 15:13:02.791 model.encoder.transformer_kwargs = None
2021-11-12 15:13:02.980 removing temporary unarchived model dir at /tmp/tmpht_g4uo7
2021-11-12 15:13:34.913 Plugin allennlp_models available
2021-11-12 15:13:34.919 Plugin allennlp_server available
2021-11-12 15:13:34.920 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:13:34.920 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpnpey2hz1
2021-11-12 15:13:37.015 dataset_reader.type = example_reader
2021-11-12 15:13:37.015 dataset_reader.max_instances = None
2021-11-12 15:13:37.015 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:13:37.015 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:13:37.015 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:13:37.016 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:13:37.016 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:13:37.017 type = bert-base-cased
2021-11-12 15:13:37.017 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:13:37.018 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:13:37.018 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:13:37.020 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:13:37.022 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:13:37.022 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:13:37.023 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:13:37.023 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:13:37.024 type = bert-base-cased
2021-11-12 15:13:37.024 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:13:37.024 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:13:37.025 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:13:37.026 dataset_reader.to_index = 6
2021-11-12 15:13:37.027 dataset_reader.type = example_reader
2021-11-12 15:13:37.027 dataset_reader.max_instances = None
2021-11-12 15:13:37.027 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:13:37.028 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:13:37.028 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:13:37.029 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:13:37.029 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:13:37.029 type = bert-base-cased
2021-11-12 15:13:37.029 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:13:37.030 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:13:37.030 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:13:37.031 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:13:37.033 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:13:37.033 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:13:37.034 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:13:37.034 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:13:37.034 type = bert-base-cased
2021-11-12 15:13:37.035 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:13:37.035 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:13:37.035 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:13:37.036 dataset_reader.to_index = 6
2021-11-12 15:13:37.037 type = from_instances
2021-11-12 15:13:37.037 Loading token dictionary from /tmp/tmpnpey2hz1/vocabulary.
2021-11-12 15:13:37.038 model.type = sentence_level_classifier
2021-11-12 15:13:37.038 model.embedder.type = ref
2021-11-12 15:13:37.040 model.embedder.type = basic
2021-11-12 15:13:37.040 model.embedder.token_embedders.type = ref
2021-11-12 15:13:37.043 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:13:37.043 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:13:37.044 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:13:37.044 type = bert-base-cased
2021-11-12 15:13:37.044 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:13:37.044 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:13:37.045 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:13:37.045 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:13:37.045 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:13:37.045 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:13:37.046 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:13:37.046 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:13:37.046 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:13:37.046 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:13:37.046 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:13:37.093 model.encoder.type = bert_pooler
2021-11-12 15:13:37.093 model.encoder.type = bert_pooler
2021-11-12 15:13:37.093 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:13:37.094 type = bert-base-cased
2021-11-12 15:13:37.094 model.encoder.override_weights_file = None
2021-11-12 15:13:37.094 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:13:37.094 model.encoder.load_weights = True
2021-11-12 15:13:37.095 model.encoder.requires_grad = True
2021-11-12 15:13:37.095 model.encoder.dropout = 0.0
2021-11-12 15:13:37.095 model.encoder.transformer_kwargs = None
2021-11-12 15:13:37.286 removing temporary unarchived model dir at /tmp/tmpnpey2hz1
2021-11-12 15:14:45.063 Plugin allennlp_models available
2021-11-12 15:14:45.075 Plugin allennlp_server available
2021-11-12 15:14:45.076 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:14:45.076 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpe8_3fi5r
2021-11-12 15:14:47.206 dataset_reader.type = example_reader
2021-11-12 15:14:47.207 dataset_reader.max_instances = None
2021-11-12 15:14:47.207 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:14:47.207 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:14:47.207 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:14:47.207 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:14:47.208 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:14:47.208 type = bert-base-cased
2021-11-12 15:14:47.208 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:14:47.208 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:14:47.208 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:14:47.209 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:14:47.210 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:14:47.210 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:14:47.211 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:14:47.211 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:14:47.211 type = bert-base-cased
2021-11-12 15:14:47.211 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:14:47.211 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:14:47.211 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:14:47.212 dataset_reader.to_index = 6
2021-11-12 15:14:47.212 dataset_reader.type = example_reader
2021-11-12 15:14:47.212 dataset_reader.max_instances = None
2021-11-12 15:14:47.212 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:14:47.212 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:14:47.212 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:14:47.213 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:14:47.213 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:14:47.213 type = bert-base-cased
2021-11-12 15:14:47.213 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:14:47.213 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:14:47.213 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:14:47.214 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:14:47.214 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:14:47.214 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:14:47.215 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:14:47.215 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:14:47.215 type = bert-base-cased
2021-11-12 15:14:47.215 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:14:47.215 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:14:47.215 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:14:47.216 dataset_reader.to_index = 6
2021-11-12 15:14:47.216 type = from_instances
2021-11-12 15:14:47.216 Loading token dictionary from /tmp/tmpe8_3fi5r/vocabulary.
2021-11-12 15:14:47.217 model.type = sentence_level_classifier
2021-11-12 15:14:47.217 model.embedder.type = ref
2021-11-12 15:14:47.218 model.embedder.type = basic
2021-11-12 15:14:47.218 model.embedder.token_embedders.type = ref
2021-11-12 15:14:47.219 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:14:47.220 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:14:47.220 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:14:47.220 type = bert-base-cased
2021-11-12 15:14:47.220 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:14:47.220 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:14:47.220 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:14:47.220 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:14:47.220 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:14:47.220 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:14:47.220 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:14:47.220 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:14:47.220 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:14:47.220 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:14:47.220 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:14:47.269 model.encoder.type = bert_pooler
2021-11-12 15:14:47.269 model.encoder.type = bert_pooler
2021-11-12 15:14:47.269 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:14:47.270 type = bert-base-cased
2021-11-12 15:14:47.270 model.encoder.override_weights_file = None
2021-11-12 15:14:47.270 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:14:47.270 model.encoder.load_weights = True
2021-11-12 15:14:47.270 model.encoder.requires_grad = True
2021-11-12 15:14:47.271 model.encoder.dropout = 0.0
2021-11-12 15:14:47.271 model.encoder.transformer_kwargs = None
2021-11-12 15:14:47.501 removing temporary unarchived model dir at /tmp/tmpe8_3fi5r
2021-11-12 15:15:21.002 Plugin allennlp_models available
2021-11-12 15:15:21.008 Plugin allennlp_server available
2021-11-12 15:15:21.008 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:15:21.009 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmprb78_dai
2021-11-12 15:15:23.138 dataset_reader.type = example_reader
2021-11-12 15:15:23.139 dataset_reader.max_instances = None
2021-11-12 15:15:23.139 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:15:23.139 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:15:23.139 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:15:23.139 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:15:23.140 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:15:23.140 type = bert-base-cased
2021-11-12 15:15:23.140 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:15:23.140 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:15:23.140 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:15:23.141 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:15:23.142 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:15:23.142 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:15:23.143 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:15:23.143 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:15:23.143 type = bert-base-cased
2021-11-12 15:15:23.143 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:15:23.143 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:15:23.144 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:15:23.144 dataset_reader.to_index = 6
2021-11-12 15:15:23.145 dataset_reader.type = example_reader
2021-11-12 15:15:23.145 dataset_reader.max_instances = None
2021-11-12 15:15:23.145 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:15:23.145 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:15:23.145 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:15:23.145 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:15:23.146 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:15:23.146 type = bert-base-cased
2021-11-12 15:15:23.146 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:15:23.146 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:15:23.146 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:15:23.147 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:15:23.148 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:15:23.148 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:15:23.149 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:15:23.149 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:15:23.150 type = bert-base-cased
2021-11-12 15:15:23.150 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:15:23.150 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:15:23.150 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:15:23.152 dataset_reader.to_index = 6
2021-11-12 15:15:23.152 type = from_instances
2021-11-12 15:15:23.153 Loading token dictionary from /tmp/tmprb78_dai/vocabulary.
2021-11-12 15:15:23.154 model.type = sentence_level_classifier
2021-11-12 15:15:23.154 model.embedder.type = ref
2021-11-12 15:15:23.156 model.embedder.type = basic
2021-11-12 15:15:23.157 model.embedder.token_embedders.type = ref
2021-11-12 15:15:23.161 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:15:23.161 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:15:23.162 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:15:23.162 type = bert-base-cased
2021-11-12 15:15:23.162 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:15:23.163 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:15:23.163 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:15:23.163 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:15:23.163 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:15:23.163 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:15:23.164 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:15:23.164 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:15:23.164 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:15:23.164 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:15:23.164 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:15:23.215 model.encoder.type = bert_pooler
2021-11-12 15:15:23.215 model.encoder.type = bert_pooler
2021-11-12 15:15:23.215 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:15:23.215 type = bert-base-cased
2021-11-12 15:15:23.215 model.encoder.override_weights_file = None
2021-11-12 15:15:23.216 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:15:23.216 model.encoder.load_weights = True
2021-11-12 15:15:23.216 model.encoder.requires_grad = True
2021-11-12 15:15:23.216 model.encoder.dropout = 0.0
2021-11-12 15:15:23.216 model.encoder.transformer_kwargs = None
2021-11-12 15:15:23.406 removing temporary unarchived model dir at /tmp/tmprb78_dai
2021-11-12 15:15:37.369 Plugin allennlp_models available
2021-11-12 15:15:37.374 Plugin allennlp_server available
2021-11-12 15:15:37.374 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:15:37.375 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp10qm8x5o
2021-11-12 15:15:39.466 dataset_reader.type = example_reader
2021-11-12 15:15:39.467 dataset_reader.max_instances = None
2021-11-12 15:15:39.467 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:15:39.467 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:15:39.467 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:15:39.467 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:15:39.467 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:15:39.468 type = bert-base-cased
2021-11-12 15:15:39.468 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:15:39.468 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:15:39.468 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:15:39.469 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:15:39.470 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:15:39.470 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:15:39.471 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:15:39.471 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:15:39.471 type = bert-base-cased
2021-11-12 15:15:39.471 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:15:39.471 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:15:39.471 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:15:39.472 dataset_reader.to_index = 6
2021-11-12 15:15:39.472 dataset_reader.type = example_reader
2021-11-12 15:15:39.472 dataset_reader.max_instances = None
2021-11-12 15:15:39.472 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:15:39.473 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:15:39.473 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:15:39.473 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:15:39.473 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:15:39.474 type = bert-base-cased
2021-11-12 15:15:39.474 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:15:39.474 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:15:39.474 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:15:39.475 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:15:39.475 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:15:39.476 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:15:39.476 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:15:39.476 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:15:39.476 type = bert-base-cased
2021-11-12 15:15:39.476 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:15:39.476 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:15:39.477 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:15:39.477 dataset_reader.to_index = 6
2021-11-12 15:15:39.478 type = from_instances
2021-11-12 15:15:39.478 Loading token dictionary from /tmp/tmp10qm8x5o/vocabulary.
2021-11-12 15:15:39.478 model.type = sentence_level_classifier
2021-11-12 15:15:39.478 model.embedder.type = ref
2021-11-12 15:15:39.479 model.embedder.type = basic
2021-11-12 15:15:39.479 model.embedder.token_embedders.type = ref
2021-11-12 15:15:39.481 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:15:39.481 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:15:39.482 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:15:39.482 type = bert-base-cased
2021-11-12 15:15:39.482 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:15:39.482 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:15:39.482 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:15:39.482 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:15:39.482 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:15:39.482 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:15:39.482 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:15:39.482 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:15:39.482 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:15:39.482 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:15:39.482 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:15:39.530 model.encoder.type = bert_pooler
2021-11-12 15:15:39.531 model.encoder.type = bert_pooler
2021-11-12 15:15:39.531 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:15:39.531 type = bert-base-cased
2021-11-12 15:15:39.532 model.encoder.override_weights_file = None
2021-11-12 15:15:39.532 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:15:39.532 model.encoder.load_weights = True
2021-11-12 15:15:39.532 model.encoder.requires_grad = True
2021-11-12 15:15:39.532 model.encoder.dropout = 0.0
2021-11-12 15:15:39.533 model.encoder.transformer_kwargs = None
2021-11-12 15:15:39.727 removing temporary unarchived model dir at /tmp/tmp10qm8x5o
2021-11-12 15:15:48.057 Plugin allennlp_models available
2021-11-12 15:15:48.064 Plugin allennlp_server available
2021-11-12 15:15:48.065 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:15:48.065 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpddu1d4md
2021-11-12 15:15:50.165 dataset_reader.type = example_reader
2021-11-12 15:15:50.165 dataset_reader.max_instances = None
2021-11-12 15:15:50.165 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:15:50.165 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:15:50.165 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:15:50.166 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:15:50.166 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:15:50.166 type = bert-base-cased
2021-11-12 15:15:50.166 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:15:50.166 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:15:50.167 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:15:50.168 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:15:50.169 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:15:50.169 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:15:50.169 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:15:50.169 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:15:50.170 type = bert-base-cased
2021-11-12 15:15:50.170 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:15:50.170 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:15:50.170 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:15:50.171 dataset_reader.to_index = 6
2021-11-12 15:15:50.171 dataset_reader.type = example_reader
2021-11-12 15:15:50.171 dataset_reader.max_instances = None
2021-11-12 15:15:50.171 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:15:50.171 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:15:50.172 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:15:50.172 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:15:50.172 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:15:50.172 type = bert-base-cased
2021-11-12 15:15:50.172 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:15:50.172 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:15:50.172 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:15:50.173 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:15:50.174 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:15:50.174 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:15:50.174 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:15:50.175 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:15:50.175 type = bert-base-cased
2021-11-12 15:15:50.175 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:15:50.175 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:15:50.175 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:15:50.176 dataset_reader.to_index = 6
2021-11-12 15:15:50.176 type = from_instances
2021-11-12 15:15:50.176 Loading token dictionary from /tmp/tmpddu1d4md/vocabulary.
2021-11-12 15:15:50.177 model.type = sentence_level_classifier
2021-11-12 15:15:50.177 model.embedder.type = ref
2021-11-12 15:15:50.178 model.embedder.type = basic
2021-11-12 15:15:50.178 model.embedder.token_embedders.type = ref
2021-11-12 15:15:50.180 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:15:50.180 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:15:50.180 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:15:50.181 type = bert-base-cased
2021-11-12 15:15:50.181 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:15:50.181 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:15:50.181 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:15:50.182 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:15:50.182 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:15:50.182 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:15:50.182 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:15:50.182 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:15:50.182 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:15:50.182 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:15:50.182 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:15:50.223 model.encoder.type = bert_pooler
2021-11-12 15:15:50.223 model.encoder.type = bert_pooler
2021-11-12 15:15:50.223 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:15:50.223 type = bert-base-cased
2021-11-12 15:15:50.223 model.encoder.override_weights_file = None
2021-11-12 15:15:50.224 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:15:50.224 model.encoder.load_weights = True
2021-11-12 15:15:50.224 model.encoder.requires_grad = True
2021-11-12 15:15:50.224 model.encoder.dropout = 0.0
2021-11-12 15:15:50.225 model.encoder.transformer_kwargs = None
2021-11-12 15:15:50.437 removing temporary unarchived model dir at /tmp/tmpddu1d4md
2021-11-12 15:16:22.808 Plugin allennlp_models available
2021-11-12 15:16:22.815 Plugin allennlp_server available
2021-11-12 15:16:22.816 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:16:22.816 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpf34i4gx8
2021-11-12 15:16:24.914 dataset_reader.type = example_reader
2021-11-12 15:16:24.914 dataset_reader.max_instances = None
2021-11-12 15:16:24.914 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:16:24.914 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:16:24.914 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:16:24.915 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:16:24.915 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:16:24.915 type = bert-base-cased
2021-11-12 15:16:24.916 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:16:24.916 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:16:24.916 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:16:24.917 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:16:24.918 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:16:24.918 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:16:24.919 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:16:24.919 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:16:24.919 type = bert-base-cased
2021-11-12 15:16:24.919 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:16:24.919 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:16:24.920 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:16:24.920 dataset_reader.to_index = 6
2021-11-12 15:16:24.921 dataset_reader.type = example_reader
2021-11-12 15:16:24.921 dataset_reader.max_instances = None
2021-11-12 15:16:24.921 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:16:24.921 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:16:24.921 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:16:24.922 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:16:24.922 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:16:24.922 type = bert-base-cased
2021-11-12 15:16:24.922 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:16:24.923 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:16:24.923 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:16:24.924 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:16:24.924 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:16:24.925 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:16:24.925 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:16:24.925 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:16:24.926 type = bert-base-cased
2021-11-12 15:16:24.926 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:16:24.926 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:16:24.926 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:16:24.928 dataset_reader.to_index = 6
2021-11-12 15:16:24.929 type = from_instances
2021-11-12 15:16:24.929 Loading token dictionary from /tmp/tmpf34i4gx8/vocabulary.
2021-11-12 15:16:24.930 model.type = sentence_level_classifier
2021-11-12 15:16:24.930 model.embedder.type = ref
2021-11-12 15:16:24.932 model.embedder.type = basic
2021-11-12 15:16:24.933 model.embedder.token_embedders.type = ref
2021-11-12 15:16:24.937 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:16:24.938 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:16:24.938 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:16:24.939 type = bert-base-cased
2021-11-12 15:16:24.939 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:16:24.939 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:16:24.939 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:16:24.940 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:16:24.940 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:16:24.940 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:16:24.940 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:16:24.940 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:16:24.940 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:16:24.940 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:16:24.941 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:16:24.993 model.encoder.type = bert_pooler
2021-11-12 15:16:24.993 model.encoder.type = bert_pooler
2021-11-12 15:16:24.993 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:16:24.993 type = bert-base-cased
2021-11-12 15:16:24.994 model.encoder.override_weights_file = None
2021-11-12 15:16:24.994 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:16:24.994 model.encoder.load_weights = True
2021-11-12 15:16:24.994 model.encoder.requires_grad = True
2021-11-12 15:16:24.994 model.encoder.dropout = 0.0
2021-11-12 15:16:24.995 model.encoder.transformer_kwargs = None
2021-11-12 15:16:25.183 removing temporary unarchived model dir at /tmp/tmpf34i4gx8
2021-11-12 15:16:42.887 Plugin allennlp_models available
2021-11-12 15:16:42.895 Plugin allennlp_server available
2021-11-12 15:16:42.895 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:16:42.896 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpa2o3eiyk
2021-11-12 15:16:44.992 dataset_reader.type = example_reader
2021-11-12 15:16:44.992 dataset_reader.max_instances = None
2021-11-12 15:16:44.993 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:16:44.993 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:16:44.994 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:16:44.994 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:16:44.994 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:16:44.994 type = bert-base-cased
2021-11-12 15:16:44.995 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:16:44.995 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:16:44.995 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:16:44.996 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:16:44.997 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:16:44.997 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:16:44.998 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:16:44.998 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:16:44.999 type = bert-base-cased
2021-11-12 15:16:44.999 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:16:44.999 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:16:44.999 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:16:45.000 dataset_reader.to_index = 6
2021-11-12 15:16:45.000 dataset_reader.type = example_reader
2021-11-12 15:16:45.000 dataset_reader.max_instances = None
2021-11-12 15:16:45.001 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:16:45.001 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:16:45.001 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:16:45.001 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:16:45.002 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:16:45.002 type = bert-base-cased
2021-11-12 15:16:45.002 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:16:45.003 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:16:45.003 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:16:45.004 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:16:45.005 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:16:45.005 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:16:45.005 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:16:45.005 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:16:45.005 type = bert-base-cased
2021-11-12 15:16:45.006 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:16:45.006 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:16:45.006 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:16:45.007 dataset_reader.to_index = 6
2021-11-12 15:16:45.007 type = from_instances
2021-11-12 15:16:45.008 Loading token dictionary from /tmp/tmpa2o3eiyk/vocabulary.
2021-11-12 15:16:45.008 model.type = sentence_level_classifier
2021-11-12 15:16:45.008 model.embedder.type = ref
2021-11-12 15:16:45.009 model.embedder.type = basic
2021-11-12 15:16:45.010 model.embedder.token_embedders.type = ref
2021-11-12 15:16:45.011 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:16:45.012 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:16:45.012 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:16:45.012 type = bert-base-cased
2021-11-12 15:16:45.013 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:16:45.013 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:16:45.013 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:16:45.013 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:16:45.013 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:16:45.013 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:16:45.014 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:16:45.014 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:16:45.014 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:16:45.014 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:16:45.014 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:16:45.064 model.encoder.type = bert_pooler
2021-11-12 15:16:45.064 model.encoder.type = bert_pooler
2021-11-12 15:16:45.065 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:16:45.065 type = bert-base-cased
2021-11-12 15:16:45.065 model.encoder.override_weights_file = None
2021-11-12 15:16:45.065 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:16:45.065 model.encoder.load_weights = True
2021-11-12 15:16:45.065 model.encoder.requires_grad = True
2021-11-12 15:16:45.065 model.encoder.dropout = 0.0
2021-11-12 15:16:45.066 model.encoder.transformer_kwargs = None
2021-11-12 15:16:45.266 removing temporary unarchived model dir at /tmp/tmpa2o3eiyk
2021-11-12 15:17:02.052 Plugin allennlp_models available
2021-11-12 15:17:02.060 Plugin allennlp_server available
2021-11-12 15:17:02.061 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:17:02.061 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpact_cqa5
2021-11-12 15:17:04.161 dataset_reader.type = example_reader
2021-11-12 15:17:04.161 dataset_reader.max_instances = None
2021-11-12 15:17:04.161 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:17:04.162 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:17:04.162 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:17:04.162 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:17:04.162 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:17:04.163 type = bert-base-cased
2021-11-12 15:17:04.163 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:17:04.163 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:17:04.163 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:17:04.164 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:17:04.165 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:17:04.165 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:17:04.165 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:17:04.165 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:17:04.166 type = bert-base-cased
2021-11-12 15:17:04.166 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:17:04.166 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:17:04.166 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:17:04.166 dataset_reader.to_index = 6
2021-11-12 15:17:04.167 dataset_reader.type = example_reader
2021-11-12 15:17:04.167 dataset_reader.max_instances = None
2021-11-12 15:17:04.167 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:17:04.167 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:17:04.167 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:17:04.168 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:17:04.168 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:17:04.168 type = bert-base-cased
2021-11-12 15:17:04.168 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:17:04.168 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:17:04.168 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:17:04.169 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:17:04.170 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:17:04.170 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:17:04.170 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:17:04.170 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:17:04.171 type = bert-base-cased
2021-11-12 15:17:04.171 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:17:04.171 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:17:04.171 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:17:04.171 dataset_reader.to_index = 6
2021-11-12 15:17:04.171 type = from_instances
2021-11-12 15:17:04.172 Loading token dictionary from /tmp/tmpact_cqa5/vocabulary.
2021-11-12 15:17:04.172 model.type = sentence_level_classifier
2021-11-12 15:17:04.172 model.embedder.type = ref
2021-11-12 15:17:04.173 model.embedder.type = basic
2021-11-12 15:17:04.173 model.embedder.token_embedders.type = ref
2021-11-12 15:17:04.175 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:17:04.175 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:17:04.175 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:17:04.175 type = bert-base-cased
2021-11-12 15:17:04.176 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:17:04.176 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:17:04.176 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:17:04.176 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:17:04.176 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:17:04.176 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:17:04.176 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:17:04.176 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:17:04.176 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:17:04.176 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:17:04.176 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:17:04.228 model.encoder.type = bert_pooler
2021-11-12 15:17:04.228 model.encoder.type = bert_pooler
2021-11-12 15:17:04.229 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:17:04.229 type = bert-base-cased
2021-11-12 15:17:04.229 model.encoder.override_weights_file = None
2021-11-12 15:17:04.229 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:17:04.229 model.encoder.load_weights = True
2021-11-12 15:17:04.229 model.encoder.requires_grad = True
2021-11-12 15:17:04.229 model.encoder.dropout = 0.0
2021-11-12 15:17:04.229 model.encoder.transformer_kwargs = None
2021-11-12 15:17:04.422 removing temporary unarchived model dir at /tmp/tmpact_cqa5
2021-11-12 15:17:17.170 Plugin allennlp_models available
2021-11-12 15:17:17.180 Plugin allennlp_server available
2021-11-12 15:17:17.181 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:17:17.181 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpmu7kv17x
2021-11-12 15:17:19.304 dataset_reader.type = example_reader
2021-11-12 15:17:19.304 dataset_reader.max_instances = None
2021-11-12 15:17:19.304 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:17:19.304 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:17:19.305 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:17:19.305 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:17:19.305 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:17:19.305 type = bert-base-cased
2021-11-12 15:17:19.305 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:17:19.305 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:17:19.306 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:17:19.307 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:17:19.308 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:17:19.308 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:17:19.308 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:17:19.308 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:17:19.309 type = bert-base-cased
2021-11-12 15:17:19.309 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:17:19.309 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:17:19.309 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:17:19.310 dataset_reader.to_index = 6
2021-11-12 15:17:19.310 dataset_reader.type = example_reader
2021-11-12 15:17:19.310 dataset_reader.max_instances = None
2021-11-12 15:17:19.310 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:17:19.310 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:17:19.310 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:17:19.310 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:17:19.311 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:17:19.311 type = bert-base-cased
2021-11-12 15:17:19.311 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:17:19.311 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:17:19.311 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:17:19.312 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:17:19.312 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:17:19.313 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:17:19.313 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:17:19.313 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:17:19.313 type = bert-base-cased
2021-11-12 15:17:19.313 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:17:19.313 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:17:19.313 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:17:19.314 dataset_reader.to_index = 6
2021-11-12 15:17:19.314 type = from_instances
2021-11-12 15:17:19.314 Loading token dictionary from /tmp/tmpmu7kv17x/vocabulary.
2021-11-12 15:17:19.315 model.type = sentence_level_classifier
2021-11-12 15:17:19.315 model.embedder.type = ref
2021-11-12 15:17:19.316 model.embedder.type = basic
2021-11-12 15:17:19.316 model.embedder.token_embedders.type = ref
2021-11-12 15:17:19.318 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:17:19.318 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:17:19.319 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:17:19.319 type = bert-base-cased
2021-11-12 15:17:19.319 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:17:19.319 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:17:19.319 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:17:19.319 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:17:19.319 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:17:19.319 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:17:19.319 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:17:19.319 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:17:19.319 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:17:19.319 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:17:19.319 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:17:19.368 model.encoder.type = bert_pooler
2021-11-12 15:17:19.368 model.encoder.type = bert_pooler
2021-11-12 15:17:19.369 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:17:19.369 type = bert-base-cased
2021-11-12 15:17:19.369 model.encoder.override_weights_file = None
2021-11-12 15:17:19.370 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:17:19.370 model.encoder.load_weights = True
2021-11-12 15:17:19.370 model.encoder.requires_grad = True
2021-11-12 15:17:19.370 model.encoder.dropout = 0.0
2021-11-12 15:17:19.371 model.encoder.transformer_kwargs = None
2021-11-12 15:17:19.558 removing temporary unarchived model dir at /tmp/tmpmu7kv17x
2021-11-12 15:17:28.697 Plugin allennlp_models available
2021-11-12 15:17:28.705 Plugin allennlp_server available
2021-11-12 15:17:28.706 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:17:28.707 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpn3kvq9ay
2021-11-12 15:17:30.811 dataset_reader.type = example_reader
2021-11-12 15:17:30.811 dataset_reader.max_instances = None
2021-11-12 15:17:30.811 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:17:30.811 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:17:30.811 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:17:30.811 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:17:30.811 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:17:30.811 type = bert-base-cased
2021-11-12 15:17:30.812 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:17:30.812 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:17:30.812 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:17:30.814 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:17:30.816 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:17:30.817 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:17:30.817 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:17:30.818 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:17:30.818 type = bert-base-cased
2021-11-12 15:17:30.818 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:17:30.819 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:17:30.819 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:17:30.820 dataset_reader.to_index = 6
2021-11-12 15:17:30.821 dataset_reader.type = example_reader
2021-11-12 15:17:30.822 dataset_reader.max_instances = None
2021-11-12 15:17:30.822 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:17:30.822 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:17:30.823 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:17:30.823 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:17:30.824 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:17:30.824 type = bert-base-cased
2021-11-12 15:17:30.824 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:17:30.825 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:17:30.825 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:17:30.826 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:17:30.828 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:17:30.828 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:17:30.829 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:17:30.829 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:17:30.830 type = bert-base-cased
2021-11-12 15:17:30.830 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:17:30.830 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:17:30.830 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:17:30.832 dataset_reader.to_index = 6
2021-11-12 15:17:30.832 type = from_instances
2021-11-12 15:17:30.832 Loading token dictionary from /tmp/tmpn3kvq9ay/vocabulary.
2021-11-12 15:17:30.833 model.type = sentence_level_classifier
2021-11-12 15:17:30.834 model.embedder.type = ref
2021-11-12 15:17:30.835 model.embedder.type = basic
2021-11-12 15:17:30.835 model.embedder.token_embedders.type = ref
2021-11-12 15:17:30.839 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:17:30.839 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:17:30.839 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:17:30.840 type = bert-base-cased
2021-11-12 15:17:30.840 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:17:30.840 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:17:30.841 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:17:30.841 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:17:30.841 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:17:30.841 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:17:30.841 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:17:30.842 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:17:30.842 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:17:30.842 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:17:30.842 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:17:30.891 model.encoder.type = bert_pooler
2021-11-12 15:17:30.891 model.encoder.type = bert_pooler
2021-11-12 15:17:30.892 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:17:30.892 type = bert-base-cased
2021-11-12 15:17:30.892 model.encoder.override_weights_file = None
2021-11-12 15:17:30.892 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:17:30.892 model.encoder.load_weights = True
2021-11-12 15:17:30.892 model.encoder.requires_grad = True
2021-11-12 15:17:30.892 model.encoder.dropout = 0.0
2021-11-12 15:17:30.893 model.encoder.transformer_kwargs = None
2021-11-12 15:17:31.086 removing temporary unarchived model dir at /tmp/tmpn3kvq9ay
2021-11-12 15:17:48.737 Plugin allennlp_models available
2021-11-12 15:17:48.747 Plugin allennlp_server available
2021-11-12 15:17:48.748 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:17:48.748 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpjlp61nsr
2021-11-12 15:17:50.886 dataset_reader.type = example_reader
2021-11-12 15:17:50.887 dataset_reader.max_instances = None
2021-11-12 15:17:50.887 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:17:50.887 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:17:50.887 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:17:50.887 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:17:50.888 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:17:50.888 type = bert-base-cased
2021-11-12 15:17:50.888 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:17:50.888 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:17:50.888 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:17:50.889 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:17:50.890 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:17:50.890 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:17:50.890 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:17:50.890 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:17:50.891 type = bert-base-cased
2021-11-12 15:17:50.891 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:17:50.891 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:17:50.891 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:17:50.891 dataset_reader.to_index = 6
2021-11-12 15:17:50.892 dataset_reader.type = example_reader
2021-11-12 15:17:50.892 dataset_reader.max_instances = None
2021-11-12 15:17:50.892 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:17:50.892 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:17:50.892 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:17:50.892 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:17:50.893 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:17:50.893 type = bert-base-cased
2021-11-12 15:17:50.893 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:17:50.893 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:17:50.893 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:17:50.893 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:17:50.894 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:17:50.894 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:17:50.894 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:17:50.895 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:17:50.895 type = bert-base-cased
2021-11-12 15:17:50.895 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:17:50.895 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:17:50.895 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:17:50.896 dataset_reader.to_index = 6
2021-11-12 15:17:50.896 type = from_instances
2021-11-12 15:17:50.896 Loading token dictionary from /tmp/tmpjlp61nsr/vocabulary.
2021-11-12 15:17:50.897 model.type = sentence_level_classifier
2021-11-12 15:17:50.897 model.embedder.type = ref
2021-11-12 15:17:50.898 model.embedder.type = basic
2021-11-12 15:17:50.898 model.embedder.token_embedders.type = ref
2021-11-12 15:17:50.899 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:17:50.900 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:17:50.900 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:17:50.900 type = bert-base-cased
2021-11-12 15:17:50.900 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:17:50.901 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:17:50.901 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:17:50.901 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:17:50.901 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:17:50.901 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:17:50.901 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:17:50.901 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:17:50.901 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:17:50.901 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:17:50.901 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:17:50.951 model.encoder.type = bert_pooler
2021-11-12 15:17:50.951 model.encoder.type = bert_pooler
2021-11-12 15:17:50.951 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:17:50.952 type = bert-base-cased
2021-11-12 15:17:50.952 model.encoder.override_weights_file = None
2021-11-12 15:17:50.952 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:17:50.952 model.encoder.load_weights = True
2021-11-12 15:17:50.952 model.encoder.requires_grad = True
2021-11-12 15:17:50.953 model.encoder.dropout = 0.0
2021-11-12 15:17:50.953 model.encoder.transformer_kwargs = None
2021-11-12 15:17:51.173 removing temporary unarchived model dir at /tmp/tmpjlp61nsr
2021-11-12 15:18:07.124 Plugin allennlp_models available
2021-11-12 15:18:07.134 Plugin allennlp_server available
2021-11-12 15:18:07.135 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:18:07.136 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpziiq1ifm
2021-11-12 15:18:09.243 dataset_reader.type = example_reader
2021-11-12 15:18:09.243 dataset_reader.max_instances = None
2021-11-12 15:18:09.243 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:18:09.243 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:18:09.244 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:18:09.244 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:18:09.244 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:18:09.244 type = bert-base-cased
2021-11-12 15:18:09.244 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:18:09.244 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:18:09.244 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:18:09.245 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:18:09.246 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:18:09.246 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:18:09.247 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:18:09.247 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:18:09.247 type = bert-base-cased
2021-11-12 15:18:09.247 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:18:09.247 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:18:09.247 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:18:09.248 dataset_reader.to_index = 6
2021-11-12 15:18:09.248 dataset_reader.type = example_reader
2021-11-12 15:18:09.248 dataset_reader.max_instances = None
2021-11-12 15:18:09.248 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:18:09.249 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:18:09.249 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:18:09.249 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:18:09.249 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:18:09.249 type = bert-base-cased
2021-11-12 15:18:09.250 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:18:09.250 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:18:09.250 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:18:09.250 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:18:09.251 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:18:09.251 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:18:09.251 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:18:09.252 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:18:09.252 type = bert-base-cased
2021-11-12 15:18:09.252 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:18:09.252 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:18:09.252 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:18:09.253 dataset_reader.to_index = 6
2021-11-12 15:18:09.253 type = from_instances
2021-11-12 15:18:09.253 Loading token dictionary from /tmp/tmpziiq1ifm/vocabulary.
2021-11-12 15:18:09.254 model.type = sentence_level_classifier
2021-11-12 15:18:09.254 model.embedder.type = ref
2021-11-12 15:18:09.255 model.embedder.type = basic
2021-11-12 15:18:09.255 model.embedder.token_embedders.type = ref
2021-11-12 15:18:09.257 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:18:09.257 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:18:09.257 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:18:09.257 type = bert-base-cased
2021-11-12 15:18:09.257 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:18:09.257 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:18:09.258 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:18:09.258 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:18:09.258 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:18:09.258 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:18:09.258 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:18:09.258 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:18:09.258 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:18:09.258 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:18:09.258 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:18:09.307 model.encoder.type = bert_pooler
2021-11-12 15:18:09.307 model.encoder.type = bert_pooler
2021-11-12 15:18:09.308 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:18:09.308 type = bert-base-cased
2021-11-12 15:18:09.308 model.encoder.override_weights_file = None
2021-11-12 15:18:09.308 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:18:09.308 model.encoder.load_weights = True
2021-11-12 15:18:09.308 model.encoder.requires_grad = True
2021-11-12 15:18:09.309 model.encoder.dropout = 0.0
2021-11-12 15:18:09.309 model.encoder.transformer_kwargs = None
2021-11-12 15:18:09.499 removing temporary unarchived model dir at /tmp/tmpziiq1ifm
2021-11-12 15:18:38.959 Plugin allennlp_models available
2021-11-12 15:18:38.969 Plugin allennlp_server available
2021-11-12 15:18:38.970 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:18:38.970 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpq93wxz9s
2021-11-12 15:18:41.076 dataset_reader.type = example_reader
2021-11-12 15:18:41.077 dataset_reader.max_instances = None
2021-11-12 15:18:41.077 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:18:41.077 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:18:41.077 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:18:41.078 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:18:41.078 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:18:41.078 type = bert-base-cased
2021-11-12 15:18:41.078 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:18:41.078 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:18:41.078 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:18:41.079 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:18:41.080 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:18:41.080 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:18:41.081 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:18:41.081 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:18:41.081 type = bert-base-cased
2021-11-12 15:18:41.081 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:18:41.081 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:18:41.082 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:18:41.082 dataset_reader.to_index = 6
2021-11-12 15:18:41.083 dataset_reader.type = example_reader
2021-11-12 15:18:41.083 dataset_reader.max_instances = None
2021-11-12 15:18:41.083 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:18:41.083 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:18:41.083 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:18:41.084 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:18:41.084 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:18:41.084 type = bert-base-cased
2021-11-12 15:18:41.084 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:18:41.084 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:18:41.085 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:18:41.085 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:18:41.086 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:18:41.086 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:18:41.087 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:18:41.087 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:18:41.087 type = bert-base-cased
2021-11-12 15:18:41.087 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:18:41.087 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:18:41.087 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:18:41.088 dataset_reader.to_index = 6
2021-11-12 15:18:41.088 type = from_instances
2021-11-12 15:18:41.089 Loading token dictionary from /tmp/tmpq93wxz9s/vocabulary.
2021-11-12 15:18:41.089 model.type = sentence_level_classifier
2021-11-12 15:18:41.089 model.embedder.type = ref
2021-11-12 15:18:41.090 model.embedder.type = basic
2021-11-12 15:18:41.090 model.embedder.token_embedders.type = ref
2021-11-12 15:18:41.092 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:18:41.092 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:18:41.092 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:18:41.093 type = bert-base-cased
2021-11-12 15:18:41.093 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:18:41.093 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:18:41.093 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:18:41.093 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:18:41.094 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:18:41.094 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:18:41.094 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:18:41.094 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:18:41.094 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:18:41.094 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:18:41.094 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:18:41.136 model.encoder.type = bert_pooler
2021-11-12 15:18:41.136 model.encoder.type = bert_pooler
2021-11-12 15:18:41.136 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:18:41.137 type = bert-base-cased
2021-11-12 15:18:41.137 model.encoder.override_weights_file = None
2021-11-12 15:18:41.137 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:18:41.137 model.encoder.load_weights = True
2021-11-12 15:18:41.137 model.encoder.requires_grad = True
2021-11-12 15:18:41.137 model.encoder.dropout = 0.0
2021-11-12 15:18:41.137 model.encoder.transformer_kwargs = None
2021-11-12 15:18:41.339 removing temporary unarchived model dir at /tmp/tmpq93wxz9s
2021-11-12 15:18:54.584 Plugin allennlp_models available
2021-11-12 15:18:54.589 Plugin allennlp_server available
2021-11-12 15:18:54.590 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:18:54.590 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpklwwghmy
2021-11-12 15:18:56.713 dataset_reader.type = example_reader
2021-11-12 15:18:56.713 dataset_reader.max_instances = None
2021-11-12 15:18:56.713 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:18:56.713 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:18:56.714 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:18:56.714 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:18:56.714 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:18:56.714 type = bert-base-cased
2021-11-12 15:18:56.715 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:18:56.715 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:18:56.715 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:18:56.716 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:18:56.717 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:18:56.717 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:18:56.717 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:18:56.717 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:18:56.718 type = bert-base-cased
2021-11-12 15:18:56.718 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:18:56.718 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:18:56.718 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:18:56.719 dataset_reader.to_index = 6
2021-11-12 15:18:56.719 dataset_reader.type = example_reader
2021-11-12 15:18:56.719 dataset_reader.max_instances = None
2021-11-12 15:18:56.719 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:18:56.720 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:18:56.720 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:18:56.720 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:18:56.720 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:18:56.721 type = bert-base-cased
2021-11-12 15:18:56.721 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:18:56.721 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:18:56.721 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:18:56.722 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:18:56.722 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:18:56.723 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:18:56.723 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:18:56.723 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:18:56.723 type = bert-base-cased
2021-11-12 15:18:56.723 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:18:56.723 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:18:56.723 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:18:56.724 dataset_reader.to_index = 6
2021-11-12 15:18:56.724 type = from_instances
2021-11-12 15:18:56.725 Loading token dictionary from /tmp/tmpklwwghmy/vocabulary.
2021-11-12 15:18:56.725 model.type = sentence_level_classifier
2021-11-12 15:18:56.725 model.embedder.type = ref
2021-11-12 15:18:56.726 model.embedder.type = basic
2021-11-12 15:18:56.726 model.embedder.token_embedders.type = ref
2021-11-12 15:18:56.728 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:18:56.728 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:18:56.729 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:18:56.729 type = bert-base-cased
2021-11-12 15:18:56.729 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:18:56.729 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:18:56.729 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:18:56.729 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:18:56.729 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:18:56.729 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:18:56.729 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:18:56.729 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:18:56.729 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:18:56.730 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:18:56.730 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:18:56.779 model.encoder.type = bert_pooler
2021-11-12 15:18:56.780 model.encoder.type = bert_pooler
2021-11-12 15:18:56.780 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:18:56.780 type = bert-base-cased
2021-11-12 15:18:56.780 model.encoder.override_weights_file = None
2021-11-12 15:18:56.780 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:18:56.781 model.encoder.load_weights = True
2021-11-12 15:18:56.781 model.encoder.requires_grad = True
2021-11-12 15:18:56.781 model.encoder.dropout = 0.0
2021-11-12 15:18:56.781 model.encoder.transformer_kwargs = None
2021-11-12 15:18:56.973 removing temporary unarchived model dir at /tmp/tmpklwwghmy
2021-11-12 15:19:05.818 Plugin allennlp_models available
2021-11-12 15:19:05.826 Plugin allennlp_server available
2021-11-12 15:19:05.827 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:19:05.828 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmplt7e65wv
2021-11-12 15:19:07.949 dataset_reader.type = example_reader
2021-11-12 15:19:07.949 dataset_reader.max_instances = None
2021-11-12 15:19:07.950 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:19:07.950 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:19:07.950 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:19:07.950 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:19:07.951 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:19:07.951 type = bert-base-cased
2021-11-12 15:19:07.951 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:19:07.951 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:19:07.951 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:19:07.952 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:19:07.953 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:19:07.953 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:19:07.954 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:19:07.954 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:19:07.954 type = bert-base-cased
2021-11-12 15:19:07.954 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:19:07.954 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:19:07.955 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:19:07.955 dataset_reader.to_index = 6
2021-11-12 15:19:07.955 dataset_reader.type = example_reader
2021-11-12 15:19:07.956 dataset_reader.max_instances = None
2021-11-12 15:19:07.956 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:19:07.956 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:19:07.956 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:19:07.957 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:19:07.957 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:19:07.958 type = bert-base-cased
2021-11-12 15:19:07.958 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:19:07.958 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:19:07.959 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:19:07.961 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:19:07.962 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:19:07.963 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:19:07.963 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:19:07.964 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:19:07.964 type = bert-base-cased
2021-11-12 15:19:07.964 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:19:07.964 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:19:07.965 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:19:07.966 dataset_reader.to_index = 6
2021-11-12 15:19:07.967 type = from_instances
2021-11-12 15:19:07.967 Loading token dictionary from /tmp/tmplt7e65wv/vocabulary.
2021-11-12 15:19:07.968 model.type = sentence_level_classifier
2021-11-12 15:19:07.969 model.embedder.type = ref
2021-11-12 15:19:07.971 model.embedder.type = basic
2021-11-12 15:19:07.971 model.embedder.token_embedders.type = ref
2021-11-12 15:19:07.975 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:19:07.976 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:19:07.976 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:19:07.976 type = bert-base-cased
2021-11-12 15:19:07.977 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:19:07.977 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:19:07.977 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:19:07.977 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:19:07.977 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:19:07.978 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:19:07.978 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:19:07.978 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:19:07.978 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:19:07.978 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:19:07.979 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:19:08.028 model.encoder.type = bert_pooler
2021-11-12 15:19:08.028 model.encoder.type = bert_pooler
2021-11-12 15:19:08.028 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:19:08.028 type = bert-base-cased
2021-11-12 15:19:08.028 model.encoder.override_weights_file = None
2021-11-12 15:19:08.028 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:19:08.028 model.encoder.load_weights = True
2021-11-12 15:19:08.029 model.encoder.requires_grad = True
2021-11-12 15:19:08.029 model.encoder.dropout = 0.0
2021-11-12 15:19:08.029 model.encoder.transformer_kwargs = None
2021-11-12 15:19:08.215 removing temporary unarchived model dir at /tmp/tmplt7e65wv
2021-11-12 15:20:01.732 Plugin allennlp_models available
2021-11-12 15:20:01.740 Plugin allennlp_server available
2021-11-12 15:20:01.741 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:20:01.742 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpkgiy3t_2
2021-11-12 15:20:03.854 dataset_reader.type = example_reader
2021-11-12 15:20:03.854 dataset_reader.max_instances = None
2021-11-12 15:20:03.854 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:20:03.854 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:20:03.854 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:20:03.855 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:20:03.855 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:20:03.855 type = bert-base-cased
2021-11-12 15:20:03.855 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:20:03.855 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:20:03.855 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:20:03.856 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:20:03.857 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:20:03.857 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:20:03.858 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:20:03.858 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:20:03.858 type = bert-base-cased
2021-11-12 15:20:03.858 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:20:03.859 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:20:03.859 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:20:03.859 dataset_reader.to_index = 6
2021-11-12 15:20:03.860 dataset_reader.type = example_reader
2021-11-12 15:20:03.860 dataset_reader.max_instances = None
2021-11-12 15:20:03.860 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:20:03.860 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:20:03.860 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:20:03.860 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:20:03.860 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:20:03.861 type = bert-base-cased
2021-11-12 15:20:03.861 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:20:03.861 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:20:03.861 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:20:03.862 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:20:03.862 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:20:03.862 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:20:03.863 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:20:03.863 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:20:03.863 type = bert-base-cased
2021-11-12 15:20:03.863 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:20:03.863 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:20:03.864 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:20:03.864 dataset_reader.to_index = 6
2021-11-12 15:20:03.864 type = from_instances
2021-11-12 15:20:03.865 Loading token dictionary from /tmp/tmpkgiy3t_2/vocabulary.
2021-11-12 15:20:03.865 model.type = sentence_level_classifier
2021-11-12 15:20:03.865 model.embedder.type = ref
2021-11-12 15:20:03.866 model.embedder.type = basic
2021-11-12 15:20:03.866 model.embedder.token_embedders.type = ref
2021-11-12 15:20:03.868 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:20:03.868 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:20:03.869 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:20:03.869 type = bert-base-cased
2021-11-12 15:20:03.869 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:20:03.869 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:20:03.869 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:20:03.869 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:20:03.869 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:20:03.869 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:20:03.869 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:20:03.870 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:20:03.870 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:20:03.870 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:20:03.870 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:20:03.917 model.encoder.type = bert_pooler
2021-11-12 15:20:03.917 model.encoder.type = bert_pooler
2021-11-12 15:20:03.918 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:20:03.918 type = bert-base-cased
2021-11-12 15:20:03.919 model.encoder.override_weights_file = None
2021-11-12 15:20:03.919 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:20:03.919 model.encoder.load_weights = True
2021-11-12 15:20:03.919 model.encoder.requires_grad = True
2021-11-12 15:20:03.920 model.encoder.dropout = 0.0
2021-11-12 15:20:03.920 model.encoder.transformer_kwargs = None
2021-11-12 15:20:04.111 removing temporary unarchived model dir at /tmp/tmpkgiy3t_2
2021-11-12 15:20:12.349 Plugin allennlp_models available
2021-11-12 15:20:12.356 Plugin allennlp_server available
2021-11-12 15:20:12.357 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:20:12.357 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpoxe8aq_p
2021-11-12 15:20:14.483 dataset_reader.type = example_reader
2021-11-12 15:20:14.483 dataset_reader.max_instances = None
2021-11-12 15:20:14.483 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:20:14.483 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:20:14.483 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:20:14.483 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:20:14.484 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:20:14.484 type = bert-base-cased
2021-11-12 15:20:14.484 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:20:14.484 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:20:14.485 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:20:14.486 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:20:14.486 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:20:14.487 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:20:14.487 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:20:14.487 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:20:14.487 type = bert-base-cased
2021-11-12 15:20:14.487 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:20:14.487 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:20:14.488 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:20:14.488 dataset_reader.to_index = 6
2021-11-12 15:20:14.489 dataset_reader.type = example_reader
2021-11-12 15:20:14.489 dataset_reader.max_instances = None
2021-11-12 15:20:14.489 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:20:14.489 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:20:14.489 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:20:14.489 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:20:14.490 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:20:14.490 type = bert-base-cased
2021-11-12 15:20:14.490 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:20:14.490 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:20:14.490 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:20:14.491 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:20:14.492 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:20:14.492 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:20:14.492 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:20:14.493 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:20:14.493 type = bert-base-cased
2021-11-12 15:20:14.493 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:20:14.493 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:20:14.493 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:20:14.494 dataset_reader.to_index = 6
2021-11-12 15:20:14.494 type = from_instances
2021-11-12 15:20:14.494 Loading token dictionary from /tmp/tmpoxe8aq_p/vocabulary.
2021-11-12 15:20:14.495 model.type = sentence_level_classifier
2021-11-12 15:20:14.495 model.embedder.type = ref
2021-11-12 15:20:14.496 model.embedder.type = basic
2021-11-12 15:20:14.496 model.embedder.token_embedders.type = ref
2021-11-12 15:20:14.497 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:20:14.498 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:20:14.498 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:20:14.498 type = bert-base-cased
2021-11-12 15:20:14.498 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:20:14.499 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:20:14.499 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:20:14.499 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:20:14.499 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:20:14.499 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:20:14.499 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:20:14.499 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:20:14.499 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:20:14.499 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:20:14.499 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:20:14.548 model.encoder.type = bert_pooler
2021-11-12 15:20:14.548 model.encoder.type = bert_pooler
2021-11-12 15:20:14.548 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:20:14.549 type = bert-base-cased
2021-11-12 15:20:14.549 model.encoder.override_weights_file = None
2021-11-12 15:20:14.549 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:20:14.549 model.encoder.load_weights = True
2021-11-12 15:20:14.549 model.encoder.requires_grad = True
2021-11-12 15:20:14.549 model.encoder.dropout = 0.0
2021-11-12 15:20:14.549 model.encoder.transformer_kwargs = None
2021-11-12 15:20:14.740 removing temporary unarchived model dir at /tmp/tmpoxe8aq_p
2021-11-12 15:20:30.084 Plugin allennlp_models available
2021-11-12 15:20:30.093 Plugin allennlp_server available
2021-11-12 15:20:30.093 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:20:30.094 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpjyweoaoc
2021-11-12 15:20:32.191 dataset_reader.type = example_reader
2021-11-12 15:20:32.191 dataset_reader.max_instances = None
2021-11-12 15:20:32.191 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:20:32.191 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:20:32.191 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:20:32.192 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:20:32.192 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:20:32.193 type = bert-base-cased
2021-11-12 15:20:32.193 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:20:32.193 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:20:32.193 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:20:32.194 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:20:32.195 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:20:32.195 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:20:32.196 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:20:32.196 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:20:32.196 type = bert-base-cased
2021-11-12 15:20:32.196 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:20:32.196 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:20:32.196 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:20:32.197 dataset_reader.to_index = 6
2021-11-12 15:20:32.197 dataset_reader.type = example_reader
2021-11-12 15:20:32.197 dataset_reader.max_instances = None
2021-11-12 15:20:32.197 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:20:32.198 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:20:32.198 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:20:32.198 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:20:32.198 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:20:32.198 type = bert-base-cased
2021-11-12 15:20:32.198 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:20:32.199 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:20:32.199 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:20:32.199 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:20:32.200 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:20:32.200 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:20:32.201 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:20:32.201 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:20:32.201 type = bert-base-cased
2021-11-12 15:20:32.201 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:20:32.201 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:20:32.201 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:20:32.202 dataset_reader.to_index = 6
2021-11-12 15:20:32.202 type = from_instances
2021-11-12 15:20:32.202 Loading token dictionary from /tmp/tmpjyweoaoc/vocabulary.
2021-11-12 15:20:32.203 model.type = sentence_level_classifier
2021-11-12 15:20:32.203 model.embedder.type = ref
2021-11-12 15:20:32.204 model.embedder.type = basic
2021-11-12 15:20:32.204 model.embedder.token_embedders.type = ref
2021-11-12 15:20:32.206 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:20:32.206 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:20:32.206 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:20:32.206 type = bert-base-cased
2021-11-12 15:20:32.206 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:20:32.207 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:20:32.207 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:20:32.207 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:20:32.207 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:20:32.207 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:20:32.207 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:20:32.207 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:20:32.207 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:20:32.207 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:20:32.207 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:20:32.256 model.encoder.type = bert_pooler
2021-11-12 15:20:32.256 model.encoder.type = bert_pooler
2021-11-12 15:20:32.256 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:20:32.257 type = bert-base-cased
2021-11-12 15:20:32.257 model.encoder.override_weights_file = None
2021-11-12 15:20:32.257 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:20:32.257 model.encoder.load_weights = True
2021-11-12 15:20:32.258 model.encoder.requires_grad = True
2021-11-12 15:20:32.258 model.encoder.dropout = 0.0
2021-11-12 15:20:32.258 model.encoder.transformer_kwargs = None
2021-11-12 15:20:32.473 removing temporary unarchived model dir at /tmp/tmpjyweoaoc
2021-11-12 15:20:46.601 Plugin allennlp_models available
2021-11-12 15:20:46.611 Plugin allennlp_server available
2021-11-12 15:20:46.612 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:20:46.613 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpl7s67d4t
2021-11-12 15:20:48.722 dataset_reader.type = example_reader
2021-11-12 15:20:48.723 dataset_reader.max_instances = None
2021-11-12 15:20:48.723 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:20:48.723 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:20:48.723 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:20:48.724 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:20:48.724 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:20:48.724 type = bert-base-cased
2021-11-12 15:20:48.724 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:20:48.724 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:20:48.725 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:20:48.727 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:20:48.729 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:20:48.730 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:20:48.730 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:20:48.731 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:20:48.731 type = bert-base-cased
2021-11-12 15:20:48.731 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:20:48.732 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:20:48.732 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:20:48.734 dataset_reader.to_index = 6
2021-11-12 15:20:48.734 dataset_reader.type = example_reader
2021-11-12 15:20:48.734 dataset_reader.max_instances = None
2021-11-12 15:20:48.735 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:20:48.735 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:20:48.736 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:20:48.736 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:20:48.736 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:20:48.737 type = bert-base-cased
2021-11-12 15:20:48.737 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:20:48.737 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:20:48.738 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:20:48.739 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:20:48.741 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:20:48.741 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:20:48.742 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:20:48.742 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:20:48.743 type = bert-base-cased
2021-11-12 15:20:48.743 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:20:48.743 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:20:48.743 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:20:48.745 dataset_reader.to_index = 6
2021-11-12 15:20:48.745 type = from_instances
2021-11-12 15:20:48.745 Loading token dictionary from /tmp/tmpl7s67d4t/vocabulary.
2021-11-12 15:20:48.746 model.type = sentence_level_classifier
2021-11-12 15:20:48.746 model.embedder.type = ref
2021-11-12 15:20:48.748 model.embedder.type = basic
2021-11-12 15:20:48.748 model.embedder.token_embedders.type = ref
2021-11-12 15:20:48.752 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:20:48.752 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:20:48.752 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:20:48.753 type = bert-base-cased
2021-11-12 15:20:48.753 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:20:48.753 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:20:48.753 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:20:48.754 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:20:48.754 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:20:48.754 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:20:48.754 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:20:48.754 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:20:48.754 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:20:48.755 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:20:48.755 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:20:48.795 model.encoder.type = bert_pooler
2021-11-12 15:20:48.795 model.encoder.type = bert_pooler
2021-11-12 15:20:48.795 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:20:48.795 type = bert-base-cased
2021-11-12 15:20:48.796 model.encoder.override_weights_file = None
2021-11-12 15:20:48.796 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:20:48.796 model.encoder.load_weights = True
2021-11-12 15:20:48.796 model.encoder.requires_grad = True
2021-11-12 15:20:48.797 model.encoder.dropout = 0.0
2021-11-12 15:20:48.797 model.encoder.transformer_kwargs = None
2021-11-12 15:20:48.988 removing temporary unarchived model dir at /tmp/tmpl7s67d4t
2021-11-12 15:21:15.314 Plugin allennlp_models available
2021-11-12 15:21:15.320 Plugin allennlp_server available
2021-11-12 15:21:15.321 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:21:15.322 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpg71_ufml
2021-11-12 15:21:17.449 dataset_reader.type = example_reader
2021-11-12 15:21:17.449 dataset_reader.max_instances = None
2021-11-12 15:21:17.449 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:21:17.449 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:21:17.450 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:21:17.450 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:21:17.450 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:21:17.451 type = bert-base-cased
2021-11-12 15:21:17.451 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:21:17.451 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:21:17.451 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:21:17.452 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:21:17.453 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:21:17.453 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:21:17.453 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:21:17.454 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:21:17.454 type = bert-base-cased
2021-11-12 15:21:17.454 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:21:17.454 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:21:17.454 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:21:17.455 dataset_reader.to_index = 6
2021-11-12 15:21:17.455 dataset_reader.type = example_reader
2021-11-12 15:21:17.455 dataset_reader.max_instances = None
2021-11-12 15:21:17.455 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:21:17.455 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:21:17.455 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:21:17.455 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:21:17.456 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:21:17.456 type = bert-base-cased
2021-11-12 15:21:17.456 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:21:17.456 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:21:17.456 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:21:17.457 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:21:17.458 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:21:17.458 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:21:17.458 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:21:17.458 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:21:17.458 type = bert-base-cased
2021-11-12 15:21:17.458 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:21:17.458 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:21:17.458 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:21:17.459 dataset_reader.to_index = 6
2021-11-12 15:21:17.459 type = from_instances
2021-11-12 15:21:17.459 Loading token dictionary from /tmp/tmpg71_ufml/vocabulary.
2021-11-12 15:21:17.460 model.type = sentence_level_classifier
2021-11-12 15:21:17.460 model.embedder.type = ref
2021-11-12 15:21:17.461 model.embedder.type = basic
2021-11-12 15:21:17.461 model.embedder.token_embedders.type = ref
2021-11-12 15:21:17.463 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:21:17.463 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:21:17.463 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:21:17.463 type = bert-base-cased
2021-11-12 15:21:17.463 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:21:17.464 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:21:17.464 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:21:17.464 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:21:17.464 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:21:17.464 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:21:17.464 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:21:17.464 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:21:17.464 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:21:17.464 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:21:17.464 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:21:17.514 model.encoder.type = bert_pooler
2021-11-12 15:21:17.514 model.encoder.type = bert_pooler
2021-11-12 15:21:17.514 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:21:17.514 type = bert-base-cased
2021-11-12 15:21:17.515 model.encoder.override_weights_file = None
2021-11-12 15:21:17.515 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:21:17.515 model.encoder.load_weights = True
2021-11-12 15:21:17.515 model.encoder.requires_grad = True
2021-11-12 15:21:17.515 model.encoder.dropout = 0.0
2021-11-12 15:21:17.515 model.encoder.transformer_kwargs = None
2021-11-12 15:21:17.706 removing temporary unarchived model dir at /tmp/tmpg71_ufml
2021-11-12 15:21:36.475 Plugin allennlp_models available
2021-11-12 15:21:36.483 Plugin allennlp_server available
2021-11-12 15:21:36.483 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:21:36.484 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpwc72soo8
2021-11-12 15:21:38.581 dataset_reader.type = example_reader
2021-11-12 15:21:38.582 dataset_reader.max_instances = None
2021-11-12 15:21:38.582 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:21:38.582 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:21:38.582 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:21:38.583 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:21:38.583 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:21:38.583 type = bert-base-cased
2021-11-12 15:21:38.583 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:21:38.583 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:21:38.583 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:21:38.584 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:21:38.585 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:21:38.586 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:21:38.586 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:21:38.586 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:21:38.586 type = bert-base-cased
2021-11-12 15:21:38.586 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:21:38.587 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:21:38.587 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:21:38.587 dataset_reader.to_index = 6
2021-11-12 15:21:38.588 dataset_reader.type = example_reader
2021-11-12 15:21:38.588 dataset_reader.max_instances = None
2021-11-12 15:21:38.588 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:21:38.588 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:21:38.588 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:21:38.588 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:21:38.589 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:21:38.589 type = bert-base-cased
2021-11-12 15:21:38.589 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:21:38.589 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:21:38.589 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:21:38.590 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:21:38.591 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:21:38.591 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:21:38.591 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:21:38.591 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:21:38.591 type = bert-base-cased
2021-11-12 15:21:38.592 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:21:38.592 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:21:38.592 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:21:38.593 dataset_reader.to_index = 6
2021-11-12 15:21:38.593 type = from_instances
2021-11-12 15:21:38.593 Loading token dictionary from /tmp/tmpwc72soo8/vocabulary.
2021-11-12 15:21:38.593 model.type = sentence_level_classifier
2021-11-12 15:21:38.594 model.embedder.type = ref
2021-11-12 15:21:38.594 model.embedder.type = basic
2021-11-12 15:21:38.594 model.embedder.token_embedders.type = ref
2021-11-12 15:21:38.596 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:21:38.596 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:21:38.597 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:21:38.597 type = bert-base-cased
2021-11-12 15:21:38.597 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:21:38.597 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:21:38.597 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:21:38.597 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:21:38.597 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:21:38.598 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:21:38.598 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:21:38.598 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:21:38.598 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:21:38.598 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:21:38.598 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:21:38.648 model.encoder.type = bert_pooler
2021-11-12 15:21:38.649 model.encoder.type = bert_pooler
2021-11-12 15:21:38.649 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:21:38.649 type = bert-base-cased
2021-11-12 15:21:38.649 model.encoder.override_weights_file = None
2021-11-12 15:21:38.650 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:21:38.650 model.encoder.load_weights = True
2021-11-12 15:21:38.650 model.encoder.requires_grad = True
2021-11-12 15:21:38.650 model.encoder.dropout = 0.0
2021-11-12 15:21:38.650 model.encoder.transformer_kwargs = None
2021-11-12 15:21:38.841 removing temporary unarchived model dir at /tmp/tmpwc72soo8
2021-11-12 15:21:59.093 Plugin allennlp_models available
2021-11-12 15:21:59.102 Plugin allennlp_server available
2021-11-12 15:21:59.103 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:21:59.103 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpmphcm6on
2021-11-12 15:22:01.229 dataset_reader.type = example_reader
2021-11-12 15:22:01.229 dataset_reader.max_instances = None
2021-11-12 15:22:01.229 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:22:01.231 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:22:01.231 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:22:01.231 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:22:01.231 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:22:01.231 type = bert-base-cased
2021-11-12 15:22:01.232 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:22:01.232 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:22:01.232 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:22:01.233 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:22:01.234 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:22:01.234 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:22:01.235 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:22:01.235 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:22:01.235 type = bert-base-cased
2021-11-12 15:22:01.235 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:22:01.235 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:22:01.235 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:22:01.236 dataset_reader.to_index = 6
2021-11-12 15:22:01.236 dataset_reader.type = example_reader
2021-11-12 15:22:01.236 dataset_reader.max_instances = None
2021-11-12 15:22:01.237 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:22:01.237 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:22:01.237 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:22:01.237 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:22:01.237 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:22:01.237 type = bert-base-cased
2021-11-12 15:22:01.237 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:22:01.238 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:22:01.238 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:22:01.238 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:22:01.239 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:22:01.239 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:22:01.240 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:22:01.240 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:22:01.240 type = bert-base-cased
2021-11-12 15:22:01.240 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:22:01.240 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:22:01.240 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:22:01.241 dataset_reader.to_index = 6
2021-11-12 15:22:01.241 type = from_instances
2021-11-12 15:22:01.241 Loading token dictionary from /tmp/tmpmphcm6on/vocabulary.
2021-11-12 15:22:01.242 model.type = sentence_level_classifier
2021-11-12 15:22:01.242 model.embedder.type = ref
2021-11-12 15:22:01.243 model.embedder.type = basic
2021-11-12 15:22:01.243 model.embedder.token_embedders.type = ref
2021-11-12 15:22:01.245 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:22:01.245 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:22:01.245 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:22:01.245 type = bert-base-cased
2021-11-12 15:22:01.245 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:22:01.246 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:22:01.246 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:22:01.246 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:22:01.246 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:22:01.246 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:22:01.246 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:22:01.246 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:22:01.246 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:22:01.246 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:22:01.246 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:22:01.294 model.encoder.type = bert_pooler
2021-11-12 15:22:01.294 model.encoder.type = bert_pooler
2021-11-12 15:22:01.294 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:22:01.294 type = bert-base-cased
2021-11-12 15:22:01.295 model.encoder.override_weights_file = None
2021-11-12 15:22:01.295 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:22:01.295 model.encoder.load_weights = True
2021-11-12 15:22:01.295 model.encoder.requires_grad = True
2021-11-12 15:22:01.295 model.encoder.dropout = 0.0
2021-11-12 15:22:01.295 model.encoder.transformer_kwargs = None
2021-11-12 15:22:01.485 removing temporary unarchived model dir at /tmp/tmpmphcm6on
2021-11-12 15:22:51.795 Plugin allennlp_models available
2021-11-12 15:22:51.802 Plugin allennlp_server available
2021-11-12 15:22:51.802 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:22:51.803 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp3ci4xp8d
2021-11-12 15:22:53.924 dataset_reader.type = example_reader
2021-11-12 15:22:53.924 dataset_reader.max_instances = None
2021-11-12 15:22:53.924 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:22:53.924 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:22:53.925 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:22:53.925 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:22:53.925 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:22:53.926 type = bert-base-cased
2021-11-12 15:22:53.926 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:22:53.926 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:22:53.926 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:22:53.927 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:22:53.928 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:22:53.928 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:22:53.928 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:22:53.929 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:22:53.929 type = bert-base-cased
2021-11-12 15:22:53.929 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:22:53.929 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:22:53.929 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:22:53.930 dataset_reader.to_index = 6
2021-11-12 15:22:53.930 dataset_reader.type = example_reader
2021-11-12 15:22:53.931 dataset_reader.max_instances = None
2021-11-12 15:22:53.931 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:22:53.931 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:22:53.931 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:22:53.931 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:22:53.931 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:22:53.932 type = bert-base-cased
2021-11-12 15:22:53.932 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:22:53.932 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:22:53.932 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:22:53.933 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:22:53.934 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:22:53.934 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:22:53.934 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:22:53.934 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:22:53.935 type = bert-base-cased
2021-11-12 15:22:53.935 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:22:53.935 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:22:53.935 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:22:53.935 dataset_reader.to_index = 6
2021-11-12 15:22:53.936 type = from_instances
2021-11-12 15:22:53.936 Loading token dictionary from /tmp/tmp3ci4xp8d/vocabulary.
2021-11-12 15:22:53.936 model.type = sentence_level_classifier
2021-11-12 15:22:53.937 model.embedder.type = ref
2021-11-12 15:22:53.937 model.embedder.type = basic
2021-11-12 15:22:53.938 model.embedder.token_embedders.type = ref
2021-11-12 15:22:53.939 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:22:53.939 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:22:53.940 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:22:53.940 type = bert-base-cased
2021-11-12 15:22:53.940 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:22:53.940 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:22:53.941 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:22:53.941 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:22:53.941 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:22:53.941 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:22:53.941 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:22:53.941 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:22:53.941 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:22:53.941 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:22:53.941 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:22:53.989 model.encoder.type = bert_pooler
2021-11-12 15:22:53.989 model.encoder.type = bert_pooler
2021-11-12 15:22:53.989 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:22:53.989 type = bert-base-cased
2021-11-12 15:22:53.990 model.encoder.override_weights_file = None
2021-11-12 15:22:53.990 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:22:53.990 model.encoder.load_weights = True
2021-11-12 15:22:53.990 model.encoder.requires_grad = True
2021-11-12 15:22:53.990 model.encoder.dropout = 0.0
2021-11-12 15:22:53.990 model.encoder.transformer_kwargs = None
2021-11-12 15:22:54.182 removing temporary unarchived model dir at /tmp/tmp3ci4xp8d
2021-11-12 15:23:06.054 Plugin allennlp_models available
2021-11-12 15:23:06.059 Plugin allennlp_server available
2021-11-12 15:23:06.059 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:23:06.059 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpe_yfg9dg
2021-11-12 15:23:08.174 dataset_reader.type = example_reader
2021-11-12 15:23:08.174 dataset_reader.max_instances = None
2021-11-12 15:23:08.175 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:23:08.175 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:23:08.175 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:23:08.176 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:23:08.176 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:23:08.176 type = bert-base-cased
2021-11-12 15:23:08.176 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:23:08.176 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:23:08.177 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:23:08.177 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:23:08.178 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:23:08.179 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:23:08.179 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:23:08.179 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:23:08.179 type = bert-base-cased
2021-11-12 15:23:08.179 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:23:08.179 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:23:08.180 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:23:08.180 dataset_reader.to_index = 6
2021-11-12 15:23:08.181 dataset_reader.type = example_reader
2021-11-12 15:23:08.181 dataset_reader.max_instances = None
2021-11-12 15:23:08.181 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:23:08.181 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:23:08.181 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:23:08.182 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:23:08.182 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:23:08.182 type = bert-base-cased
2021-11-12 15:23:08.182 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:23:08.182 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:23:08.182 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:23:08.183 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:23:08.184 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:23:08.184 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:23:08.184 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:23:08.184 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:23:08.185 type = bert-base-cased
2021-11-12 15:23:08.185 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:23:08.185 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:23:08.185 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:23:08.186 dataset_reader.to_index = 6
2021-11-12 15:23:08.186 type = from_instances
2021-11-12 15:23:08.186 Loading token dictionary from /tmp/tmpe_yfg9dg/vocabulary.
2021-11-12 15:23:08.187 model.type = sentence_level_classifier
2021-11-12 15:23:08.187 model.embedder.type = ref
2021-11-12 15:23:08.187 model.embedder.type = basic
2021-11-12 15:23:08.188 model.embedder.token_embedders.type = ref
2021-11-12 15:23:08.189 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:23:08.190 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:23:08.190 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:23:08.190 type = bert-base-cased
2021-11-12 15:23:08.190 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:23:08.190 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:23:08.190 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:23:08.191 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:23:08.191 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:23:08.191 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:23:08.191 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:23:08.191 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:23:08.191 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:23:08.191 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:23:08.191 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:23:08.237 model.encoder.type = bert_pooler
2021-11-12 15:23:08.237 model.encoder.type = bert_pooler
2021-11-12 15:23:08.237 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:23:08.238 type = bert-base-cased
2021-11-12 15:23:08.238 model.encoder.override_weights_file = None
2021-11-12 15:23:08.238 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:23:08.238 model.encoder.load_weights = True
2021-11-12 15:23:08.238 model.encoder.requires_grad = True
2021-11-12 15:23:08.238 model.encoder.dropout = 0.0
2021-11-12 15:23:08.238 model.encoder.transformer_kwargs = None
2021-11-12 15:23:08.431 removing temporary unarchived model dir at /tmp/tmpe_yfg9dg
2021-11-12 15:23:23.567 Plugin allennlp_models available
2021-11-12 15:23:23.575 Plugin allennlp_server available
2021-11-12 15:23:23.576 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:23:23.576 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp8pt4eawp
2021-11-12 15:23:25.681 dataset_reader.type = example_reader
2021-11-12 15:23:25.681 dataset_reader.max_instances = None
2021-11-12 15:23:25.681 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:23:25.681 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:23:25.681 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:23:25.681 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:23:25.682 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:23:25.682 type = bert-base-cased
2021-11-12 15:23:25.682 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:23:25.682 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:23:25.682 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:23:25.684 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:23:25.685 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:23:25.685 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:23:25.685 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:23:25.685 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:23:25.686 type = bert-base-cased
2021-11-12 15:23:25.686 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:23:25.686 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:23:25.686 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:23:25.686 dataset_reader.to_index = 6
2021-11-12 15:23:25.687 dataset_reader.type = example_reader
2021-11-12 15:23:25.687 dataset_reader.max_instances = None
2021-11-12 15:23:25.687 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:23:25.687 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:23:25.687 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:23:25.687 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:23:25.688 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:23:25.688 type = bert-base-cased
2021-11-12 15:23:25.688 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:23:25.688 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:23:25.688 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:23:25.689 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:23:25.690 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:23:25.690 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:23:25.690 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:23:25.690 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:23:25.691 type = bert-base-cased
2021-11-12 15:23:25.691 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:23:25.691 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:23:25.691 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:23:25.692 dataset_reader.to_index = 6
2021-11-12 15:23:25.692 type = from_instances
2021-11-12 15:23:25.692 Loading token dictionary from /tmp/tmp8pt4eawp/vocabulary.
2021-11-12 15:23:25.693 model.type = sentence_level_classifier
2021-11-12 15:23:25.694 model.embedder.type = ref
2021-11-12 15:23:25.696 model.embedder.type = basic
2021-11-12 15:23:25.696 model.embedder.token_embedders.type = ref
2021-11-12 15:23:25.700 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:23:25.701 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:23:25.702 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:23:25.702 type = bert-base-cased
2021-11-12 15:23:25.703 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:23:25.703 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:23:25.703 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:23:25.703 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:23:25.704 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:23:25.704 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:23:25.704 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:23:25.704 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:23:25.704 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:23:25.705 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:23:25.705 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:23:25.755 model.encoder.type = bert_pooler
2021-11-12 15:23:25.755 model.encoder.type = bert_pooler
2021-11-12 15:23:25.755 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:23:25.755 type = bert-base-cased
2021-11-12 15:23:25.756 model.encoder.override_weights_file = None
2021-11-12 15:23:25.756 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:23:25.756 model.encoder.load_weights = True
2021-11-12 15:23:25.756 model.encoder.requires_grad = True
2021-11-12 15:23:25.757 model.encoder.dropout = 0.0
2021-11-12 15:23:25.757 model.encoder.transformer_kwargs = None
2021-11-12 15:23:25.951 removing temporary unarchived model dir at /tmp/tmp8pt4eawp
2021-11-12 15:23:42.367 Plugin allennlp_models available
2021-11-12 15:23:42.374 Plugin allennlp_server available
2021-11-12 15:23:42.374 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:23:42.375 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp7ulu_uce
2021-11-12 15:23:44.474 dataset_reader.type = example_reader
2021-11-12 15:23:44.474 dataset_reader.max_instances = None
2021-11-12 15:23:44.474 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:23:44.475 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:23:44.475 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:23:44.475 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:23:44.476 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:23:44.476 type = bert-base-cased
2021-11-12 15:23:44.476 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:23:44.476 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:23:44.476 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:23:44.478 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:23:44.479 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:23:44.479 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:23:44.479 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:23:44.480 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:23:44.480 type = bert-base-cased
2021-11-12 15:23:44.480 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:23:44.480 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:23:44.481 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:23:44.481 dataset_reader.to_index = 6
2021-11-12 15:23:44.482 dataset_reader.type = example_reader
2021-11-12 15:23:44.482 dataset_reader.max_instances = None
2021-11-12 15:23:44.482 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:23:44.482 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:23:44.482 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:23:44.482 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:23:44.482 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:23:44.482 type = bert-base-cased
2021-11-12 15:23:44.482 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:23:44.482 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:23:44.483 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:23:44.483 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:23:44.484 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:23:44.484 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:23:44.484 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:23:44.485 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:23:44.485 type = bert-base-cased
2021-11-12 15:23:44.485 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:23:44.485 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:23:44.485 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:23:44.486 dataset_reader.to_index = 6
2021-11-12 15:23:44.486 type = from_instances
2021-11-12 15:23:44.486 Loading token dictionary from /tmp/tmp7ulu_uce/vocabulary.
2021-11-12 15:23:44.487 model.type = sentence_level_classifier
2021-11-12 15:23:44.487 model.embedder.type = ref
2021-11-12 15:23:44.487 model.embedder.type = basic
2021-11-12 15:23:44.488 model.embedder.token_embedders.type = ref
2021-11-12 15:23:44.489 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:23:44.490 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:23:44.490 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:23:44.490 type = bert-base-cased
2021-11-12 15:23:44.490 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:23:44.490 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:23:44.490 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:23:44.490 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:23:44.490 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:23:44.490 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:23:44.490 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:23:44.490 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:23:44.490 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:23:44.490 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:23:44.490 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:23:44.540 model.encoder.type = bert_pooler
2021-11-12 15:23:44.540 model.encoder.type = bert_pooler
2021-11-12 15:23:44.540 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:23:44.541 type = bert-base-cased
2021-11-12 15:23:44.541 model.encoder.override_weights_file = None
2021-11-12 15:23:44.541 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:23:44.541 model.encoder.load_weights = True
2021-11-12 15:23:44.541 model.encoder.requires_grad = True
2021-11-12 15:23:44.541 model.encoder.dropout = 0.0
2021-11-12 15:23:44.542 model.encoder.transformer_kwargs = None
2021-11-12 15:23:44.734 removing temporary unarchived model dir at /tmp/tmp7ulu_uce
2021-11-12 15:23:55.246 Plugin allennlp_models available
2021-11-12 15:23:55.255 Plugin allennlp_server available
2021-11-12 15:23:55.256 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:23:55.257 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpo65og6vg
2021-11-12 15:23:57.357 dataset_reader.type = example_reader
2021-11-12 15:23:57.357 dataset_reader.max_instances = None
2021-11-12 15:23:57.357 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:23:57.357 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:23:57.357 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:23:57.358 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:23:57.358 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:23:57.358 type = bert-base-cased
2021-11-12 15:23:57.358 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:23:57.358 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:23:57.359 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:23:57.360 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:23:57.361 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:23:57.361 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:23:57.361 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:23:57.361 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:23:57.361 type = bert-base-cased
2021-11-12 15:23:57.361 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:23:57.361 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:23:57.361 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:23:57.362 dataset_reader.to_index = 6
2021-11-12 15:23:57.363 dataset_reader.type = example_reader
2021-11-12 15:23:57.363 dataset_reader.max_instances = None
2021-11-12 15:23:57.363 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:23:57.363 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:23:57.363 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:23:57.363 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:23:57.364 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:23:57.364 type = bert-base-cased
2021-11-12 15:23:57.364 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:23:57.364 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:23:57.364 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:23:57.365 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:23:57.367 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:23:57.368 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:23:57.368 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:23:57.368 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:23:57.369 type = bert-base-cased
2021-11-12 15:23:57.369 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:23:57.369 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:23:57.369 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:23:57.371 dataset_reader.to_index = 6
2021-11-12 15:23:57.372 type = from_instances
2021-11-12 15:23:57.372 Loading token dictionary from /tmp/tmpo65og6vg/vocabulary.
2021-11-12 15:23:57.373 model.type = sentence_level_classifier
2021-11-12 15:23:57.373 model.embedder.type = ref
2021-11-12 15:23:57.375 model.embedder.type = basic
2021-11-12 15:23:57.376 model.embedder.token_embedders.type = ref
2021-11-12 15:23:57.380 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:23:57.380 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:23:57.381 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:23:57.381 type = bert-base-cased
2021-11-12 15:23:57.382 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:23:57.382 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:23:57.382 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:23:57.382 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:23:57.382 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:23:57.383 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:23:57.383 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:23:57.383 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:23:57.383 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:23:57.384 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:23:57.384 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:23:57.433 model.encoder.type = bert_pooler
2021-11-12 15:23:57.433 model.encoder.type = bert_pooler
2021-11-12 15:23:57.433 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:23:57.434 type = bert-base-cased
2021-11-12 15:23:57.434 model.encoder.override_weights_file = None
2021-11-12 15:23:57.434 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:23:57.434 model.encoder.load_weights = True
2021-11-12 15:23:57.434 model.encoder.requires_grad = True
2021-11-12 15:23:57.434 model.encoder.dropout = 0.0
2021-11-12 15:23:57.434 model.encoder.transformer_kwargs = None
2021-11-12 15:23:57.626 removing temporary unarchived model dir at /tmp/tmpo65og6vg
2021-11-12 15:24:16.965 Plugin allennlp_models available
2021-11-12 15:24:16.974 Plugin allennlp_server available
2021-11-12 15:24:16.974 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:24:16.975 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpo1lgjlfh
2021-11-12 15:24:19.119 dataset_reader.type = example_reader
2021-11-12 15:24:19.119 dataset_reader.max_instances = None
2021-11-12 15:24:19.119 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:24:19.119 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:24:19.120 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:24:19.120 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:24:19.120 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:24:19.121 type = bert-base-cased
2021-11-12 15:24:19.121 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:24:19.121 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:24:19.121 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:24:19.122 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:24:19.123 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:24:19.123 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:24:19.123 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:24:19.123 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:24:19.123 type = bert-base-cased
2021-11-12 15:24:19.124 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:24:19.124 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:24:19.124 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:24:19.125 dataset_reader.to_index = 6
2021-11-12 15:24:19.125 dataset_reader.type = example_reader
2021-11-12 15:24:19.125 dataset_reader.max_instances = None
2021-11-12 15:24:19.125 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:24:19.126 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:24:19.126 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:24:19.126 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:24:19.126 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:24:19.126 type = bert-base-cased
2021-11-12 15:24:19.126 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:24:19.126 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:24:19.126 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:24:19.127 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:24:19.128 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:24:19.128 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:24:19.128 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:24:19.128 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:24:19.129 type = bert-base-cased
2021-11-12 15:24:19.129 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:24:19.129 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:24:19.129 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:24:19.130 dataset_reader.to_index = 6
2021-11-12 15:24:19.130 type = from_instances
2021-11-12 15:24:19.130 Loading token dictionary from /tmp/tmpo1lgjlfh/vocabulary.
2021-11-12 15:24:19.131 model.type = sentence_level_classifier
2021-11-12 15:24:19.131 model.embedder.type = ref
2021-11-12 15:24:19.132 model.embedder.type = basic
2021-11-12 15:24:19.132 model.embedder.token_embedders.type = ref
2021-11-12 15:24:19.133 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:24:19.134 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:24:19.134 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:24:19.134 type = bert-base-cased
2021-11-12 15:24:19.134 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:24:19.135 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:24:19.135 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:24:19.135 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:24:19.135 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:24:19.135 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:24:19.135 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:24:19.135 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:24:19.135 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:24:19.135 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:24:19.135 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:24:19.184 model.encoder.type = bert_pooler
2021-11-12 15:24:19.185 model.encoder.type = bert_pooler
2021-11-12 15:24:19.185 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:24:19.185 type = bert-base-cased
2021-11-12 15:24:19.185 model.encoder.override_weights_file = None
2021-11-12 15:24:19.185 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:24:19.186 model.encoder.load_weights = True
2021-11-12 15:24:19.186 model.encoder.requires_grad = True
2021-11-12 15:24:19.186 model.encoder.dropout = 0.0
2021-11-12 15:24:19.186 model.encoder.transformer_kwargs = None
2021-11-12 15:24:19.381 removing temporary unarchived model dir at /tmp/tmpo1lgjlfh
2021-11-12 15:24:43.129 Plugin allennlp_models available
2021-11-12 15:24:43.137 Plugin allennlp_server available
2021-11-12 15:24:43.138 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:24:43.138 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp5xs1u5bc
2021-11-12 15:24:45.246 dataset_reader.type = example_reader
2021-11-12 15:24:45.247 dataset_reader.max_instances = None
2021-11-12 15:24:45.247 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:24:45.247 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:24:45.247 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:24:45.247 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:24:45.247 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:24:45.247 type = bert-base-cased
2021-11-12 15:24:45.247 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:24:45.248 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:24:45.248 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:24:45.249 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:24:45.250 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:24:45.250 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:24:45.250 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:24:45.250 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:24:45.251 type = bert-base-cased
2021-11-12 15:24:45.251 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:24:45.251 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:24:45.251 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:24:45.252 dataset_reader.to_index = 6
2021-11-12 15:24:45.252 dataset_reader.type = example_reader
2021-11-12 15:24:45.252 dataset_reader.max_instances = None
2021-11-12 15:24:45.252 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:24:45.253 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:24:45.253 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:24:45.254 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:24:45.254 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:24:45.254 type = bert-base-cased
2021-11-12 15:24:45.255 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:24:45.255 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:24:45.255 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:24:45.257 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:24:45.259 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:24:45.260 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:24:45.260 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:24:45.260 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:24:45.261 type = bert-base-cased
2021-11-12 15:24:45.261 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:24:45.261 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:24:45.261 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:24:45.263 dataset_reader.to_index = 6
2021-11-12 15:24:45.263 type = from_instances
2021-11-12 15:24:45.264 Loading token dictionary from /tmp/tmp5xs1u5bc/vocabulary.
2021-11-12 15:24:45.265 model.type = sentence_level_classifier
2021-11-12 15:24:45.265 model.embedder.type = ref
2021-11-12 15:24:45.267 model.embedder.type = basic
2021-11-12 15:24:45.268 model.embedder.token_embedders.type = ref
2021-11-12 15:24:45.271 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:24:45.271 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:24:45.272 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:24:45.272 type = bert-base-cased
2021-11-12 15:24:45.273 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:24:45.273 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:24:45.273 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:24:45.273 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:24:45.273 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:24:45.274 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:24:45.274 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:24:45.274 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:24:45.274 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:24:45.274 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:24:45.275 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:24:45.327 model.encoder.type = bert_pooler
2021-11-12 15:24:45.327 model.encoder.type = bert_pooler
2021-11-12 15:24:45.327 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:24:45.328 type = bert-base-cased
2021-11-12 15:24:45.328 model.encoder.override_weights_file = None
2021-11-12 15:24:45.328 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:24:45.328 model.encoder.load_weights = True
2021-11-12 15:24:45.328 model.encoder.requires_grad = True
2021-11-12 15:24:45.328 model.encoder.dropout = 0.0
2021-11-12 15:24:45.329 model.encoder.transformer_kwargs = None
2021-11-12 15:24:45.522 removing temporary unarchived model dir at /tmp/tmp5xs1u5bc
2021-11-12 15:25:07.494 Plugin allennlp_models available
2021-11-12 15:25:07.502 Plugin allennlp_server available
2021-11-12 15:25:07.503 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:25:07.504 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpxszle4i1
2021-11-12 15:25:09.634 dataset_reader.type = example_reader
2021-11-12 15:25:09.635 dataset_reader.max_instances = None
2021-11-12 15:25:09.635 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:25:09.635 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:25:09.636 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:25:09.636 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:25:09.636 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:25:09.637 type = bert-base-cased
2021-11-12 15:25:09.637 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:25:09.638 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:25:09.638 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:25:09.640 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:25:09.642 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:25:09.643 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:25:09.643 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:25:09.644 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:25:09.644 type = bert-base-cased
2021-11-12 15:25:09.645 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:25:09.645 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:25:09.645 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:25:09.647 dataset_reader.to_index = 6
2021-11-12 15:25:09.647 dataset_reader.type = example_reader
2021-11-12 15:25:09.648 dataset_reader.max_instances = None
2021-11-12 15:25:09.648 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:25:09.648 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:25:09.649 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:25:09.649 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:25:09.650 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:25:09.650 type = bert-base-cased
2021-11-12 15:25:09.650 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:25:09.650 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:25:09.651 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:25:09.652 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:25:09.654 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:25:09.654 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:25:09.655 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:25:09.655 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:25:09.655 type = bert-base-cased
2021-11-12 15:25:09.655 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:25:09.656 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:25:09.656 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:25:09.657 dataset_reader.to_index = 6
2021-11-12 15:25:09.658 type = from_instances
2021-11-12 15:25:09.658 Loading token dictionary from /tmp/tmpxszle4i1/vocabulary.
2021-11-12 15:25:09.659 model.type = sentence_level_classifier
2021-11-12 15:25:09.659 model.embedder.type = ref
2021-11-12 15:25:09.660 model.embedder.type = basic
2021-11-12 15:25:09.661 model.embedder.token_embedders.type = ref
2021-11-12 15:25:09.665 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:25:09.666 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:25:09.667 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:25:09.667 type = bert-base-cased
2021-11-12 15:25:09.667 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:25:09.667 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:25:09.668 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:25:09.668 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:25:09.668 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:25:09.668 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:25:09.668 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:25:09.669 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:25:09.669 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:25:09.669 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:25:09.669 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:25:09.718 model.encoder.type = bert_pooler
2021-11-12 15:25:09.718 model.encoder.type = bert_pooler
2021-11-12 15:25:09.718 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:25:09.719 type = bert-base-cased
2021-11-12 15:25:09.719 model.encoder.override_weights_file = None
2021-11-12 15:25:09.719 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:25:09.719 model.encoder.load_weights = True
2021-11-12 15:25:09.719 model.encoder.requires_grad = True
2021-11-12 15:25:09.719 model.encoder.dropout = 0.0
2021-11-12 15:25:09.719 model.encoder.transformer_kwargs = None
2021-11-12 15:25:09.909 removing temporary unarchived model dir at /tmp/tmpxszle4i1
2021-11-12 15:25:25.406 Plugin allennlp_models available
2021-11-12 15:25:25.410 Plugin allennlp_server available
2021-11-12 15:25:25.411 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:25:25.411 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp_6mg1rym
2021-11-12 15:25:27.515 dataset_reader.type = example_reader
2021-11-12 15:25:27.516 dataset_reader.max_instances = None
2021-11-12 15:25:27.516 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:25:27.516 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:25:27.516 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:25:27.517 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:25:27.517 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:25:27.517 type = bert-base-cased
2021-11-12 15:25:27.517 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:25:27.518 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:25:27.518 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:25:27.519 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:25:27.520 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:25:27.520 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:25:27.520 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:25:27.520 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:25:27.521 type = bert-base-cased
2021-11-12 15:25:27.521 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:25:27.522 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:25:27.522 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:25:27.523 dataset_reader.to_index = 6
2021-11-12 15:25:27.524 dataset_reader.type = example_reader
2021-11-12 15:25:27.525 dataset_reader.max_instances = None
2021-11-12 15:25:27.525 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:25:27.525 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:25:27.526 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:25:27.526 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:25:27.527 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:25:27.527 type = bert-base-cased
2021-11-12 15:25:27.527 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:25:27.527 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:25:27.528 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:25:27.530 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:25:27.532 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:25:27.532 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:25:27.532 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:25:27.533 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:25:27.533 type = bert-base-cased
2021-11-12 15:25:27.533 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:25:27.534 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:25:27.534 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:25:27.535 dataset_reader.to_index = 6
2021-11-12 15:25:27.536 type = from_instances
2021-11-12 15:25:27.536 Loading token dictionary from /tmp/tmp_6mg1rym/vocabulary.
2021-11-12 15:25:27.537 model.type = sentence_level_classifier
2021-11-12 15:25:27.538 model.embedder.type = ref
2021-11-12 15:25:27.539 model.embedder.type = basic
2021-11-12 15:25:27.540 model.embedder.token_embedders.type = ref
2021-11-12 15:25:27.543 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:25:27.543 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:25:27.544 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:25:27.544 type = bert-base-cased
2021-11-12 15:25:27.544 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:25:27.545 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:25:27.545 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:25:27.545 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:25:27.545 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:25:27.545 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:25:27.545 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:25:27.545 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:25:27.546 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:25:27.546 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:25:27.546 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:25:27.587 model.encoder.type = bert_pooler
2021-11-12 15:25:27.587 model.encoder.type = bert_pooler
2021-11-12 15:25:27.587 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:25:27.587 type = bert-base-cased
2021-11-12 15:25:27.588 model.encoder.override_weights_file = None
2021-11-12 15:25:27.588 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:25:27.588 model.encoder.load_weights = True
2021-11-12 15:25:27.589 model.encoder.requires_grad = True
2021-11-12 15:25:27.589 model.encoder.dropout = 0.0
2021-11-12 15:25:27.589 model.encoder.transformer_kwargs = None
2021-11-12 15:25:27.780 removing temporary unarchived model dir at /tmp/tmp_6mg1rym
2021-11-12 15:25:40.364 Plugin allennlp_models available
2021-11-12 15:25:40.370 Plugin allennlp_server available
2021-11-12 15:25:40.371 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:25:40.371 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpp2s3zfc8
2021-11-12 15:25:42.464 dataset_reader.type = example_reader
2021-11-12 15:25:42.465 dataset_reader.max_instances = None
2021-11-12 15:25:42.465 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:25:42.465 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:25:42.465 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:25:42.465 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:25:42.465 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:25:42.465 type = bert-base-cased
2021-11-12 15:25:42.465 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:25:42.466 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:25:42.466 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:25:42.467 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:25:42.468 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:25:42.468 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:25:42.468 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:25:42.468 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:25:42.469 type = bert-base-cased
2021-11-12 15:25:42.469 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:25:42.469 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:25:42.469 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:25:42.470 dataset_reader.to_index = 6
2021-11-12 15:25:42.470 dataset_reader.type = example_reader
2021-11-12 15:25:42.470 dataset_reader.max_instances = None
2021-11-12 15:25:42.470 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:25:42.470 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:25:42.470 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:25:42.471 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:25:42.471 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:25:42.471 type = bert-base-cased
2021-11-12 15:25:42.471 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:25:42.471 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:25:42.471 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:25:42.472 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:25:42.473 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:25:42.473 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:25:42.473 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:25:42.473 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:25:42.473 type = bert-base-cased
2021-11-12 15:25:42.473 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:25:42.473 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:25:42.473 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:25:42.474 dataset_reader.to_index = 6
2021-11-12 15:25:42.474 type = from_instances
2021-11-12 15:25:42.475 Loading token dictionary from /tmp/tmpp2s3zfc8/vocabulary.
2021-11-12 15:25:42.475 model.type = sentence_level_classifier
2021-11-12 15:25:42.475 model.embedder.type = ref
2021-11-12 15:25:42.476 model.embedder.type = basic
2021-11-12 15:25:42.476 model.embedder.token_embedders.type = ref
2021-11-12 15:25:42.478 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:25:42.478 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:25:42.478 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:25:42.478 type = bert-base-cased
2021-11-12 15:25:42.479 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:25:42.479 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:25:42.479 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:25:42.479 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:25:42.479 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:25:42.479 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:25:42.479 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:25:42.479 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:25:42.479 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:25:42.479 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:25:42.479 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:25:42.521 model.encoder.type = bert_pooler
2021-11-12 15:25:42.521 model.encoder.type = bert_pooler
2021-11-12 15:25:42.521 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:25:42.521 type = bert-base-cased
2021-11-12 15:25:42.521 model.encoder.override_weights_file = None
2021-11-12 15:25:42.522 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:25:42.522 model.encoder.load_weights = True
2021-11-12 15:25:42.522 model.encoder.requires_grad = True
2021-11-12 15:25:42.522 model.encoder.dropout = 0.0
2021-11-12 15:25:42.522 model.encoder.transformer_kwargs = None
2021-11-12 15:25:42.713 removing temporary unarchived model dir at /tmp/tmpp2s3zfc8
2021-11-12 15:26:18.572 Plugin allennlp_models available
2021-11-12 15:26:18.580 Plugin allennlp_server available
2021-11-12 15:26:18.581 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:26:18.581 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpmkpenx9n
2021-11-12 15:26:20.678 dataset_reader.type = example_reader
2021-11-12 15:26:20.678 dataset_reader.max_instances = None
2021-11-12 15:26:20.679 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:26:20.679 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:26:20.679 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:26:20.680 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:26:20.680 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:26:20.680 type = bert-base-cased
2021-11-12 15:26:20.680 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:26:20.681 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:26:20.681 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:26:20.682 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:26:20.683 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:26:20.683 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:26:20.683 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:26:20.683 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:26:20.684 type = bert-base-cased
2021-11-12 15:26:20.684 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:26:20.684 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:26:20.684 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:26:20.685 dataset_reader.to_index = 6
2021-11-12 15:26:20.685 dataset_reader.type = example_reader
2021-11-12 15:26:20.685 dataset_reader.max_instances = None
2021-11-12 15:26:20.685 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:26:20.685 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:26:20.686 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:26:20.686 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:26:20.686 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:26:20.686 type = bert-base-cased
2021-11-12 15:26:20.686 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:26:20.686 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:26:20.686 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:26:20.687 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:26:20.688 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:26:20.688 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:26:20.688 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:26:20.689 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:26:20.689 type = bert-base-cased
2021-11-12 15:26:20.689 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:26:20.689 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:26:20.689 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:26:20.690 dataset_reader.to_index = 6
2021-11-12 15:26:20.690 type = from_instances
2021-11-12 15:26:20.690 Loading token dictionary from /tmp/tmpmkpenx9n/vocabulary.
2021-11-12 15:26:20.691 model.type = sentence_level_classifier
2021-11-12 15:26:20.691 model.embedder.type = ref
2021-11-12 15:26:20.692 model.embedder.type = basic
2021-11-12 15:26:20.692 model.embedder.token_embedders.type = ref
2021-11-12 15:26:20.694 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:26:20.694 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:26:20.694 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:26:20.694 type = bert-base-cased
2021-11-12 15:26:20.694 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:26:20.695 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:26:20.695 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:26:20.695 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:26:20.695 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:26:20.695 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:26:20.695 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:26:20.695 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:26:20.695 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:26:20.695 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:26:20.695 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:26:20.744 model.encoder.type = bert_pooler
2021-11-12 15:26:20.744 model.encoder.type = bert_pooler
2021-11-12 15:26:20.745 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:26:20.745 type = bert-base-cased
2021-11-12 15:26:20.745 model.encoder.override_weights_file = None
2021-11-12 15:26:20.745 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:26:20.745 model.encoder.load_weights = True
2021-11-12 15:26:20.745 model.encoder.requires_grad = True
2021-11-12 15:26:20.746 model.encoder.dropout = 0.0
2021-11-12 15:26:20.746 model.encoder.transformer_kwargs = None
2021-11-12 15:26:20.943 removing temporary unarchived model dir at /tmp/tmpmkpenx9n
2021-11-12 15:26:47.257 Plugin allennlp_models available
2021-11-12 15:26:47.264 Plugin allennlp_server available
2021-11-12 15:26:47.264 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:26:47.265 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpti4bohwc
2021-11-12 15:26:49.370 dataset_reader.type = example_reader
2021-11-12 15:26:49.370 dataset_reader.max_instances = None
2021-11-12 15:26:49.370 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:26:49.370 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:26:49.371 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:26:49.371 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:26:49.371 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:26:49.371 type = bert-base-cased
2021-11-12 15:26:49.372 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:26:49.372 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:26:49.372 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:26:49.373 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:26:49.374 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:26:49.374 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:26:49.374 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:26:49.374 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:26:49.375 type = bert-base-cased
2021-11-12 15:26:49.375 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:26:49.375 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:26:49.375 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:26:49.376 dataset_reader.to_index = 6
2021-11-12 15:26:49.376 dataset_reader.type = example_reader
2021-11-12 15:26:49.376 dataset_reader.max_instances = None
2021-11-12 15:26:49.377 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:26:49.377 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:26:49.377 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:26:49.377 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:26:49.377 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:26:49.378 type = bert-base-cased
2021-11-12 15:26:49.378 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:26:49.378 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:26:49.378 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:26:49.379 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:26:49.379 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:26:49.380 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:26:49.380 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:26:49.380 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:26:49.380 type = bert-base-cased
2021-11-12 15:26:49.380 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:26:49.381 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:26:49.381 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:26:49.382 dataset_reader.to_index = 6
2021-11-12 15:26:49.382 type = from_instances
2021-11-12 15:26:49.382 Loading token dictionary from /tmp/tmpti4bohwc/vocabulary.
2021-11-12 15:26:49.383 model.type = sentence_level_classifier
2021-11-12 15:26:49.383 model.embedder.type = ref
2021-11-12 15:26:49.383 model.embedder.type = basic
2021-11-12 15:26:49.384 model.embedder.token_embedders.type = ref
2021-11-12 15:26:49.385 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:26:49.385 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:26:49.386 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:26:49.386 type = bert-base-cased
2021-11-12 15:26:49.386 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:26:49.386 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:26:49.386 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:26:49.386 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:26:49.386 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:26:49.386 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:26:49.386 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:26:49.386 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:26:49.386 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:26:49.386 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:26:49.386 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:26:49.435 model.encoder.type = bert_pooler
2021-11-12 15:26:49.435 model.encoder.type = bert_pooler
2021-11-12 15:26:49.435 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:26:49.435 type = bert-base-cased
2021-11-12 15:26:49.435 model.encoder.override_weights_file = None
2021-11-12 15:26:49.436 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:26:49.436 model.encoder.load_weights = True
2021-11-12 15:26:49.436 model.encoder.requires_grad = True
2021-11-12 15:26:49.436 model.encoder.dropout = 0.0
2021-11-12 15:26:49.436 model.encoder.transformer_kwargs = None
2021-11-12 15:26:49.633 removing temporary unarchived model dir at /tmp/tmpti4bohwc
2021-11-12 15:27:03.037 Plugin allennlp_models available
2021-11-12 15:27:03.043 Plugin allennlp_server available
2021-11-12 15:27:03.044 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:27:03.044 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp_hqh3om0
2021-11-12 15:27:05.136 dataset_reader.type = example_reader
2021-11-12 15:27:05.136 dataset_reader.max_instances = None
2021-11-12 15:27:05.136 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:27:05.138 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:27:05.138 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:27:05.139 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:27:05.139 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:27:05.139 type = bert-base-cased
2021-11-12 15:27:05.139 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:27:05.140 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:27:05.140 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:27:05.141 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:27:05.142 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:27:05.142 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:27:05.142 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:27:05.143 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:27:05.143 type = bert-base-cased
2021-11-12 15:27:05.143 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:27:05.143 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:27:05.143 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:27:05.144 dataset_reader.to_index = 6
2021-11-12 15:27:05.144 dataset_reader.type = example_reader
2021-11-12 15:27:05.144 dataset_reader.max_instances = None
2021-11-12 15:27:05.144 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:27:05.144 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:27:05.144 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:27:05.144 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:27:05.145 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:27:05.145 type = bert-base-cased
2021-11-12 15:27:05.145 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:27:05.145 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:27:05.145 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:27:05.146 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:27:05.147 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:27:05.147 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:27:05.147 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:27:05.147 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:27:05.148 type = bert-base-cased
2021-11-12 15:27:05.148 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:27:05.148 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:27:05.148 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:27:05.149 dataset_reader.to_index = 6
2021-11-12 15:27:05.149 type = from_instances
2021-11-12 15:27:05.149 Loading token dictionary from /tmp/tmp_hqh3om0/vocabulary.
2021-11-12 15:27:05.150 model.type = sentence_level_classifier
2021-11-12 15:27:05.150 model.embedder.type = ref
2021-11-12 15:27:05.150 model.embedder.type = basic
2021-11-12 15:27:05.151 model.embedder.token_embedders.type = ref
2021-11-12 15:27:05.152 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:27:05.153 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:27:05.153 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:27:05.154 type = bert-base-cased
2021-11-12 15:27:05.154 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:27:05.154 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:27:05.154 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:27:05.155 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:27:05.155 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:27:05.155 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:27:05.155 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:27:05.155 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:27:05.156 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:27:05.156 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:27:05.156 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:27:05.207 model.encoder.type = bert_pooler
2021-11-12 15:27:05.208 model.encoder.type = bert_pooler
2021-11-12 15:27:05.208 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:27:05.208 type = bert-base-cased
2021-11-12 15:27:05.208 model.encoder.override_weights_file = None
2021-11-12 15:27:05.208 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:27:05.208 model.encoder.load_weights = True
2021-11-12 15:27:05.209 model.encoder.requires_grad = True
2021-11-12 15:27:05.209 model.encoder.dropout = 0.0
2021-11-12 15:27:05.209 model.encoder.transformer_kwargs = None
2021-11-12 15:27:05.400 removing temporary unarchived model dir at /tmp/tmp_hqh3om0
2021-11-12 15:27:39.852 Plugin allennlp_models available
2021-11-12 15:27:39.862 Plugin allennlp_server available
2021-11-12 15:27:39.863 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:27:39.864 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmps9al3ctr
2021-11-12 15:27:42.004 dataset_reader.type = example_reader
2021-11-12 15:27:42.004 dataset_reader.max_instances = None
2021-11-12 15:27:42.004 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:27:42.004 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:27:42.004 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:27:42.005 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:27:42.005 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:27:42.005 type = bert-base-cased
2021-11-12 15:27:42.005 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:27:42.005 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:27:42.006 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:27:42.007 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:27:42.008 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:27:42.008 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:27:42.008 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:27:42.008 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:27:42.008 type = bert-base-cased
2021-11-12 15:27:42.009 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:27:42.009 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:27:42.009 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:27:42.010 dataset_reader.to_index = 6
2021-11-12 15:27:42.010 dataset_reader.type = example_reader
2021-11-12 15:27:42.010 dataset_reader.max_instances = None
2021-11-12 15:27:42.010 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:27:42.010 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:27:42.011 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:27:42.011 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:27:42.011 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:27:42.011 type = bert-base-cased
2021-11-12 15:27:42.011 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:27:42.011 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:27:42.012 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:27:42.012 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:27:42.013 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:27:42.013 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:27:42.013 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:27:42.013 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:27:42.014 type = bert-base-cased
2021-11-12 15:27:42.014 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:27:42.014 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:27:42.014 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:27:42.015 dataset_reader.to_index = 6
2021-11-12 15:27:42.015 type = from_instances
2021-11-12 15:27:42.015 Loading token dictionary from /tmp/tmps9al3ctr/vocabulary.
2021-11-12 15:27:42.016 model.type = sentence_level_classifier
2021-11-12 15:27:42.016 model.embedder.type = ref
2021-11-12 15:27:42.017 model.embedder.type = basic
2021-11-12 15:27:42.017 model.embedder.token_embedders.type = ref
2021-11-12 15:27:42.019 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:27:42.019 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:27:42.020 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:27:42.020 type = bert-base-cased
2021-11-12 15:27:42.020 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:27:42.020 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:27:42.020 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:27:42.020 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:27:42.021 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:27:42.021 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:27:42.021 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:27:42.021 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:27:42.021 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:27:42.021 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:27:42.021 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:27:42.072 model.encoder.type = bert_pooler
2021-11-12 15:27:42.072 model.encoder.type = bert_pooler
2021-11-12 15:27:42.072 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:27:42.073 type = bert-base-cased
2021-11-12 15:27:42.073 model.encoder.override_weights_file = None
2021-11-12 15:27:42.073 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:27:42.073 model.encoder.load_weights = True
2021-11-12 15:27:42.073 model.encoder.requires_grad = True
2021-11-12 15:27:42.073 model.encoder.dropout = 0.0
2021-11-12 15:27:42.073 model.encoder.transformer_kwargs = None
2021-11-12 15:27:42.265 removing temporary unarchived model dir at /tmp/tmps9al3ctr
2021-11-12 15:28:18.464 Plugin allennlp_models available
2021-11-12 15:28:18.473 Plugin allennlp_server available
2021-11-12 15:28:18.474 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:28:18.474 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpysnh7ut1
2021-11-12 15:28:20.581 dataset_reader.type = example_reader
2021-11-12 15:28:20.581 dataset_reader.max_instances = None
2021-11-12 15:28:20.581 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:28:20.581 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:28:20.582 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:28:20.582 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:28:20.582 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:28:20.582 type = bert-base-cased
2021-11-12 15:28:20.583 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:28:20.583 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:28:20.583 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:28:20.584 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:28:20.585 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:28:20.585 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:28:20.585 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:28:20.586 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:28:20.586 type = bert-base-cased
2021-11-12 15:28:20.586 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:28:20.586 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:28:20.586 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:28:20.586 dataset_reader.to_index = 6
2021-11-12 15:28:20.587 dataset_reader.type = example_reader
2021-11-12 15:28:20.587 dataset_reader.max_instances = None
2021-11-12 15:28:20.587 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:28:20.587 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:28:20.587 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:28:20.588 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:28:20.588 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:28:20.588 type = bert-base-cased
2021-11-12 15:28:20.588 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:28:20.588 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:28:20.588 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:28:20.589 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:28:20.590 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:28:20.590 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:28:20.590 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:28:20.590 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:28:20.590 type = bert-base-cased
2021-11-12 15:28:20.590 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:28:20.590 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:28:20.590 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:28:20.591 dataset_reader.to_index = 6
2021-11-12 15:28:20.591 type = from_instances
2021-11-12 15:28:20.592 Loading token dictionary from /tmp/tmpysnh7ut1/vocabulary.
2021-11-12 15:28:20.592 model.type = sentence_level_classifier
2021-11-12 15:28:20.592 model.embedder.type = ref
2021-11-12 15:28:20.593 model.embedder.type = basic
2021-11-12 15:28:20.594 model.embedder.token_embedders.type = ref
2021-11-12 15:28:20.595 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:28:20.595 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:28:20.596 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:28:20.596 type = bert-base-cased
2021-11-12 15:28:20.596 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:28:20.596 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:28:20.597 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:28:20.597 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:28:20.597 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:28:20.597 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:28:20.597 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:28:20.597 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:28:20.597 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:28:20.597 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:28:20.597 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:28:20.638 model.encoder.type = bert_pooler
2021-11-12 15:28:20.638 model.encoder.type = bert_pooler
2021-11-12 15:28:20.638 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:28:20.638 type = bert-base-cased
2021-11-12 15:28:20.639 model.encoder.override_weights_file = None
2021-11-12 15:28:20.639 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:28:20.639 model.encoder.load_weights = True
2021-11-12 15:28:20.639 model.encoder.requires_grad = True
2021-11-12 15:28:20.639 model.encoder.dropout = 0.0
2021-11-12 15:28:20.640 model.encoder.transformer_kwargs = None
2021-11-12 15:28:20.842 removing temporary unarchived model dir at /tmp/tmpysnh7ut1
2021-11-12 15:28:27.635 Plugin allennlp_models available
2021-11-12 15:28:27.641 Plugin allennlp_server available
2021-11-12 15:28:27.642 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:28:27.642 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpmyqckh7e
2021-11-12 15:28:29.730 dataset_reader.type = example_reader
2021-11-12 15:28:29.731 dataset_reader.max_instances = None
2021-11-12 15:28:29.731 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:28:29.738 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:28:29.739 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:28:29.739 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:28:29.739 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:28:29.739 type = bert-base-cased
2021-11-12 15:28:29.739 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:28:29.739 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:28:29.740 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:28:29.741 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:28:29.742 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:28:29.742 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:28:29.742 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:28:29.742 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:28:29.742 type = bert-base-cased
2021-11-12 15:28:29.742 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:28:29.742 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:28:29.743 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:28:29.743 dataset_reader.to_index = 6
2021-11-12 15:28:29.744 dataset_reader.type = example_reader
2021-11-12 15:28:29.744 dataset_reader.max_instances = None
2021-11-12 15:28:29.744 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:28:29.744 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:28:29.744 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:28:29.744 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:28:29.745 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:28:29.745 type = bert-base-cased
2021-11-12 15:28:29.745 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:28:29.745 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:28:29.745 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:28:29.746 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:28:29.747 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:28:29.747 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:28:29.747 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:28:29.747 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:28:29.747 type = bert-base-cased
2021-11-12 15:28:29.747 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:28:29.747 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:28:29.747 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:28:29.748 dataset_reader.to_index = 6
2021-11-12 15:28:29.748 type = from_instances
2021-11-12 15:28:29.748 Loading token dictionary from /tmp/tmpmyqckh7e/vocabulary.
2021-11-12 15:28:29.749 model.type = sentence_level_classifier
2021-11-12 15:28:29.749 model.embedder.type = ref
2021-11-12 15:28:29.750 model.embedder.type = basic
2021-11-12 15:28:29.750 model.embedder.token_embedders.type = ref
2021-11-12 15:28:29.752 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:28:29.752 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:28:29.752 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:28:29.752 type = bert-base-cased
2021-11-12 15:28:29.752 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:28:29.752 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:28:29.753 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:28:29.753 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:28:29.753 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:28:29.753 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:28:29.753 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:28:29.753 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:28:29.753 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:28:29.753 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:28:29.753 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:28:29.801 model.encoder.type = bert_pooler
2021-11-12 15:28:29.801 model.encoder.type = bert_pooler
2021-11-12 15:28:29.802 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:28:29.802 type = bert-base-cased
2021-11-12 15:28:29.802 model.encoder.override_weights_file = None
2021-11-12 15:28:29.802 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:28:29.802 model.encoder.load_weights = True
2021-11-12 15:28:29.802 model.encoder.requires_grad = True
2021-11-12 15:28:29.802 model.encoder.dropout = 0.0
2021-11-12 15:28:29.803 model.encoder.transformer_kwargs = None
2021-11-12 15:28:29.998 removing temporary unarchived model dir at /tmp/tmpmyqckh7e
2021-11-12 15:29:34.829 Plugin allennlp_models available
2021-11-12 15:29:34.838 Plugin allennlp_server available
2021-11-12 15:29:34.839 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:29:34.839 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpm0h41bxj
2021-11-12 15:29:36.941 dataset_reader.type = example_reader
2021-11-12 15:29:36.942 dataset_reader.max_instances = None
2021-11-12 15:29:36.942 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:29:36.942 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:29:36.942 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:29:36.942 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:29:36.943 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:29:36.943 type = bert-base-cased
2021-11-12 15:29:36.943 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:29:36.943 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:29:36.944 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:29:36.944 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:29:36.945 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:29:36.946 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:29:36.946 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:29:36.946 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:29:36.946 type = bert-base-cased
2021-11-12 15:29:36.946 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:29:36.946 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:29:36.946 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:29:36.947 dataset_reader.to_index = 6
2021-11-12 15:29:36.947 dataset_reader.type = example_reader
2021-11-12 15:29:36.948 dataset_reader.max_instances = None
2021-11-12 15:29:36.948 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:29:36.948 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:29:36.948 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:29:36.948 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:29:36.949 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:29:36.949 type = bert-base-cased
2021-11-12 15:29:36.949 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:29:36.949 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:29:36.949 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:29:36.950 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:29:36.951 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:29:36.951 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:29:36.951 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:29:36.951 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:29:36.951 type = bert-base-cased
2021-11-12 15:29:36.952 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:29:36.952 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:29:36.952 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:29:36.953 dataset_reader.to_index = 6
2021-11-12 15:29:36.953 type = from_instances
2021-11-12 15:29:36.953 Loading token dictionary from /tmp/tmpm0h41bxj/vocabulary.
2021-11-12 15:29:36.954 model.type = sentence_level_classifier
2021-11-12 15:29:36.954 model.embedder.type = ref
2021-11-12 15:29:36.955 model.embedder.type = basic
2021-11-12 15:29:36.955 model.embedder.token_embedders.type = ref
2021-11-12 15:29:36.957 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:29:36.957 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:29:36.957 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:29:36.958 type = bert-base-cased
2021-11-12 15:29:36.958 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:29:36.958 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:29:36.958 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:29:36.958 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:29:36.958 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:29:36.958 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:29:36.958 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:29:36.958 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:29:36.958 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:29:36.958 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:29:36.958 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:29:37.007 model.encoder.type = bert_pooler
2021-11-12 15:29:37.007 model.encoder.type = bert_pooler
2021-11-12 15:29:37.008 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:29:37.008 type = bert-base-cased
2021-11-12 15:29:37.008 model.encoder.override_weights_file = None
2021-11-12 15:29:37.009 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:29:37.009 model.encoder.load_weights = True
2021-11-12 15:29:37.009 model.encoder.requires_grad = True
2021-11-12 15:29:37.009 model.encoder.dropout = 0.0
2021-11-12 15:29:37.009 model.encoder.transformer_kwargs = None
2021-11-12 15:29:37.197 removing temporary unarchived model dir at /tmp/tmpm0h41bxj
2021-11-12 15:29:59.408 Plugin allennlp_models available
2021-11-12 15:29:59.415 Plugin allennlp_server available
2021-11-12 15:29:59.415 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:29:59.416 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpa9dmfstd
2021-11-12 15:30:01.509 dataset_reader.type = example_reader
2021-11-12 15:30:01.509 dataset_reader.max_instances = None
2021-11-12 15:30:01.509 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:30:01.509 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:30:01.509 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:30:01.510 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:30:01.510 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:30:01.510 type = bert-base-cased
2021-11-12 15:30:01.510 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:30:01.510 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:30:01.510 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:30:01.511 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:30:01.512 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:30:01.512 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:30:01.513 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:30:01.513 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:30:01.513 type = bert-base-cased
2021-11-12 15:30:01.513 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:30:01.513 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:30:01.513 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:30:01.514 dataset_reader.to_index = 6
2021-11-12 15:30:01.514 dataset_reader.type = example_reader
2021-11-12 15:30:01.515 dataset_reader.max_instances = None
2021-11-12 15:30:01.515 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:30:01.515 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:30:01.515 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:30:01.515 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:30:01.515 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:30:01.515 type = bert-base-cased
2021-11-12 15:30:01.515 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:30:01.516 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:30:01.516 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:30:01.516 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:30:01.517 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:30:01.517 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:30:01.517 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:30:01.518 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:30:01.518 type = bert-base-cased
2021-11-12 15:30:01.518 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:30:01.518 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:30:01.518 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:30:01.519 dataset_reader.to_index = 6
2021-11-12 15:30:01.519 type = from_instances
2021-11-12 15:30:01.519 Loading token dictionary from /tmp/tmpa9dmfstd/vocabulary.
2021-11-12 15:30:01.519 model.type = sentence_level_classifier
2021-11-12 15:30:01.520 model.embedder.type = ref
2021-11-12 15:30:01.520 model.embedder.type = basic
2021-11-12 15:30:01.521 model.embedder.token_embedders.type = ref
2021-11-12 15:30:01.522 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:30:01.523 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:30:01.523 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:30:01.524 type = bert-base-cased
2021-11-12 15:30:01.524 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:30:01.524 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:30:01.524 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:30:01.524 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:30:01.524 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:30:01.524 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:30:01.524 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:30:01.524 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:30:01.524 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:30:01.524 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:30:01.524 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:30:01.565 model.encoder.type = bert_pooler
2021-11-12 15:30:01.566 model.encoder.type = bert_pooler
2021-11-12 15:30:01.566 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:30:01.566 type = bert-base-cased
2021-11-12 15:30:01.567 model.encoder.override_weights_file = None
2021-11-12 15:30:01.567 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:30:01.567 model.encoder.load_weights = True
2021-11-12 15:30:01.567 model.encoder.requires_grad = True
2021-11-12 15:30:01.567 model.encoder.dropout = 0.0
2021-11-12 15:30:01.568 model.encoder.transformer_kwargs = None
2021-11-12 15:30:01.760 removing temporary unarchived model dir at /tmp/tmpa9dmfstd
2021-11-12 15:30:12.842 Plugin allennlp_models available
2021-11-12 15:30:12.850 Plugin allennlp_server available
2021-11-12 15:30:12.850 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:30:12.851 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpsu34he0p
2021-11-12 15:30:14.978 dataset_reader.type = example_reader
2021-11-12 15:30:14.979 dataset_reader.max_instances = None
2021-11-12 15:30:14.979 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:30:14.979 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:30:14.980 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:30:14.981 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:30:14.981 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:30:14.981 type = bert-base-cased
2021-11-12 15:30:14.982 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:30:14.982 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:30:14.983 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:30:14.984 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:30:14.986 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:30:14.986 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:30:14.986 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:30:14.987 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:30:14.987 type = bert-base-cased
2021-11-12 15:30:14.987 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:30:14.987 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:30:14.988 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:30:14.989 dataset_reader.to_index = 6
2021-11-12 15:30:14.989 dataset_reader.type = example_reader
2021-11-12 15:30:14.990 dataset_reader.max_instances = None
2021-11-12 15:30:14.990 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:30:14.990 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:30:14.991 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:30:14.991 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:30:14.991 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:30:14.992 type = bert-base-cased
2021-11-12 15:30:14.992 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:30:14.992 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:30:14.992 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:30:14.994 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:30:14.995 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:30:14.995 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:30:14.995 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:30:14.995 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:30:14.996 type = bert-base-cased
2021-11-12 15:30:14.996 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:30:14.996 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:30:14.996 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:30:14.997 dataset_reader.to_index = 6
2021-11-12 15:30:14.998 type = from_instances
2021-11-12 15:30:14.998 Loading token dictionary from /tmp/tmpsu34he0p/vocabulary.
2021-11-12 15:30:14.999 model.type = sentence_level_classifier
2021-11-12 15:30:14.999 model.embedder.type = ref
2021-11-12 15:30:15.000 model.embedder.type = basic
2021-11-12 15:30:15.001 model.embedder.token_embedders.type = ref
2021-11-12 15:30:15.003 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:30:15.003 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:30:15.004 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:30:15.004 type = bert-base-cased
2021-11-12 15:30:15.004 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:30:15.005 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:30:15.005 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:30:15.005 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:30:15.005 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:30:15.005 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:30:15.006 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:30:15.006 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:30:15.006 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:30:15.007 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:30:15.007 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:30:15.064 model.encoder.type = bert_pooler
2021-11-12 15:30:15.065 model.encoder.type = bert_pooler
2021-11-12 15:30:15.065 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:30:15.065 type = bert-base-cased
2021-11-12 15:30:15.065 model.encoder.override_weights_file = None
2021-11-12 15:30:15.065 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:30:15.066 model.encoder.load_weights = True
2021-11-12 15:30:15.066 model.encoder.requires_grad = True
2021-11-12 15:30:15.066 model.encoder.dropout = 0.0
2021-11-12 15:30:15.066 model.encoder.transformer_kwargs = None
2021-11-12 15:30:15.270 removing temporary unarchived model dir at /tmp/tmpsu34he0p
2021-11-12 15:30:43.099 Plugin allennlp_models available
2021-11-12 15:30:43.108 Plugin allennlp_server available
2021-11-12 15:30:43.109 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:30:43.109 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp0j1pokvl
2021-11-12 15:30:45.207 dataset_reader.type = example_reader
2021-11-12 15:30:45.207 dataset_reader.max_instances = None
2021-11-12 15:30:45.207 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:30:45.208 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:30:45.208 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:30:45.209 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:30:45.209 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:30:45.209 type = bert-base-cased
2021-11-12 15:30:45.209 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:30:45.209 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:30:45.209 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:30:45.210 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:30:45.211 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:30:45.211 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:30:45.212 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:30:45.212 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:30:45.212 type = bert-base-cased
2021-11-12 15:30:45.212 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:30:45.213 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:30:45.213 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:30:45.213 dataset_reader.to_index = 6
2021-11-12 15:30:45.214 dataset_reader.type = example_reader
2021-11-12 15:30:45.214 dataset_reader.max_instances = None
2021-11-12 15:30:45.214 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:30:45.214 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:30:45.214 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:30:45.215 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:30:45.215 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:30:45.215 type = bert-base-cased
2021-11-12 15:30:45.215 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:30:45.215 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:30:45.215 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:30:45.216 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:30:45.217 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:30:45.217 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:30:45.217 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:30:45.218 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:30:45.218 type = bert-base-cased
2021-11-12 15:30:45.218 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:30:45.218 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:30:45.218 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:30:45.219 dataset_reader.to_index = 6
2021-11-12 15:30:45.219 type = from_instances
2021-11-12 15:30:45.219 Loading token dictionary from /tmp/tmp0j1pokvl/vocabulary.
2021-11-12 15:30:45.220 model.type = sentence_level_classifier
2021-11-12 15:30:45.220 model.embedder.type = ref
2021-11-12 15:30:45.221 model.embedder.type = basic
2021-11-12 15:30:45.221 model.embedder.token_embedders.type = ref
2021-11-12 15:30:45.223 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:30:45.223 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:30:45.223 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:30:45.224 type = bert-base-cased
2021-11-12 15:30:45.224 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:30:45.224 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:30:45.224 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:30:45.224 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:30:45.224 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:30:45.224 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:30:45.224 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:30:45.224 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:30:45.224 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:30:45.224 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:30:45.225 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:30:45.273 model.encoder.type = bert_pooler
2021-11-12 15:30:45.273 model.encoder.type = bert_pooler
2021-11-12 15:30:45.274 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:30:45.274 type = bert-base-cased
2021-11-12 15:30:45.274 model.encoder.override_weights_file = None
2021-11-12 15:30:45.274 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:30:45.274 model.encoder.load_weights = True
2021-11-12 15:30:45.274 model.encoder.requires_grad = True
2021-11-12 15:30:45.274 model.encoder.dropout = 0.0
2021-11-12 15:30:45.275 model.encoder.transformer_kwargs = None
2021-11-12 15:30:45.466 removing temporary unarchived model dir at /tmp/tmp0j1pokvl
2021-11-12 15:34:37.606 Plugin allennlp_models available
2021-11-12 15:34:37.618 Plugin allennlp_server available
2021-11-12 15:34:37.619 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:34:37.619 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpcxbdzmck
2021-11-12 15:34:39.722 dataset_reader.type = example_reader
2021-11-12 15:34:39.722 dataset_reader.max_instances = None
2021-11-12 15:34:39.722 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:34:39.722 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:34:39.723 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:34:39.723 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:34:39.723 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:34:39.724 type = bert-base-cased
2021-11-12 15:34:39.724 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:34:39.724 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:34:39.724 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:34:39.725 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:34:39.726 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:34:39.726 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:34:39.727 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:34:39.727 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:34:39.727 type = bert-base-cased
2021-11-12 15:34:39.727 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:34:39.727 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:34:39.728 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:34:39.728 dataset_reader.to_index = 6
2021-11-12 15:34:39.728 dataset_reader.type = example_reader
2021-11-12 15:34:39.729 dataset_reader.max_instances = None
2021-11-12 15:34:39.729 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:34:39.729 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:34:39.729 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:34:39.729 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:34:39.729 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:34:39.730 type = bert-base-cased
2021-11-12 15:34:39.730 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:34:39.730 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:34:39.730 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:34:39.731 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:34:39.732 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:34:39.732 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:34:39.732 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:34:39.732 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:34:39.733 type = bert-base-cased
2021-11-12 15:34:39.733 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:34:39.733 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:34:39.734 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:34:39.735 dataset_reader.to_index = 6
2021-11-12 15:34:39.736 type = from_instances
2021-11-12 15:34:39.736 Loading token dictionary from /tmp/tmpcxbdzmck/vocabulary.
2021-11-12 15:34:39.737 model.type = sentence_level_classifier
2021-11-12 15:34:39.738 model.embedder.type = ref
2021-11-12 15:34:39.740 model.embedder.type = basic
2021-11-12 15:34:39.740 model.embedder.token_embedders.type = ref
2021-11-12 15:34:39.744 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:34:39.744 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:34:39.745 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:34:39.746 type = bert-base-cased
2021-11-12 15:34:39.746 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:34:39.746 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:34:39.746 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:34:39.746 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:34:39.747 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:34:39.747 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:34:39.747 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:34:39.747 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:34:39.747 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:34:39.747 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:34:39.748 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:34:39.798 model.encoder.type = bert_pooler
2021-11-12 15:34:39.799 model.encoder.type = bert_pooler
2021-11-12 15:34:39.799 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:34:39.799 type = bert-base-cased
2021-11-12 15:34:39.799 model.encoder.override_weights_file = None
2021-11-12 15:34:39.799 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:34:39.799 model.encoder.load_weights = True
2021-11-12 15:34:39.800 model.encoder.requires_grad = True
2021-11-12 15:34:39.800 model.encoder.dropout = 0.0
2021-11-12 15:34:39.800 model.encoder.transformer_kwargs = None
2021-11-12 15:34:39.989 removing temporary unarchived model dir at /tmp/tmpcxbdzmck
2021-11-12 15:36:18.297 Plugin allennlp_models available
2021-11-12 15:36:18.308 Plugin allennlp_server available
2021-11-12 15:36:18.309 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:36:18.309 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpmz2bj0n1
2021-11-12 15:36:20.413 dataset_reader.type = example_reader
2021-11-12 15:36:20.413 dataset_reader.max_instances = None
2021-11-12 15:36:20.413 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:36:20.414 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:36:20.414 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:36:20.414 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:36:20.414 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:36:20.415 type = bert-base-cased
2021-11-12 15:36:20.415 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:36:20.415 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:36:20.415 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:36:20.416 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:36:20.417 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:36:20.417 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:36:20.418 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:36:20.418 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:36:20.418 type = bert-base-cased
2021-11-12 15:36:20.418 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:36:20.418 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:36:20.418 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:36:20.419 dataset_reader.to_index = 6
2021-11-12 15:36:20.419 dataset_reader.type = example_reader
2021-11-12 15:36:20.419 dataset_reader.max_instances = None
2021-11-12 15:36:20.420 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:36:20.420 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:36:20.420 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:36:20.420 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:36:20.420 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:36:20.421 type = bert-base-cased
2021-11-12 15:36:20.421 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:36:20.421 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:36:20.421 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:36:20.422 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:36:20.422 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:36:20.423 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:36:20.423 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:36:20.423 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:36:20.423 type = bert-base-cased
2021-11-12 15:36:20.423 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:36:20.423 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:36:20.424 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:36:20.424 dataset_reader.to_index = 6
2021-11-12 15:36:20.424 type = from_instances
2021-11-12 15:36:20.424 Loading token dictionary from /tmp/tmpmz2bj0n1/vocabulary.
2021-11-12 15:36:20.425 model.type = sentence_level_classifier
2021-11-12 15:36:20.425 model.embedder.type = ref
2021-11-12 15:36:20.426 model.embedder.type = basic
2021-11-12 15:36:20.426 model.embedder.token_embedders.type = ref
2021-11-12 15:36:20.428 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:36:20.428 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:36:20.429 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:36:20.429 type = bert-base-cased
2021-11-12 15:36:20.429 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:36:20.429 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:36:20.429 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:36:20.429 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:36:20.430 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:36:20.430 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:36:20.430 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:36:20.430 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:36:20.430 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:36:20.430 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:36:20.431 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:36:20.479 model.encoder.type = bert_pooler
2021-11-12 15:36:20.479 model.encoder.type = bert_pooler
2021-11-12 15:36:20.479 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:36:20.479 type = bert-base-cased
2021-11-12 15:36:20.479 model.encoder.override_weights_file = None
2021-11-12 15:36:20.479 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:36:20.480 model.encoder.load_weights = True
2021-11-12 15:36:20.480 model.encoder.requires_grad = True
2021-11-12 15:36:20.480 model.encoder.dropout = 0.0
2021-11-12 15:36:20.480 model.encoder.transformer_kwargs = None
2021-11-12 15:36:20.670 removing temporary unarchived model dir at /tmp/tmpmz2bj0n1
2021-11-12 15:36:47.128 Plugin allennlp_models available
2021-11-12 15:36:47.137 Plugin allennlp_server available
2021-11-12 15:36:47.138 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:36:47.138 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp8u8f2pcv
2021-11-12 15:36:49.239 dataset_reader.type = example_reader
2021-11-12 15:36:49.240 dataset_reader.max_instances = None
2021-11-12 15:36:49.240 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:36:49.240 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:36:49.240 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:36:49.241 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:36:49.241 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:36:49.241 type = bert-base-cased
2021-11-12 15:36:49.241 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:36:49.242 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:36:49.242 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:36:49.243 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:36:49.244 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:36:49.244 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:36:49.244 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:36:49.244 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:36:49.245 type = bert-base-cased
2021-11-12 15:36:49.245 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:36:49.245 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:36:49.245 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:36:49.245 dataset_reader.to_index = 6
2021-11-12 15:36:49.246 dataset_reader.type = example_reader
2021-11-12 15:36:49.246 dataset_reader.max_instances = None
2021-11-12 15:36:49.246 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:36:49.246 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:36:49.246 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:36:49.247 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:36:49.247 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:36:49.247 type = bert-base-cased
2021-11-12 15:36:49.247 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:36:49.247 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:36:49.247 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:36:49.248 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:36:49.249 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:36:49.249 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:36:49.249 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:36:49.250 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:36:49.250 type = bert-base-cased
2021-11-12 15:36:49.250 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:36:49.250 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:36:49.250 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:36:49.251 dataset_reader.to_index = 6
2021-11-12 15:36:49.251 type = from_instances
2021-11-12 15:36:49.251 Loading token dictionary from /tmp/tmp8u8f2pcv/vocabulary.
2021-11-12 15:36:49.252 model.type = sentence_level_classifier
2021-11-12 15:36:49.252 model.embedder.type = ref
2021-11-12 15:36:49.253 model.embedder.type = basic
2021-11-12 15:36:49.254 model.embedder.token_embedders.type = ref
2021-11-12 15:36:49.254 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:36:49.254 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:36:49.255 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:36:49.255 type = bert-base-cased
2021-11-12 15:36:49.255 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:36:49.255 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:36:49.255 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:36:49.255 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:36:49.256 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:36:49.256 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:36:49.256 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:36:49.256 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:36:49.256 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:36:49.256 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:36:49.256 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:36:49.307 model.encoder.type = bert_pooler
2021-11-12 15:36:49.307 model.encoder.type = bert_pooler
2021-11-12 15:36:49.307 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:36:49.307 type = bert-base-cased
2021-11-12 15:36:49.307 model.encoder.override_weights_file = None
2021-11-12 15:36:49.308 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:36:49.308 model.encoder.load_weights = True
2021-11-12 15:36:49.308 model.encoder.requires_grad = True
2021-11-12 15:36:49.308 model.encoder.dropout = 0.0
2021-11-12 15:36:49.308 model.encoder.transformer_kwargs = None
2021-11-12 15:36:49.498 removing temporary unarchived model dir at /tmp/tmp8u8f2pcv
2021-11-12 15:37:48.105 Plugin allennlp_models available
2021-11-12 15:37:48.114 Plugin allennlp_server available
2021-11-12 15:37:48.115 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:37:48.115 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmplwo78yh2
2021-11-12 15:37:50.232 dataset_reader.type = example_reader
2021-11-12 15:37:50.233 dataset_reader.max_instances = None
2021-11-12 15:37:50.233 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:37:50.233 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:37:50.233 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:37:50.234 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:37:50.234 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:37:50.234 type = bert-base-cased
2021-11-12 15:37:50.234 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:37:50.235 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:37:50.235 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:37:50.236 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:37:50.237 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:37:50.237 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:37:50.237 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:37:50.237 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:37:50.237 type = bert-base-cased
2021-11-12 15:37:50.237 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:37:50.238 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:37:50.238 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:37:50.238 dataset_reader.to_index = 6
2021-11-12 15:37:50.239 dataset_reader.type = example_reader
2021-11-12 15:37:50.239 dataset_reader.max_instances = None
2021-11-12 15:37:50.239 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:37:50.239 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:37:50.239 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:37:50.239 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:37:50.240 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:37:50.240 type = bert-base-cased
2021-11-12 15:37:50.240 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:37:50.240 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:37:50.240 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:37:50.241 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:37:50.242 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:37:50.242 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:37:50.242 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:37:50.243 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:37:50.243 type = bert-base-cased
2021-11-12 15:37:50.243 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:37:50.243 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:37:50.243 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:37:50.244 dataset_reader.to_index = 6
2021-11-12 15:37:50.244 type = from_instances
2021-11-12 15:37:50.244 Loading token dictionary from /tmp/tmplwo78yh2/vocabulary.
2021-11-12 15:37:50.245 model.type = sentence_level_classifier
2021-11-12 15:37:50.245 model.embedder.type = ref
2021-11-12 15:37:50.246 model.embedder.type = basic
2021-11-12 15:37:50.246 model.embedder.token_embedders.type = ref
2021-11-12 15:37:50.248 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:37:50.248 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:37:50.248 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:37:50.248 type = bert-base-cased
2021-11-12 15:37:50.248 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:37:50.248 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:37:50.249 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:37:50.249 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:37:50.249 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:37:50.249 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:37:50.249 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:37:50.249 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:37:50.249 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:37:50.249 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:37:50.249 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:37:50.295 model.encoder.type = bert_pooler
2021-11-12 15:37:50.295 model.encoder.type = bert_pooler
2021-11-12 15:37:50.295 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:37:50.295 type = bert-base-cased
2021-11-12 15:37:50.296 model.encoder.override_weights_file = None
2021-11-12 15:37:50.296 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:37:50.296 model.encoder.load_weights = True
2021-11-12 15:37:50.296 model.encoder.requires_grad = True
2021-11-12 15:37:50.296 model.encoder.dropout = 0.0
2021-11-12 15:37:50.296 model.encoder.transformer_kwargs = None
2021-11-12 15:37:50.484 removing temporary unarchived model dir at /tmp/tmplwo78yh2
2021-11-12 15:38:08.035 Plugin allennlp_models available
2021-11-12 15:38:08.041 Plugin allennlp_server available
2021-11-12 15:38:08.042 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:38:08.042 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp5x0bq_gq
2021-11-12 15:38:10.129 dataset_reader.type = example_reader
2021-11-12 15:38:10.129 dataset_reader.max_instances = None
2021-11-12 15:38:10.129 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:38:10.129 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:38:10.130 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:38:10.130 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:38:10.130 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:38:10.130 type = bert-base-cased
2021-11-12 15:38:10.131 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:38:10.131 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:38:10.131 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:38:10.132 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:38:10.133 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:38:10.133 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:38:10.133 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:38:10.133 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:38:10.134 type = bert-base-cased
2021-11-12 15:38:10.134 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:38:10.134 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:38:10.134 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:38:10.135 dataset_reader.to_index = 6
2021-11-12 15:38:10.135 dataset_reader.type = example_reader
2021-11-12 15:38:10.135 dataset_reader.max_instances = None
2021-11-12 15:38:10.135 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:38:10.136 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:38:10.136 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:38:10.136 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:38:10.136 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:38:10.136 type = bert-base-cased
2021-11-12 15:38:10.136 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:38:10.137 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:38:10.137 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:38:10.139 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:38:10.141 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:38:10.142 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:38:10.142 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:38:10.143 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:38:10.143 type = bert-base-cased
2021-11-12 15:38:10.143 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:38:10.144 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:38:10.144 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:38:10.146 dataset_reader.to_index = 6
2021-11-12 15:38:10.146 type = from_instances
2021-11-12 15:38:10.147 Loading token dictionary from /tmp/tmp5x0bq_gq/vocabulary.
2021-11-12 15:38:10.148 model.type = sentence_level_classifier
2021-11-12 15:38:10.148 model.embedder.type = ref
2021-11-12 15:38:10.150 model.embedder.type = basic
2021-11-12 15:38:10.151 model.embedder.token_embedders.type = ref
2021-11-12 15:38:10.155 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:38:10.155 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:38:10.155 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:38:10.156 type = bert-base-cased
2021-11-12 15:38:10.156 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:38:10.156 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:38:10.156 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:38:10.157 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:38:10.157 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:38:10.157 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:38:10.158 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:38:10.158 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:38:10.158 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:38:10.158 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:38:10.158 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:38:10.208 model.encoder.type = bert_pooler
2021-11-12 15:38:10.208 model.encoder.type = bert_pooler
2021-11-12 15:38:10.209 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:38:10.209 type = bert-base-cased
2021-11-12 15:38:10.209 model.encoder.override_weights_file = None
2021-11-12 15:38:10.209 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:38:10.209 model.encoder.load_weights = True
2021-11-12 15:38:10.210 model.encoder.requires_grad = True
2021-11-12 15:38:10.210 model.encoder.dropout = 0.0
2021-11-12 15:38:10.210 model.encoder.transformer_kwargs = None
2021-11-12 15:38:10.400 removing temporary unarchived model dir at /tmp/tmp5x0bq_gq
2021-11-12 15:38:17.817 Plugin allennlp_models available
2021-11-12 15:38:17.824 Plugin allennlp_server available
2021-11-12 15:38:17.824 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:38:17.824 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpajiubwle
2021-11-12 15:38:19.920 dataset_reader.type = example_reader
2021-11-12 15:38:19.920 dataset_reader.max_instances = None
2021-11-12 15:38:19.920 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:38:19.920 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:38:19.920 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:38:19.921 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:38:19.921 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:38:19.921 type = bert-base-cased
2021-11-12 15:38:19.921 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:38:19.921 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:38:19.921 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:38:19.922 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:38:19.923 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:38:19.924 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:38:19.924 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:38:19.924 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:38:19.925 type = bert-base-cased
2021-11-12 15:38:19.925 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:38:19.925 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:38:19.926 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:38:19.927 dataset_reader.to_index = 6
2021-11-12 15:38:19.928 dataset_reader.type = example_reader
2021-11-12 15:38:19.929 dataset_reader.max_instances = None
2021-11-12 15:38:19.929 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:38:19.929 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:38:19.930 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:38:19.930 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:38:19.931 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:38:19.931 type = bert-base-cased
2021-11-12 15:38:19.931 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:38:19.932 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:38:19.932 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:38:19.934 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:38:19.936 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:38:19.936 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:38:19.937 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:38:19.937 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:38:19.938 type = bert-base-cased
2021-11-12 15:38:19.938 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:38:19.938 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:38:19.938 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:38:19.940 dataset_reader.to_index = 6
2021-11-12 15:38:19.941 type = from_instances
2021-11-12 15:38:19.941 Loading token dictionary from /tmp/tmpajiubwle/vocabulary.
2021-11-12 15:38:19.942 model.type = sentence_level_classifier
2021-11-12 15:38:19.943 model.embedder.type = ref
2021-11-12 15:38:19.944 model.embedder.type = basic
2021-11-12 15:38:19.945 model.embedder.token_embedders.type = ref
2021-11-12 15:38:19.948 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:38:19.949 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:38:19.949 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:38:19.949 type = bert-base-cased
2021-11-12 15:38:19.950 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:38:19.950 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:38:19.950 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:38:19.950 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:38:19.951 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:38:19.951 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:38:19.951 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:38:19.951 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:38:19.951 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:38:19.951 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:38:19.952 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:38:20.001 model.encoder.type = bert_pooler
2021-11-12 15:38:20.001 model.encoder.type = bert_pooler
2021-11-12 15:38:20.001 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:38:20.002 type = bert-base-cased
2021-11-12 15:38:20.002 model.encoder.override_weights_file = None
2021-11-12 15:38:20.002 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:38:20.002 model.encoder.load_weights = True
2021-11-12 15:38:20.003 model.encoder.requires_grad = True
2021-11-12 15:38:20.003 model.encoder.dropout = 0.0
2021-11-12 15:38:20.003 model.encoder.transformer_kwargs = None
2021-11-12 15:38:20.196 removing temporary unarchived model dir at /tmp/tmpajiubwle
2021-11-12 15:41:41.091 Plugin allennlp_models available
2021-11-12 15:41:41.100 Plugin allennlp_server available
2021-11-12 15:41:41.101 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:41:41.101 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp27msl1jd
2021-11-12 15:41:43.200 dataset_reader.type = example_reader
2021-11-12 15:41:43.201 dataset_reader.max_instances = None
2021-11-12 15:41:43.201 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:41:43.201 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:41:43.201 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:41:43.201 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:41:43.201 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:41:43.202 type = bert-base-cased
2021-11-12 15:41:43.202 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:41:43.202 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:41:43.202 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:41:43.203 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:41:43.204 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:41:43.204 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:41:43.204 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:41:43.205 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:41:43.205 type = bert-base-cased
2021-11-12 15:41:43.205 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:41:43.205 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:41:43.205 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:41:43.206 dataset_reader.to_index = 6
2021-11-12 15:41:43.206 dataset_reader.type = example_reader
2021-11-12 15:41:43.206 dataset_reader.max_instances = None
2021-11-12 15:41:43.206 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:41:43.206 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:41:43.207 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:41:43.207 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:41:43.207 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:41:43.207 type = bert-base-cased
2021-11-12 15:41:43.207 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:41:43.207 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:41:43.207 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:41:43.208 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:41:43.208 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:41:43.209 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:41:43.209 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:41:43.209 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:41:43.209 type = bert-base-cased
2021-11-12 15:41:43.209 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:41:43.210 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:41:43.210 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:41:43.210 dataset_reader.to_index = 6
2021-11-12 15:41:43.211 type = from_instances
2021-11-12 15:41:43.211 Loading token dictionary from /tmp/tmp27msl1jd/vocabulary.
2021-11-12 15:41:43.211 model.type = sentence_level_classifier
2021-11-12 15:41:43.211 model.embedder.type = ref
2021-11-12 15:41:43.212 model.embedder.type = basic
2021-11-12 15:41:43.212 model.embedder.token_embedders.type = ref
2021-11-12 15:41:43.214 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:41:43.214 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:41:43.214 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:41:43.214 type = bert-base-cased
2021-11-12 15:41:43.215 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:41:43.215 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:41:43.215 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:41:43.215 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:41:43.215 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:41:43.215 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:41:43.215 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:41:43.215 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:41:43.215 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:41:43.215 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:41:43.215 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:41:43.264 model.encoder.type = bert_pooler
2021-11-12 15:41:43.265 model.encoder.type = bert_pooler
2021-11-12 15:41:43.265 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:41:43.265 type = bert-base-cased
2021-11-12 15:41:43.265 model.encoder.override_weights_file = None
2021-11-12 15:41:43.265 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:41:43.266 model.encoder.load_weights = True
2021-11-12 15:41:43.266 model.encoder.requires_grad = True
2021-11-12 15:41:43.266 model.encoder.dropout = 0.0
2021-11-12 15:41:43.266 model.encoder.transformer_kwargs = None
2021-11-12 15:41:43.457 removing temporary unarchived model dir at /tmp/tmp27msl1jd
2021-11-12 15:45:26.392 Plugin allennlp_models available
2021-11-12 15:45:26.398 Plugin allennlp_server available
2021-11-12 15:45:26.399 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:45:26.399 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp6uhei39_
2021-11-12 15:45:28.522 dataset_reader.type = example_reader
2021-11-12 15:45:28.523 dataset_reader.max_instances = None
2021-11-12 15:45:28.523 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:45:28.523 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:45:28.524 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:45:28.524 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:45:28.525 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:45:28.525 type = bert-base-cased
2021-11-12 15:45:28.525 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:45:28.525 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:45:28.525 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:45:28.527 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:45:28.528 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:45:28.528 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:45:28.529 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:45:28.529 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:45:28.529 type = bert-base-cased
2021-11-12 15:45:28.529 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:45:28.530 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:45:28.530 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:45:28.531 dataset_reader.to_index = 6
2021-11-12 15:45:28.531 dataset_reader.type = example_reader
2021-11-12 15:45:28.531 dataset_reader.max_instances = None
2021-11-12 15:45:28.532 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:45:28.532 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:45:28.532 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:45:28.532 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:45:28.533 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:45:28.533 type = bert-base-cased
2021-11-12 15:45:28.533 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:45:28.533 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:45:28.533 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:45:28.534 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:45:28.535 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:45:28.536 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:45:28.536 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:45:28.536 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:45:28.536 type = bert-base-cased
2021-11-12 15:45:28.537 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:45:28.537 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:45:28.537 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:45:28.538 dataset_reader.to_index = 6
2021-11-12 15:45:28.538 type = from_instances
2021-11-12 15:45:28.538 Loading token dictionary from /tmp/tmp6uhei39_/vocabulary.
2021-11-12 15:45:28.539 model.type = sentence_level_classifier
2021-11-12 15:45:28.539 model.embedder.type = ref
2021-11-12 15:45:28.540 model.embedder.type = basic
2021-11-12 15:45:28.540 model.embedder.token_embedders.type = ref
2021-11-12 15:45:28.542 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:45:28.542 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:45:28.542 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:45:28.543 type = bert-base-cased
2021-11-12 15:45:28.543 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:45:28.543 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:45:28.543 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:45:28.543 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:45:28.543 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:45:28.543 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:45:28.544 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:45:28.544 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:45:28.544 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:45:28.544 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:45:28.544 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:45:28.595 model.encoder.type = bert_pooler
2021-11-12 15:45:28.595 model.encoder.type = bert_pooler
2021-11-12 15:45:28.596 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:45:28.596 type = bert-base-cased
2021-11-12 15:45:28.596 model.encoder.override_weights_file = None
2021-11-12 15:45:28.596 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:45:28.596 model.encoder.load_weights = True
2021-11-12 15:45:28.597 model.encoder.requires_grad = True
2021-11-12 15:45:28.597 model.encoder.dropout = 0.0
2021-11-12 15:45:28.597 model.encoder.transformer_kwargs = None
2021-11-12 15:45:28.784 removing temporary unarchived model dir at /tmp/tmp6uhei39_
2021-11-12 15:45:50.799 Plugin allennlp_models available
2021-11-12 15:45:50.805 Plugin allennlp_server available
2021-11-12 15:45:50.806 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:45:50.807 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp89xb9moe
2021-11-12 15:45:52.898 dataset_reader.type = example_reader
2021-11-12 15:45:52.899 dataset_reader.max_instances = None
2021-11-12 15:45:52.899 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:45:52.899 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:45:52.900 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:45:52.900 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:45:52.900 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:45:52.901 type = bert-base-cased
2021-11-12 15:45:52.901 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:45:52.901 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:45:52.901 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:45:52.902 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:45:52.904 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:45:52.904 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:45:52.904 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:45:52.905 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:45:52.905 type = bert-base-cased
2021-11-12 15:45:52.905 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:45:52.905 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:45:52.905 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:45:52.906 dataset_reader.to_index = 6
2021-11-12 15:45:52.906 dataset_reader.type = example_reader
2021-11-12 15:45:52.907 dataset_reader.max_instances = None
2021-11-12 15:45:52.907 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:45:52.907 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:45:52.907 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:45:52.907 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:45:52.908 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:45:52.908 type = bert-base-cased
2021-11-12 15:45:52.908 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:45:52.908 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:45:52.909 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:45:52.910 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:45:52.911 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:45:52.911 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:45:52.911 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:45:52.911 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:45:52.911 type = bert-base-cased
2021-11-12 15:45:52.912 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:45:52.912 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:45:52.912 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:45:52.913 dataset_reader.to_index = 6
2021-11-12 15:45:52.913 type = from_instances
2021-11-12 15:45:52.913 Loading token dictionary from /tmp/tmp89xb9moe/vocabulary.
2021-11-12 15:45:52.914 model.type = sentence_level_classifier
2021-11-12 15:45:52.914 model.embedder.type = ref
2021-11-12 15:45:52.914 model.embedder.type = basic
2021-11-12 15:45:52.915 model.embedder.token_embedders.type = ref
2021-11-12 15:45:52.916 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:45:52.917 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:45:52.917 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:45:52.917 type = bert-base-cased
2021-11-12 15:45:52.917 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:45:52.918 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:45:52.918 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:45:52.918 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:45:52.918 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:45:52.918 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:45:52.918 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:45:52.918 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:45:52.918 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:45:52.918 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:45:52.918 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:45:52.967 model.encoder.type = bert_pooler
2021-11-12 15:45:52.967 model.encoder.type = bert_pooler
2021-11-12 15:45:52.967 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:45:52.968 type = bert-base-cased
2021-11-12 15:45:52.968 model.encoder.override_weights_file = None
2021-11-12 15:45:52.968 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:45:52.968 model.encoder.load_weights = True
2021-11-12 15:45:52.969 model.encoder.requires_grad = True
2021-11-12 15:45:52.969 model.encoder.dropout = 0.0
2021-11-12 15:45:52.969 model.encoder.transformer_kwargs = None
2021-11-12 15:45:53.158 removing temporary unarchived model dir at /tmp/tmp89xb9moe
2021-11-12 15:46:10.910 Plugin allennlp_models available
2021-11-12 15:46:10.916 Plugin allennlp_server available
2021-11-12 15:46:10.917 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:46:10.917 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmprk8pnl4x
2021-11-12 15:46:13.009 dataset_reader.type = example_reader
2021-11-12 15:46:13.009 dataset_reader.max_instances = None
2021-11-12 15:46:13.009 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:46:13.009 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:46:13.009 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:46:13.010 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:46:13.010 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:46:13.010 type = bert-base-cased
2021-11-12 15:46:13.010 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:46:13.011 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:46:13.011 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:46:13.012 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:46:13.013 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:46:13.014 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:46:13.014 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:46:13.014 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:46:13.014 type = bert-base-cased
2021-11-12 15:46:13.015 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:46:13.015 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:46:13.015 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:46:13.016 dataset_reader.to_index = 6
2021-11-12 15:46:13.016 dataset_reader.type = example_reader
2021-11-12 15:46:13.016 dataset_reader.max_instances = None
2021-11-12 15:46:13.016 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:46:13.016 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:46:13.016 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:46:13.017 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:46:13.017 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:46:13.017 type = bert-base-cased
2021-11-12 15:46:13.017 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:46:13.017 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:46:13.017 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:46:13.018 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:46:13.019 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:46:13.019 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:46:13.019 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:46:13.019 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:46:13.020 type = bert-base-cased
2021-11-12 15:46:13.020 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:46:13.020 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:46:13.020 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:46:13.022 dataset_reader.to_index = 6
2021-11-12 15:46:13.022 type = from_instances
2021-11-12 15:46:13.023 Loading token dictionary from /tmp/tmprk8pnl4x/vocabulary.
2021-11-12 15:46:13.024 model.type = sentence_level_classifier
2021-11-12 15:46:13.024 model.embedder.type = ref
2021-11-12 15:46:13.026 model.embedder.type = basic
2021-11-12 15:46:13.026 model.embedder.token_embedders.type = ref
2021-11-12 15:46:13.030 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:46:13.030 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:46:13.031 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:46:13.031 type = bert-base-cased
2021-11-12 15:46:13.031 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:46:13.031 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:46:13.031 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:46:13.032 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:46:13.032 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:46:13.032 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:46:13.032 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:46:13.032 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:46:13.033 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:46:13.033 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:46:13.033 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:46:13.082 model.encoder.type = bert_pooler
2021-11-12 15:46:13.083 model.encoder.type = bert_pooler
2021-11-12 15:46:13.083 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:46:13.083 type = bert-base-cased
2021-11-12 15:46:13.083 model.encoder.override_weights_file = None
2021-11-12 15:46:13.083 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:46:13.084 model.encoder.load_weights = True
2021-11-12 15:46:13.084 model.encoder.requires_grad = True
2021-11-12 15:46:13.084 model.encoder.dropout = 0.0
2021-11-12 15:46:13.084 model.encoder.transformer_kwargs = None
2021-11-12 15:46:13.273 removing temporary unarchived model dir at /tmp/tmprk8pnl4x
2021-11-12 15:47:19.875 Plugin allennlp_models available
2021-11-12 15:47:19.884 Plugin allennlp_server available
2021-11-12 15:47:19.885 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 15:47:19.885 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpy782eadr
2021-11-12 15:47:21.981 dataset_reader.type = example_reader
2021-11-12 15:47:21.981 dataset_reader.max_instances = None
2021-11-12 15:47:21.982 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:47:21.982 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:47:21.982 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:47:21.982 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:47:21.983 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:47:21.983 type = bert-base-cased
2021-11-12 15:47:21.983 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:47:21.983 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:47:21.983 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:47:21.984 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:47:21.985 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:47:21.985 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:47:21.986 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:47:21.986 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:47:21.986 type = bert-base-cased
2021-11-12 15:47:21.986 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:47:21.986 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:47:21.986 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:47:21.987 dataset_reader.to_index = 6
2021-11-12 15:47:21.987 dataset_reader.type = example_reader
2021-11-12 15:47:21.987 dataset_reader.max_instances = None
2021-11-12 15:47:21.987 dataset_reader.manual_distributed_sharding = False
2021-11-12 15:47:21.987 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 15:47:21.987 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:47:21.987 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 15:47:21.988 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 15:47:21.988 type = bert-base-cased
2021-11-12 15:47:21.988 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 15:47:21.988 dataset_reader.tokenizer.max_length = 128
2021-11-12 15:47:21.988 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 15:47:21.989 dataset_reader.text_token_indexers.type = ref
2021-11-12 15:47:21.989 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:47:21.990 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 15:47:21.990 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 15:47:21.990 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 15:47:21.991 type = bert-base-cased
2021-11-12 15:47:21.991 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 15:47:21.991 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 15:47:21.991 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 15:47:21.992 dataset_reader.to_index = 6
2021-11-12 15:47:21.992 type = from_instances
2021-11-12 15:47:21.992 Loading token dictionary from /tmp/tmpy782eadr/vocabulary.
2021-11-12 15:47:21.993 model.type = sentence_level_classifier
2021-11-12 15:47:21.993 model.embedder.type = ref
2021-11-12 15:47:21.994 model.embedder.type = basic
2021-11-12 15:47:21.994 model.embedder.token_embedders.type = ref
2021-11-12 15:47:21.995 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:47:21.996 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 15:47:21.996 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 15:47:21.996 type = bert-base-cased
2021-11-12 15:47:21.996 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 15:47:21.996 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 15:47:21.996 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 15:47:21.997 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 15:47:21.997 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 15:47:21.997 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 15:47:21.997 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 15:47:21.997 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 15:47:21.997 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 15:47:21.997 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 15:47:21.997 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 15:47:22.047 model.encoder.type = bert_pooler
2021-11-12 15:47:22.047 model.encoder.type = bert_pooler
2021-11-12 15:47:22.047 model.encoder.pretrained_model = bert-base-cased
2021-11-12 15:47:22.047 type = bert-base-cased
2021-11-12 15:47:22.048 model.encoder.override_weights_file = None
2021-11-12 15:47:22.048 model.encoder.override_weights_strip_prefix = None
2021-11-12 15:47:22.048 model.encoder.load_weights = True
2021-11-12 15:47:22.048 model.encoder.requires_grad = True
2021-11-12 15:47:22.048 model.encoder.dropout = 0.0
2021-11-12 15:47:22.048 model.encoder.transformer_kwargs = None
2021-11-12 15:47:22.236 removing temporary unarchived model dir at /tmp/tmpy782eadr
2021-11-12 16:03:18.825 Plugin allennlp_models available
2021-11-12 16:03:18.831 Plugin allennlp_server available
2021-11-12 16:03:18.831 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 16:03:18.832 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmptemxrq3_
2021-11-12 16:03:20.920 dataset_reader.type = example_reader
2021-11-12 16:03:20.921 dataset_reader.max_instances = None
2021-11-12 16:03:20.921 dataset_reader.manual_distributed_sharding = False
2021-11-12 16:03:20.921 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 16:03:20.921 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:03:20.921 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:03:20.922 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 16:03:20.922 type = bert-base-cased
2021-11-12 16:03:20.922 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 16:03:20.922 dataset_reader.tokenizer.max_length = 128
2021-11-12 16:03:20.922 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 16:03:20.924 dataset_reader.text_token_indexers.type = ref
2021-11-12 16:03:20.925 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:03:20.925 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:03:20.925 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 16:03:20.925 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 16:03:20.925 type = bert-base-cased
2021-11-12 16:03:20.926 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 16:03:20.926 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 16:03:20.926 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 16:03:20.927 dataset_reader.to_index = 6
2021-11-12 16:03:20.927 dataset_reader.type = example_reader
2021-11-12 16:03:20.927 dataset_reader.max_instances = None
2021-11-12 16:03:20.927 dataset_reader.manual_distributed_sharding = False
2021-11-12 16:03:20.927 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 16:03:20.928 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:03:20.928 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:03:20.928 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 16:03:20.928 type = bert-base-cased
2021-11-12 16:03:20.928 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 16:03:20.929 dataset_reader.tokenizer.max_length = 128
2021-11-12 16:03:20.929 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 16:03:20.930 dataset_reader.text_token_indexers.type = ref
2021-11-12 16:03:20.930 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:03:20.931 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:03:20.931 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 16:03:20.931 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 16:03:20.931 type = bert-base-cased
2021-11-12 16:03:20.932 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 16:03:20.932 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 16:03:20.932 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 16:03:20.933 dataset_reader.to_index = 6
2021-11-12 16:03:20.933 type = from_instances
2021-11-12 16:03:20.933 Loading token dictionary from /tmp/tmptemxrq3_/vocabulary.
2021-11-12 16:03:20.934 model.type = sentence_level_classifier
2021-11-12 16:03:20.934 model.embedder.type = ref
2021-11-12 16:03:20.935 model.embedder.type = basic
2021-11-12 16:03:20.935 model.embedder.token_embedders.type = ref
2021-11-12 16:03:20.936 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 16:03:20.937 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 16:03:20.937 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 16:03:20.937 type = bert-base-cased
2021-11-12 16:03:20.937 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 16:03:20.937 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 16:03:20.937 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 16:03:20.937 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 16:03:20.938 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 16:03:20.938 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 16:03:20.938 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 16:03:20.938 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 16:03:20.938 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 16:03:20.938 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 16:03:20.938 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 16:03:20.985 model.encoder.type = bert_pooler
2021-11-12 16:03:20.986 model.encoder.type = bert_pooler
2021-11-12 16:03:20.986 model.encoder.pretrained_model = bert-base-cased
2021-11-12 16:03:20.986 type = bert-base-cased
2021-11-12 16:03:20.986 model.encoder.override_weights_file = None
2021-11-12 16:03:20.986 model.encoder.override_weights_strip_prefix = None
2021-11-12 16:03:20.987 model.encoder.load_weights = True
2021-11-12 16:03:20.987 model.encoder.requires_grad = True
2021-11-12 16:03:20.987 model.encoder.dropout = 0.0
2021-11-12 16:03:20.987 model.encoder.transformer_kwargs = None
2021-11-12 16:03:21.174 removing temporary unarchived model dir at /tmp/tmptemxrq3_
2021-11-12 16:03:36.291 Plugin allennlp_models available
2021-11-12 16:03:36.301 Plugin allennlp_server available
2021-11-12 16:03:36.302 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 16:03:36.302 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpf3l6gt5y
2021-11-12 16:03:38.414 dataset_reader.type = example_reader
2021-11-12 16:03:38.414 dataset_reader.max_instances = None
2021-11-12 16:03:38.414 dataset_reader.manual_distributed_sharding = False
2021-11-12 16:03:38.414 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 16:03:38.415 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:03:38.415 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:03:38.415 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 16:03:38.416 type = bert-base-cased
2021-11-12 16:03:38.416 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 16:03:38.416 dataset_reader.tokenizer.max_length = 128
2021-11-12 16:03:38.416 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 16:03:38.417 dataset_reader.text_token_indexers.type = ref
2021-11-12 16:03:38.418 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:03:38.418 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:03:38.419 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 16:03:38.419 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 16:03:38.419 type = bert-base-cased
2021-11-12 16:03:38.419 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 16:03:38.419 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 16:03:38.419 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 16:03:38.420 dataset_reader.to_index = 6
2021-11-12 16:03:38.420 dataset_reader.type = example_reader
2021-11-12 16:03:38.420 dataset_reader.max_instances = None
2021-11-12 16:03:38.421 dataset_reader.manual_distributed_sharding = False
2021-11-12 16:03:38.421 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 16:03:38.421 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:03:38.421 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:03:38.421 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 16:03:38.421 type = bert-base-cased
2021-11-12 16:03:38.421 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 16:03:38.421 dataset_reader.tokenizer.max_length = 128
2021-11-12 16:03:38.421 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 16:03:38.422 dataset_reader.text_token_indexers.type = ref
2021-11-12 16:03:38.423 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:03:38.423 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:03:38.423 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 16:03:38.423 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 16:03:38.423 type = bert-base-cased
2021-11-12 16:03:38.423 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 16:03:38.424 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 16:03:38.424 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 16:03:38.424 dataset_reader.to_index = 6
2021-11-12 16:03:38.424 type = from_instances
2021-11-12 16:03:38.425 Loading token dictionary from /tmp/tmpf3l6gt5y/vocabulary.
2021-11-12 16:03:38.425 model.type = sentence_level_classifier
2021-11-12 16:03:38.425 model.embedder.type = ref
2021-11-12 16:03:38.426 model.embedder.type = basic
2021-11-12 16:03:38.426 model.embedder.token_embedders.type = ref
2021-11-12 16:03:38.428 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 16:03:38.428 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 16:03:38.428 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 16:03:38.428 type = bert-base-cased
2021-11-12 16:03:38.428 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 16:03:38.428 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 16:03:38.428 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 16:03:38.429 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 16:03:38.429 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 16:03:38.429 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 16:03:38.429 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 16:03:38.429 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 16:03:38.429 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 16:03:38.429 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 16:03:38.429 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 16:03:38.477 model.encoder.type = bert_pooler
2021-11-12 16:03:38.477 model.encoder.type = bert_pooler
2021-11-12 16:03:38.477 model.encoder.pretrained_model = bert-base-cased
2021-11-12 16:03:38.477 type = bert-base-cased
2021-11-12 16:03:38.478 model.encoder.override_weights_file = None
2021-11-12 16:03:38.478 model.encoder.override_weights_strip_prefix = None
2021-11-12 16:03:38.478 model.encoder.load_weights = True
2021-11-12 16:03:38.478 model.encoder.requires_grad = True
2021-11-12 16:03:38.478 model.encoder.dropout = 0.0
2021-11-12 16:03:38.478 model.encoder.transformer_kwargs = None
2021-11-12 16:03:38.667 removing temporary unarchived model dir at /tmp/tmpf3l6gt5y
2021-11-12 16:10:49.662 Plugin allennlp_models available
2021-11-12 16:10:49.670 Plugin allennlp_server available
2021-11-12 16:10:49.671 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 16:10:49.671 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpqxd0hf9r
2021-11-12 16:10:51.811 dataset_reader.type = example_reader
2021-11-12 16:10:51.811 dataset_reader.max_instances = None
2021-11-12 16:10:51.811 dataset_reader.manual_distributed_sharding = False
2021-11-12 16:10:51.811 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 16:10:51.811 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:10:51.812 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:10:51.812 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 16:10:51.812 type = bert-base-cased
2021-11-12 16:10:51.812 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 16:10:51.812 dataset_reader.tokenizer.max_length = 128
2021-11-12 16:10:51.812 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 16:10:51.813 dataset_reader.text_token_indexers.type = ref
2021-11-12 16:10:51.814 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:10:51.815 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:10:51.815 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 16:10:51.815 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 16:10:51.815 type = bert-base-cased
2021-11-12 16:10:51.815 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 16:10:51.815 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 16:10:51.816 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 16:10:51.817 dataset_reader.to_index = 6
2021-11-12 16:10:51.817 dataset_reader.type = example_reader
2021-11-12 16:10:51.818 dataset_reader.max_instances = None
2021-11-12 16:10:51.818 dataset_reader.manual_distributed_sharding = False
2021-11-12 16:10:51.818 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 16:10:51.819 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:10:51.819 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:10:51.819 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 16:10:51.820 type = bert-base-cased
2021-11-12 16:10:51.820 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 16:10:51.820 dataset_reader.tokenizer.max_length = 128
2021-11-12 16:10:51.821 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 16:10:51.822 dataset_reader.text_token_indexers.type = ref
2021-11-12 16:10:51.824 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:10:51.825 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:10:51.825 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 16:10:51.825 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 16:10:51.826 type = bert-base-cased
2021-11-12 16:10:51.826 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 16:10:51.826 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 16:10:51.826 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 16:10:51.828 dataset_reader.to_index = 6
2021-11-12 16:10:51.829 type = from_instances
2021-11-12 16:10:51.829 Loading token dictionary from /tmp/tmpqxd0hf9r/vocabulary.
2021-11-12 16:10:51.830 model.type = sentence_level_classifier
2021-11-12 16:10:51.830 model.embedder.type = ref
2021-11-12 16:10:51.832 model.embedder.type = basic
2021-11-12 16:10:51.833 model.embedder.token_embedders.type = ref
2021-11-12 16:10:51.837 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 16:10:51.837 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 16:10:51.838 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 16:10:51.839 type = bert-base-cased
2021-11-12 16:10:51.839 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 16:10:51.839 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 16:10:51.839 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 16:10:51.840 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 16:10:51.840 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 16:10:51.840 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 16:10:51.840 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 16:10:51.840 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 16:10:51.841 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 16:10:51.841 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 16:10:51.841 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 16:10:51.892 model.encoder.type = bert_pooler
2021-11-12 16:10:51.892 model.encoder.type = bert_pooler
2021-11-12 16:10:51.892 model.encoder.pretrained_model = bert-base-cased
2021-11-12 16:10:51.892 type = bert-base-cased
2021-11-12 16:10:51.893 model.encoder.override_weights_file = None
2021-11-12 16:10:51.893 model.encoder.override_weights_strip_prefix = None
2021-11-12 16:10:51.893 model.encoder.load_weights = True
2021-11-12 16:10:51.893 model.encoder.requires_grad = True
2021-11-12 16:10:51.893 model.encoder.dropout = 0.0
2021-11-12 16:10:51.893 model.encoder.transformer_kwargs = None
2021-11-12 16:10:52.083 removing temporary unarchived model dir at /tmp/tmpqxd0hf9r
2021-11-12 16:12:05.859 Plugin allennlp_models available
2021-11-12 16:12:05.871 Plugin allennlp_server available
2021-11-12 16:12:05.872 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 16:12:05.872 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp1g9sm1zl
2021-11-12 16:12:07.997 dataset_reader.type = example_reader
2021-11-12 16:12:07.998 dataset_reader.max_instances = None
2021-11-12 16:12:07.998 dataset_reader.manual_distributed_sharding = False
2021-11-12 16:12:07.998 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 16:12:07.998 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:12:07.998 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:12:07.999 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 16:12:07.999 type = bert-base-cased
2021-11-12 16:12:07.999 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 16:12:07.999 dataset_reader.tokenizer.max_length = 128
2021-11-12 16:12:07.999 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 16:12:08.000 dataset_reader.text_token_indexers.type = ref
2021-11-12 16:12:08.001 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:12:08.002 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:12:08.002 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 16:12:08.002 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 16:12:08.002 type = bert-base-cased
2021-11-12 16:12:08.002 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 16:12:08.002 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 16:12:08.002 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 16:12:08.003 dataset_reader.to_index = 6
2021-11-12 16:12:08.003 dataset_reader.type = example_reader
2021-11-12 16:12:08.004 dataset_reader.max_instances = None
2021-11-12 16:12:08.004 dataset_reader.manual_distributed_sharding = False
2021-11-12 16:12:08.004 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 16:12:08.004 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:12:08.004 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:12:08.004 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 16:12:08.004 type = bert-base-cased
2021-11-12 16:12:08.005 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 16:12:08.005 dataset_reader.tokenizer.max_length = 128
2021-11-12 16:12:08.005 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 16:12:08.006 dataset_reader.text_token_indexers.type = ref
2021-11-12 16:12:08.006 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:12:08.007 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:12:08.007 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 16:12:08.007 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 16:12:08.007 type = bert-base-cased
2021-11-12 16:12:08.007 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 16:12:08.007 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 16:12:08.007 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 16:12:08.008 dataset_reader.to_index = 6
2021-11-12 16:12:08.008 type = from_instances
2021-11-12 16:12:08.008 Loading token dictionary from /tmp/tmp1g9sm1zl/vocabulary.
2021-11-12 16:12:08.009 model.type = sentence_level_classifier
2021-11-12 16:12:08.009 model.embedder.type = ref
2021-11-12 16:12:08.010 model.embedder.type = basic
2021-11-12 16:12:08.010 model.embedder.token_embedders.type = ref
2021-11-12 16:12:08.011 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 16:12:08.011 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 16:12:08.011 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 16:12:08.011 type = bert-base-cased
2021-11-12 16:12:08.012 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 16:12:08.012 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 16:12:08.012 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 16:12:08.012 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 16:12:08.012 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 16:12:08.012 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 16:12:08.012 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 16:12:08.012 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 16:12:08.012 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 16:12:08.012 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 16:12:08.012 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 16:12:08.062 model.encoder.type = bert_pooler
2021-11-12 16:12:08.062 model.encoder.type = bert_pooler
2021-11-12 16:12:08.062 model.encoder.pretrained_model = bert-base-cased
2021-11-12 16:12:08.062 type = bert-base-cased
2021-11-12 16:12:08.062 model.encoder.override_weights_file = None
2021-11-12 16:12:08.062 model.encoder.override_weights_strip_prefix = None
2021-11-12 16:12:08.063 model.encoder.load_weights = True
2021-11-12 16:12:08.063 model.encoder.requires_grad = True
2021-11-12 16:12:08.063 model.encoder.dropout = 0.0
2021-11-12 16:12:08.063 model.encoder.transformer_kwargs = None
2021-11-12 16:12:08.253 removing temporary unarchived model dir at /tmp/tmp1g9sm1zl
2021-11-12 16:12:11.139 Plugin allennlp_models available
2021-11-12 16:12:11.146 Plugin allennlp_server available
2021-11-12 16:12:11.146 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 16:12:11.147 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp7ws3j4fo
2021-11-12 16:12:13.247 dataset_reader.type = example_reader
2021-11-12 16:12:13.248 dataset_reader.max_instances = None
2021-11-12 16:12:13.248 dataset_reader.manual_distributed_sharding = False
2021-11-12 16:12:13.248 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 16:12:13.248 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:12:13.248 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:12:13.248 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 16:12:13.249 type = bert-base-cased
2021-11-12 16:12:13.249 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 16:12:13.249 dataset_reader.tokenizer.max_length = 128
2021-11-12 16:12:13.249 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 16:12:13.250 dataset_reader.text_token_indexers.type = ref
2021-11-12 16:12:13.251 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:12:13.252 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:12:13.252 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 16:12:13.252 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 16:12:13.252 type = bert-base-cased
2021-11-12 16:12:13.252 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 16:12:13.253 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 16:12:13.253 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 16:12:13.254 dataset_reader.to_index = 6
2021-11-12 16:12:13.254 dataset_reader.type = example_reader
2021-11-12 16:12:13.254 dataset_reader.max_instances = None
2021-11-12 16:12:13.254 dataset_reader.manual_distributed_sharding = False
2021-11-12 16:12:13.254 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 16:12:13.255 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:12:13.255 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:12:13.255 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 16:12:13.255 type = bert-base-cased
2021-11-12 16:12:13.255 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 16:12:13.256 dataset_reader.tokenizer.max_length = 128
2021-11-12 16:12:13.256 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 16:12:13.256 dataset_reader.text_token_indexers.type = ref
2021-11-12 16:12:13.257 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:12:13.257 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:12:13.258 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 16:12:13.258 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 16:12:13.258 type = bert-base-cased
2021-11-12 16:12:13.258 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 16:12:13.258 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 16:12:13.258 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 16:12:13.259 dataset_reader.to_index = 6
2021-11-12 16:12:13.259 type = from_instances
2021-11-12 16:12:13.259 Loading token dictionary from /tmp/tmp7ws3j4fo/vocabulary.
2021-11-12 16:12:13.260 model.type = sentence_level_classifier
2021-11-12 16:12:13.260 model.embedder.type = ref
2021-11-12 16:12:13.261 model.embedder.type = basic
2021-11-12 16:12:13.261 model.embedder.token_embedders.type = ref
2021-11-12 16:12:13.263 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 16:12:13.263 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 16:12:13.263 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 16:12:13.263 type = bert-base-cased
2021-11-12 16:12:13.263 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 16:12:13.264 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 16:12:13.264 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 16:12:13.264 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 16:12:13.264 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 16:12:13.265 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 16:12:13.265 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 16:12:13.265 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 16:12:13.265 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 16:12:13.266 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 16:12:13.266 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 16:12:13.318 model.encoder.type = bert_pooler
2021-11-12 16:12:13.318 model.encoder.type = bert_pooler
2021-11-12 16:12:13.318 model.encoder.pretrained_model = bert-base-cased
2021-11-12 16:12:13.318 type = bert-base-cased
2021-11-12 16:12:13.318 model.encoder.override_weights_file = None
2021-11-12 16:12:13.319 model.encoder.override_weights_strip_prefix = None
2021-11-12 16:12:13.319 model.encoder.load_weights = True
2021-11-12 16:12:13.319 model.encoder.requires_grad = True
2021-11-12 16:12:13.319 model.encoder.dropout = 0.0
2021-11-12 16:12:13.320 model.encoder.transformer_kwargs = None
2021-11-12 16:12:13.510 removing temporary unarchived model dir at /tmp/tmp7ws3j4fo
2021-11-12 16:12:15.500 Plugin allennlp_models available
2021-11-12 16:12:15.512 Plugin allennlp_server available
2021-11-12 16:12:15.513 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 16:12:15.513 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpmkbuw3ma
2021-11-12 16:12:17.646 dataset_reader.type = example_reader
2021-11-12 16:12:17.647 dataset_reader.max_instances = None
2021-11-12 16:12:17.647 dataset_reader.manual_distributed_sharding = False
2021-11-12 16:12:17.647 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 16:12:17.647 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:12:17.647 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:12:17.648 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 16:12:17.648 type = bert-base-cased
2021-11-12 16:12:17.648 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 16:12:17.648 dataset_reader.tokenizer.max_length = 128
2021-11-12 16:12:17.648 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 16:12:17.649 dataset_reader.text_token_indexers.type = ref
2021-11-12 16:12:17.650 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:12:17.650 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:12:17.651 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 16:12:17.651 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 16:12:17.651 type = bert-base-cased
2021-11-12 16:12:17.651 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 16:12:17.651 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 16:12:17.651 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 16:12:17.652 dataset_reader.to_index = 6
2021-11-12 16:12:17.652 dataset_reader.type = example_reader
2021-11-12 16:12:17.653 dataset_reader.max_instances = None
2021-11-12 16:12:17.653 dataset_reader.manual_distributed_sharding = False
2021-11-12 16:12:17.653 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 16:12:17.653 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:12:17.653 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:12:17.654 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 16:12:17.654 type = bert-base-cased
2021-11-12 16:12:17.654 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 16:12:17.654 dataset_reader.tokenizer.max_length = 128
2021-11-12 16:12:17.654 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 16:12:17.655 dataset_reader.text_token_indexers.type = ref
2021-11-12 16:12:17.656 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:12:17.656 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:12:17.656 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 16:12:17.656 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 16:12:17.656 type = bert-base-cased
2021-11-12 16:12:17.657 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 16:12:17.657 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 16:12:17.657 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 16:12:17.658 dataset_reader.to_index = 6
2021-11-12 16:12:17.658 type = from_instances
2021-11-12 16:12:17.658 Loading token dictionary from /tmp/tmpmkbuw3ma/vocabulary.
2021-11-12 16:12:17.658 model.type = sentence_level_classifier
2021-11-12 16:12:17.659 model.embedder.type = ref
2021-11-12 16:12:17.660 model.embedder.type = basic
2021-11-12 16:12:17.660 model.embedder.token_embedders.type = ref
2021-11-12 16:12:17.661 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 16:12:17.661 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 16:12:17.661 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 16:12:17.661 type = bert-base-cased
2021-11-12 16:12:17.662 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 16:12:17.662 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 16:12:17.662 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 16:12:17.662 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 16:12:17.662 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 16:12:17.662 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 16:12:17.662 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 16:12:17.662 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 16:12:17.662 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 16:12:17.663 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 16:12:17.663 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 16:12:17.711 model.encoder.type = bert_pooler
2021-11-12 16:12:17.711 model.encoder.type = bert_pooler
2021-11-12 16:12:17.711 model.encoder.pretrained_model = bert-base-cased
2021-11-12 16:12:17.712 type = bert-base-cased
2021-11-12 16:12:17.712 model.encoder.override_weights_file = None
2021-11-12 16:12:17.712 model.encoder.override_weights_strip_prefix = None
2021-11-12 16:12:17.712 model.encoder.load_weights = True
2021-11-12 16:12:17.712 model.encoder.requires_grad = True
2021-11-12 16:12:17.712 model.encoder.dropout = 0.0
2021-11-12 16:12:17.712 model.encoder.transformer_kwargs = None
2021-11-12 16:12:17.899 removing temporary unarchived model dir at /tmp/tmpmkbuw3ma
2021-11-12 16:12:37.674 Plugin allennlp_models available
2021-11-12 16:12:37.682 Plugin allennlp_server available
2021-11-12 16:12:37.683 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 16:12:37.684 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp4_qdjoz9
2021-11-12 16:12:39.782 dataset_reader.type = example_reader
2021-11-12 16:12:39.782 dataset_reader.max_instances = None
2021-11-12 16:12:39.782 dataset_reader.manual_distributed_sharding = False
2021-11-12 16:12:39.782 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 16:12:39.783 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:12:39.783 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:12:39.783 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 16:12:39.783 type = bert-base-cased
2021-11-12 16:12:39.784 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 16:12:39.784 dataset_reader.tokenizer.max_length = 128
2021-11-12 16:12:39.784 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 16:12:39.785 dataset_reader.text_token_indexers.type = ref
2021-11-12 16:12:39.786 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:12:39.786 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:12:39.787 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 16:12:39.787 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 16:12:39.787 type = bert-base-cased
2021-11-12 16:12:39.787 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 16:12:39.787 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 16:12:39.787 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 16:12:39.788 dataset_reader.to_index = 6
2021-11-12 16:12:39.788 dataset_reader.type = example_reader
2021-11-12 16:12:39.789 dataset_reader.max_instances = None
2021-11-12 16:12:39.789 dataset_reader.manual_distributed_sharding = False
2021-11-12 16:12:39.789 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 16:12:39.789 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:12:39.789 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:12:39.789 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 16:12:39.789 type = bert-base-cased
2021-11-12 16:12:39.789 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 16:12:39.789 dataset_reader.tokenizer.max_length = 128
2021-11-12 16:12:39.790 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 16:12:39.790 dataset_reader.text_token_indexers.type = ref
2021-11-12 16:12:39.791 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:12:39.791 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:12:39.792 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 16:12:39.792 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 16:12:39.792 type = bert-base-cased
2021-11-12 16:12:39.792 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 16:12:39.792 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 16:12:39.792 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 16:12:39.793 dataset_reader.to_index = 6
2021-11-12 16:12:39.793 type = from_instances
2021-11-12 16:12:39.793 Loading token dictionary from /tmp/tmp4_qdjoz9/vocabulary.
2021-11-12 16:12:39.794 model.type = sentence_level_classifier
2021-11-12 16:12:39.794 model.embedder.type = ref
2021-11-12 16:12:39.795 model.embedder.type = basic
2021-11-12 16:12:39.795 model.embedder.token_embedders.type = ref
2021-11-12 16:12:39.797 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 16:12:39.797 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 16:12:39.797 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 16:12:39.798 type = bert-base-cased
2021-11-12 16:12:39.798 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 16:12:39.798 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 16:12:39.798 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 16:12:39.798 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 16:12:39.798 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 16:12:39.798 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 16:12:39.798 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 16:12:39.798 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 16:12:39.798 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 16:12:39.798 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 16:12:39.798 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 16:12:39.847 model.encoder.type = bert_pooler
2021-11-12 16:12:39.848 model.encoder.type = bert_pooler
2021-11-12 16:12:39.848 model.encoder.pretrained_model = bert-base-cased
2021-11-12 16:12:39.848 type = bert-base-cased
2021-11-12 16:12:39.848 model.encoder.override_weights_file = None
2021-11-12 16:12:39.848 model.encoder.override_weights_strip_prefix = None
2021-11-12 16:12:39.848 model.encoder.load_weights = True
2021-11-12 16:12:39.849 model.encoder.requires_grad = True
2021-11-12 16:12:39.849 model.encoder.dropout = 0.0
2021-11-12 16:12:39.849 model.encoder.transformer_kwargs = None
2021-11-12 16:12:40.048 removing temporary unarchived model dir at /tmp/tmp4_qdjoz9
2021-11-12 16:12:58.551 Plugin allennlp_models available
2021-11-12 16:12:58.560 Plugin allennlp_server available
2021-11-12 16:12:58.561 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 16:12:58.561 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpnysoilbb
2021-11-12 16:13:00.665 dataset_reader.type = example_reader
2021-11-12 16:13:00.665 dataset_reader.max_instances = None
2021-11-12 16:13:00.665 dataset_reader.manual_distributed_sharding = False
2021-11-12 16:13:00.666 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 16:13:00.666 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:13:00.666 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:13:00.666 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 16:13:00.666 type = bert-base-cased
2021-11-12 16:13:00.666 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 16:13:00.666 dataset_reader.tokenizer.max_length = 128
2021-11-12 16:13:00.667 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 16:13:00.668 dataset_reader.text_token_indexers.type = ref
2021-11-12 16:13:00.669 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:13:00.669 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:13:00.669 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 16:13:00.669 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 16:13:00.669 type = bert-base-cased
2021-11-12 16:13:00.670 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 16:13:00.670 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 16:13:00.670 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 16:13:00.671 dataset_reader.to_index = 6
2021-11-12 16:13:00.671 dataset_reader.type = example_reader
2021-11-12 16:13:00.671 dataset_reader.max_instances = None
2021-11-12 16:13:00.671 dataset_reader.manual_distributed_sharding = False
2021-11-12 16:13:00.671 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 16:13:00.672 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:13:00.672 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:13:00.672 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 16:13:00.672 type = bert-base-cased
2021-11-12 16:13:00.672 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 16:13:00.672 dataset_reader.tokenizer.max_length = 128
2021-11-12 16:13:00.673 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 16:13:00.673 dataset_reader.text_token_indexers.type = ref
2021-11-12 16:13:00.674 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:13:00.674 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:13:00.675 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 16:13:00.675 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 16:13:00.675 type = bert-base-cased
2021-11-12 16:13:00.675 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 16:13:00.675 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 16:13:00.675 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 16:13:00.676 dataset_reader.to_index = 6
2021-11-12 16:13:00.676 type = from_instances
2021-11-12 16:13:00.676 Loading token dictionary from /tmp/tmpnysoilbb/vocabulary.
2021-11-12 16:13:00.677 model.type = sentence_level_classifier
2021-11-12 16:13:00.677 model.embedder.type = ref
2021-11-12 16:13:00.678 model.embedder.type = basic
2021-11-12 16:13:00.678 model.embedder.token_embedders.type = ref
2021-11-12 16:13:00.680 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 16:13:00.680 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 16:13:00.680 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 16:13:00.680 type = bert-base-cased
2021-11-12 16:13:00.680 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 16:13:00.681 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 16:13:00.681 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 16:13:00.681 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 16:13:00.681 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 16:13:00.681 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 16:13:00.681 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 16:13:00.681 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 16:13:00.681 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 16:13:00.681 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 16:13:00.681 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 16:13:00.730 model.encoder.type = bert_pooler
2021-11-12 16:13:00.730 model.encoder.type = bert_pooler
2021-11-12 16:13:00.731 model.encoder.pretrained_model = bert-base-cased
2021-11-12 16:13:00.731 type = bert-base-cased
2021-11-12 16:13:00.731 model.encoder.override_weights_file = None
2021-11-12 16:13:00.731 model.encoder.override_weights_strip_prefix = None
2021-11-12 16:13:00.731 model.encoder.load_weights = True
2021-11-12 16:13:00.732 model.encoder.requires_grad = True
2021-11-12 16:13:00.732 model.encoder.dropout = 0.0
2021-11-12 16:13:00.732 model.encoder.transformer_kwargs = None
2021-11-12 16:13:00.920 removing temporary unarchived model dir at /tmp/tmpnysoilbb
2021-11-12 16:15:30.494 Plugin allennlp_models available
2021-11-12 16:15:30.500 Plugin allennlp_server available
2021-11-12 16:15:30.501 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 16:15:30.501 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp3c5wc0vn
2021-11-12 16:15:32.598 dataset_reader.type = example_reader
2021-11-12 16:15:32.598 dataset_reader.max_instances = None
2021-11-12 16:15:32.598 dataset_reader.manual_distributed_sharding = False
2021-11-12 16:15:32.598 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 16:15:32.598 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:15:32.599 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:15:32.599 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 16:15:32.599 type = bert-base-cased
2021-11-12 16:15:32.599 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 16:15:32.599 dataset_reader.tokenizer.max_length = 128
2021-11-12 16:15:32.600 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 16:15:32.601 dataset_reader.text_token_indexers.type = ref
2021-11-12 16:15:32.602 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:15:32.602 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:15:32.602 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 16:15:32.602 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 16:15:32.602 type = bert-base-cased
2021-11-12 16:15:32.603 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 16:15:32.603 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 16:15:32.603 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 16:15:32.603 dataset_reader.to_index = 6
2021-11-12 16:15:32.604 dataset_reader.type = example_reader
2021-11-12 16:15:32.604 dataset_reader.max_instances = None
2021-11-12 16:15:32.604 dataset_reader.manual_distributed_sharding = False
2021-11-12 16:15:32.604 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 16:15:32.604 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:15:32.605 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:15:32.605 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 16:15:32.605 type = bert-base-cased
2021-11-12 16:15:32.605 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 16:15:32.605 dataset_reader.tokenizer.max_length = 128
2021-11-12 16:15:32.605 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 16:15:32.606 dataset_reader.text_token_indexers.type = ref
2021-11-12 16:15:32.607 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:15:32.607 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:15:32.607 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 16:15:32.607 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 16:15:32.608 type = bert-base-cased
2021-11-12 16:15:32.608 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 16:15:32.608 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 16:15:32.608 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 16:15:32.609 dataset_reader.to_index = 6
2021-11-12 16:15:32.609 type = from_instances
2021-11-12 16:15:32.609 Loading token dictionary from /tmp/tmp3c5wc0vn/vocabulary.
2021-11-12 16:15:32.609 model.type = sentence_level_classifier
2021-11-12 16:15:32.610 model.embedder.type = ref
2021-11-12 16:15:32.610 model.embedder.type = basic
2021-11-12 16:15:32.611 model.embedder.token_embedders.type = ref
2021-11-12 16:15:32.611 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 16:15:32.612 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 16:15:32.612 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 16:15:32.612 type = bert-base-cased
2021-11-12 16:15:32.612 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 16:15:32.612 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 16:15:32.613 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 16:15:32.613 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 16:15:32.613 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 16:15:32.613 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 16:15:32.613 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 16:15:32.613 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 16:15:32.613 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 16:15:32.613 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 16:15:32.613 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 16:15:32.664 model.encoder.type = bert_pooler
2021-11-12 16:15:32.664 model.encoder.type = bert_pooler
2021-11-12 16:15:32.665 model.encoder.pretrained_model = bert-base-cased
2021-11-12 16:15:32.665 type = bert-base-cased
2021-11-12 16:15:32.665 model.encoder.override_weights_file = None
2021-11-12 16:15:32.665 model.encoder.override_weights_strip_prefix = None
2021-11-12 16:15:32.666 model.encoder.load_weights = True
2021-11-12 16:15:32.666 model.encoder.requires_grad = True
2021-11-12 16:15:32.666 model.encoder.dropout = 0.0
2021-11-12 16:15:32.666 model.encoder.transformer_kwargs = None
2021-11-12 16:15:32.857 removing temporary unarchived model dir at /tmp/tmp3c5wc0vn
2021-11-12 16:15:50.234 Plugin allennlp_models available
2021-11-12 16:15:50.244 Plugin allennlp_server available
2021-11-12 16:15:50.245 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 16:15:50.246 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpd3j0my06
2021-11-12 16:15:52.355 dataset_reader.type = example_reader
2021-11-12 16:15:52.356 dataset_reader.max_instances = None
2021-11-12 16:15:52.356 dataset_reader.manual_distributed_sharding = False
2021-11-12 16:15:52.356 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 16:15:52.356 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:15:52.356 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:15:52.357 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 16:15:52.357 type = bert-base-cased
2021-11-12 16:15:52.357 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 16:15:52.357 dataset_reader.tokenizer.max_length = 128
2021-11-12 16:15:52.357 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 16:15:52.358 dataset_reader.text_token_indexers.type = ref
2021-11-12 16:15:52.359 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:15:52.359 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:15:52.360 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 16:15:52.360 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 16:15:52.360 type = bert-base-cased
2021-11-12 16:15:52.360 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 16:15:52.360 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 16:15:52.360 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 16:15:52.361 dataset_reader.to_index = 6
2021-11-12 16:15:52.361 dataset_reader.type = example_reader
2021-11-12 16:15:52.361 dataset_reader.max_instances = None
2021-11-12 16:15:52.361 dataset_reader.manual_distributed_sharding = False
2021-11-12 16:15:52.361 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 16:15:52.362 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:15:52.362 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:15:52.362 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 16:15:52.362 type = bert-base-cased
2021-11-12 16:15:52.362 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 16:15:52.362 dataset_reader.tokenizer.max_length = 128
2021-11-12 16:15:52.362 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 16:15:52.363 dataset_reader.text_token_indexers.type = ref
2021-11-12 16:15:52.364 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:15:52.364 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:15:52.364 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 16:15:52.364 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 16:15:52.364 type = bert-base-cased
2021-11-12 16:15:52.365 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 16:15:52.365 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 16:15:52.365 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 16:15:52.366 dataset_reader.to_index = 6
2021-11-12 16:15:52.366 type = from_instances
2021-11-12 16:15:52.366 Loading token dictionary from /tmp/tmpd3j0my06/vocabulary.
2021-11-12 16:15:52.366 model.type = sentence_level_classifier
2021-11-12 16:15:52.367 model.embedder.type = ref
2021-11-12 16:15:52.367 model.embedder.type = basic
2021-11-12 16:15:52.368 model.embedder.token_embedders.type = ref
2021-11-12 16:15:52.369 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 16:15:52.370 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 16:15:52.370 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 16:15:52.370 type = bert-base-cased
2021-11-12 16:15:52.371 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 16:15:52.371 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 16:15:52.371 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 16:15:52.371 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 16:15:52.371 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 16:15:52.371 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 16:15:52.371 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 16:15:52.371 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 16:15:52.371 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 16:15:52.371 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 16:15:52.371 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 16:15:52.418 model.encoder.type = bert_pooler
2021-11-12 16:15:52.418 model.encoder.type = bert_pooler
2021-11-12 16:15:52.418 model.encoder.pretrained_model = bert-base-cased
2021-11-12 16:15:52.419 type = bert-base-cased
2021-11-12 16:15:52.419 model.encoder.override_weights_file = None
2021-11-12 16:15:52.419 model.encoder.override_weights_strip_prefix = None
2021-11-12 16:15:52.419 model.encoder.load_weights = True
2021-11-12 16:15:52.419 model.encoder.requires_grad = True
2021-11-12 16:15:52.420 model.encoder.dropout = 0.0
2021-11-12 16:15:52.420 model.encoder.transformer_kwargs = None
2021-11-12 16:15:52.608 removing temporary unarchived model dir at /tmp/tmpd3j0my06
2021-11-12 16:16:09.810 Plugin allennlp_models available
2021-11-12 16:16:09.815 Plugin allennlp_server available
2021-11-12 16:16:09.815 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 16:16:09.816 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp6dzx3sdx
2021-11-12 16:16:11.907 dataset_reader.type = example_reader
2021-11-12 16:16:11.907 dataset_reader.max_instances = None
2021-11-12 16:16:11.908 dataset_reader.manual_distributed_sharding = False
2021-11-12 16:16:11.908 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 16:16:11.908 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:16:11.908 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:16:11.909 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 16:16:11.909 type = bert-base-cased
2021-11-12 16:16:11.910 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 16:16:11.910 dataset_reader.tokenizer.max_length = 128
2021-11-12 16:16:11.910 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 16:16:11.912 dataset_reader.text_token_indexers.type = ref
2021-11-12 16:16:11.914 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:16:11.914 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:16:11.915 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 16:16:11.915 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 16:16:11.916 type = bert-base-cased
2021-11-12 16:16:11.916 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 16:16:11.916 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 16:16:11.917 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 16:16:11.918 dataset_reader.to_index = 6
2021-11-12 16:16:11.918 dataset_reader.type = example_reader
2021-11-12 16:16:11.919 dataset_reader.max_instances = None
2021-11-12 16:16:11.919 dataset_reader.manual_distributed_sharding = False
2021-11-12 16:16:11.920 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 16:16:11.920 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:16:11.921 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:16:11.921 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 16:16:11.921 type = bert-base-cased
2021-11-12 16:16:11.921 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 16:16:11.922 dataset_reader.tokenizer.max_length = 128
2021-11-12 16:16:11.922 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 16:16:11.923 dataset_reader.text_token_indexers.type = ref
2021-11-12 16:16:11.925 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:16:11.925 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:16:11.926 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 16:16:11.926 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 16:16:11.926 type = bert-base-cased
2021-11-12 16:16:11.927 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 16:16:11.927 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 16:16:11.927 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 16:16:11.928 dataset_reader.to_index = 6
2021-11-12 16:16:11.929 type = from_instances
2021-11-12 16:16:11.929 Loading token dictionary from /tmp/tmp6dzx3sdx/vocabulary.
2021-11-12 16:16:11.930 model.type = sentence_level_classifier
2021-11-12 16:16:11.931 model.embedder.type = ref
2021-11-12 16:16:11.932 model.embedder.type = basic
2021-11-12 16:16:11.932 model.embedder.token_embedders.type = ref
2021-11-12 16:16:11.935 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 16:16:11.935 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 16:16:11.936 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 16:16:11.936 type = bert-base-cased
2021-11-12 16:16:11.936 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 16:16:11.937 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 16:16:11.937 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 16:16:11.937 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 16:16:11.937 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 16:16:11.937 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 16:16:11.937 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 16:16:11.938 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 16:16:11.938 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 16:16:11.938 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 16:16:11.938 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 16:16:11.986 model.encoder.type = bert_pooler
2021-11-12 16:16:11.986 model.encoder.type = bert_pooler
2021-11-12 16:16:11.987 model.encoder.pretrained_model = bert-base-cased
2021-11-12 16:16:11.987 type = bert-base-cased
2021-11-12 16:16:11.987 model.encoder.override_weights_file = None
2021-11-12 16:16:11.987 model.encoder.override_weights_strip_prefix = None
2021-11-12 16:16:11.987 model.encoder.load_weights = True
2021-11-12 16:16:11.987 model.encoder.requires_grad = True
2021-11-12 16:16:11.987 model.encoder.dropout = 0.0
2021-11-12 16:16:11.988 model.encoder.transformer_kwargs = None
2021-11-12 16:16:12.170 removing temporary unarchived model dir at /tmp/tmp6dzx3sdx
2021-11-12 16:18:13.484 Plugin allennlp_models available
2021-11-12 16:18:13.494 Plugin allennlp_server available
2021-11-12 16:18:13.494 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 16:18:13.495 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmphiq8oe65
2021-11-12 16:18:15.598 dataset_reader.type = example_reader
2021-11-12 16:18:15.598 dataset_reader.max_instances = None
2021-11-12 16:18:15.598 dataset_reader.manual_distributed_sharding = False
2021-11-12 16:18:15.598 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 16:18:15.598 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:18:15.599 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:18:15.599 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 16:18:15.599 type = bert-base-cased
2021-11-12 16:18:15.599 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 16:18:15.599 dataset_reader.tokenizer.max_length = 128
2021-11-12 16:18:15.599 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 16:18:15.600 dataset_reader.text_token_indexers.type = ref
2021-11-12 16:18:15.601 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:18:15.601 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:18:15.602 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 16:18:15.602 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 16:18:15.602 type = bert-base-cased
2021-11-12 16:18:15.602 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 16:18:15.602 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 16:18:15.602 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 16:18:15.603 dataset_reader.to_index = 6
2021-11-12 16:18:15.603 dataset_reader.type = example_reader
2021-11-12 16:18:15.603 dataset_reader.max_instances = None
2021-11-12 16:18:15.603 dataset_reader.manual_distributed_sharding = False
2021-11-12 16:18:15.603 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 16:18:15.603 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:18:15.603 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:18:15.604 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 16:18:15.604 type = bert-base-cased
2021-11-12 16:18:15.604 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 16:18:15.604 dataset_reader.tokenizer.max_length = 128
2021-11-12 16:18:15.604 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 16:18:15.605 dataset_reader.text_token_indexers.type = ref
2021-11-12 16:18:15.605 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:18:15.605 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:18:15.606 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 16:18:15.606 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 16:18:15.606 type = bert-base-cased
2021-11-12 16:18:15.606 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 16:18:15.606 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 16:18:15.606 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 16:18:15.607 dataset_reader.to_index = 6
2021-11-12 16:18:15.607 type = from_instances
2021-11-12 16:18:15.607 Loading token dictionary from /tmp/tmphiq8oe65/vocabulary.
2021-11-12 16:18:15.608 model.type = sentence_level_classifier
2021-11-12 16:18:15.608 model.embedder.type = ref
2021-11-12 16:18:15.609 model.embedder.type = basic
2021-11-12 16:18:15.609 model.embedder.token_embedders.type = ref
2021-11-12 16:18:15.610 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 16:18:15.610 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 16:18:15.610 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 16:18:15.610 type = bert-base-cased
2021-11-12 16:18:15.610 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 16:18:15.611 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 16:18:15.611 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 16:18:15.611 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 16:18:15.611 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 16:18:15.611 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 16:18:15.611 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 16:18:15.611 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 16:18:15.611 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 16:18:15.611 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 16:18:15.611 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 16:18:15.660 model.encoder.type = bert_pooler
2021-11-12 16:18:15.660 model.encoder.type = bert_pooler
2021-11-12 16:18:15.661 model.encoder.pretrained_model = bert-base-cased
2021-11-12 16:18:15.661 type = bert-base-cased
2021-11-12 16:18:15.661 model.encoder.override_weights_file = None
2021-11-12 16:18:15.661 model.encoder.override_weights_strip_prefix = None
2021-11-12 16:18:15.661 model.encoder.load_weights = True
2021-11-12 16:18:15.661 model.encoder.requires_grad = True
2021-11-12 16:18:15.661 model.encoder.dropout = 0.0
2021-11-12 16:18:15.662 model.encoder.transformer_kwargs = None
2021-11-12 16:18:15.850 removing temporary unarchived model dir at /tmp/tmphiq8oe65
2021-11-12 16:19:33.303 Plugin allennlp_models available
2021-11-12 16:19:33.310 Plugin allennlp_server available
2021-11-12 16:19:33.311 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 16:19:33.311 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp_goliicy
2021-11-12 16:19:35.442 dataset_reader.type = example_reader
2021-11-12 16:19:35.442 dataset_reader.max_instances = None
2021-11-12 16:19:35.442 dataset_reader.manual_distributed_sharding = False
2021-11-12 16:19:35.442 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 16:19:35.443 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:19:35.443 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:19:35.443 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 16:19:35.443 type = bert-base-cased
2021-11-12 16:19:35.443 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 16:19:35.443 dataset_reader.tokenizer.max_length = 128
2021-11-12 16:19:35.444 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 16:19:35.445 dataset_reader.text_token_indexers.type = ref
2021-11-12 16:19:35.446 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:19:35.446 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:19:35.446 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 16:19:35.446 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 16:19:35.446 type = bert-base-cased
2021-11-12 16:19:35.446 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 16:19:35.446 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 16:19:35.446 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 16:19:35.447 dataset_reader.to_index = 6
2021-11-12 16:19:35.447 dataset_reader.type = example_reader
2021-11-12 16:19:35.447 dataset_reader.max_instances = None
2021-11-12 16:19:35.448 dataset_reader.manual_distributed_sharding = False
2021-11-12 16:19:35.448 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 16:19:35.448 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:19:35.448 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:19:35.448 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 16:19:35.448 type = bert-base-cased
2021-11-12 16:19:35.448 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 16:19:35.448 dataset_reader.tokenizer.max_length = 128
2021-11-12 16:19:35.449 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 16:19:35.451 dataset_reader.text_token_indexers.type = ref
2021-11-12 16:19:35.452 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:19:35.453 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:19:35.453 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 16:19:35.454 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 16:19:35.454 type = bert-base-cased
2021-11-12 16:19:35.454 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 16:19:35.455 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 16:19:35.455 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 16:19:35.456 dataset_reader.to_index = 6
2021-11-12 16:19:35.457 type = from_instances
2021-11-12 16:19:35.457 Loading token dictionary from /tmp/tmp_goliicy/vocabulary.
2021-11-12 16:19:35.458 model.type = sentence_level_classifier
2021-11-12 16:19:35.459 model.embedder.type = ref
2021-11-12 16:19:35.461 model.embedder.type = basic
2021-11-12 16:19:35.461 model.embedder.token_embedders.type = ref
2021-11-12 16:19:35.465 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 16:19:35.465 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 16:19:35.466 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 16:19:35.466 type = bert-base-cased
2021-11-12 16:19:35.467 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 16:19:35.467 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 16:19:35.467 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 16:19:35.467 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 16:19:35.467 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 16:19:35.468 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 16:19:35.468 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 16:19:35.468 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 16:19:35.468 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 16:19:35.468 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 16:19:35.469 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 16:19:35.517 model.encoder.type = bert_pooler
2021-11-12 16:19:35.517 model.encoder.type = bert_pooler
2021-11-12 16:19:35.517 model.encoder.pretrained_model = bert-base-cased
2021-11-12 16:19:35.518 type = bert-base-cased
2021-11-12 16:19:35.518 model.encoder.override_weights_file = None
2021-11-12 16:19:35.518 model.encoder.override_weights_strip_prefix = None
2021-11-12 16:19:35.518 model.encoder.load_weights = True
2021-11-12 16:19:35.518 model.encoder.requires_grad = True
2021-11-12 16:19:35.519 model.encoder.dropout = 0.0
2021-11-12 16:19:35.519 model.encoder.transformer_kwargs = None
2021-11-12 16:19:35.709 removing temporary unarchived model dir at /tmp/tmp_goliicy
2021-11-12 16:20:49.942 Plugin allennlp_models available
2021-11-12 16:20:49.950 Plugin allennlp_server available
2021-11-12 16:20:49.951 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 16:20:49.951 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpi8wvmiyg
2021-11-12 16:20:52.058 dataset_reader.type = example_reader
2021-11-12 16:20:52.059 dataset_reader.max_instances = None
2021-11-12 16:20:52.059 dataset_reader.manual_distributed_sharding = False
2021-11-12 16:20:52.059 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 16:20:52.059 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:20:52.060 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:20:52.060 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 16:20:52.060 type = bert-base-cased
2021-11-12 16:20:52.060 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 16:20:52.060 dataset_reader.tokenizer.max_length = 128
2021-11-12 16:20:52.060 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 16:20:52.061 dataset_reader.text_token_indexers.type = ref
2021-11-12 16:20:52.062 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:20:52.062 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:20:52.063 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 16:20:52.063 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 16:20:52.063 type = bert-base-cased
2021-11-12 16:20:52.063 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 16:20:52.063 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 16:20:52.063 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 16:20:52.064 dataset_reader.to_index = 6
2021-11-12 16:20:52.064 dataset_reader.type = example_reader
2021-11-12 16:20:52.064 dataset_reader.max_instances = None
2021-11-12 16:20:52.064 dataset_reader.manual_distributed_sharding = False
2021-11-12 16:20:52.065 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 16:20:52.065 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:20:52.065 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:20:52.065 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 16:20:52.065 type = bert-base-cased
2021-11-12 16:20:52.065 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 16:20:52.065 dataset_reader.tokenizer.max_length = 128
2021-11-12 16:20:52.065 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 16:20:52.066 dataset_reader.text_token_indexers.type = ref
2021-11-12 16:20:52.067 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:20:52.067 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:20:52.067 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 16:20:52.067 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 16:20:52.068 type = bert-base-cased
2021-11-12 16:20:52.068 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 16:20:52.068 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 16:20:52.068 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 16:20:52.069 dataset_reader.to_index = 6
2021-11-12 16:20:52.069 type = from_instances
2021-11-12 16:20:52.069 Loading token dictionary from /tmp/tmpi8wvmiyg/vocabulary.
2021-11-12 16:20:52.070 model.type = sentence_level_classifier
2021-11-12 16:20:52.070 model.embedder.type = ref
2021-11-12 16:20:52.070 model.embedder.type = basic
2021-11-12 16:20:52.071 model.embedder.token_embedders.type = ref
2021-11-12 16:20:52.072 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 16:20:52.073 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 16:20:52.073 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 16:20:52.073 type = bert-base-cased
2021-11-12 16:20:52.073 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 16:20:52.073 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 16:20:52.073 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 16:20:52.074 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 16:20:52.074 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 16:20:52.074 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 16:20:52.074 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 16:20:52.074 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 16:20:52.074 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 16:20:52.074 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 16:20:52.074 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 16:20:52.124 model.encoder.type = bert_pooler
2021-11-12 16:20:52.124 model.encoder.type = bert_pooler
2021-11-12 16:20:52.125 model.encoder.pretrained_model = bert-base-cased
2021-11-12 16:20:52.125 type = bert-base-cased
2021-11-12 16:20:52.125 model.encoder.override_weights_file = None
2021-11-12 16:20:52.125 model.encoder.override_weights_strip_prefix = None
2021-11-12 16:20:52.125 model.encoder.load_weights = True
2021-11-12 16:20:52.125 model.encoder.requires_grad = True
2021-11-12 16:20:52.125 model.encoder.dropout = 0.0
2021-11-12 16:20:52.125 model.encoder.transformer_kwargs = None
2021-11-12 16:20:52.315 removing temporary unarchived model dir at /tmp/tmpi8wvmiyg
2021-11-12 16:21:58.922 Plugin allennlp_models available
2021-11-12 16:21:58.932 Plugin allennlp_server available
2021-11-12 16:21:58.933 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 16:21:58.933 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpupg214al
2021-11-12 16:22:01.033 dataset_reader.type = example_reader
2021-11-12 16:22:01.033 dataset_reader.max_instances = None
2021-11-12 16:22:01.034 dataset_reader.manual_distributed_sharding = False
2021-11-12 16:22:01.034 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 16:22:01.034 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:22:01.034 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:22:01.034 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 16:22:01.035 type = bert-base-cased
2021-11-12 16:22:01.035 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 16:22:01.035 dataset_reader.tokenizer.max_length = 128
2021-11-12 16:22:01.035 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 16:22:01.036 dataset_reader.text_token_indexers.type = ref
2021-11-12 16:22:01.037 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:22:01.037 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:22:01.037 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 16:22:01.037 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 16:22:01.038 type = bert-base-cased
2021-11-12 16:22:01.038 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 16:22:01.038 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 16:22:01.038 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 16:22:01.038 dataset_reader.to_index = 6
2021-11-12 16:22:01.039 dataset_reader.type = example_reader
2021-11-12 16:22:01.039 dataset_reader.max_instances = None
2021-11-12 16:22:01.039 dataset_reader.manual_distributed_sharding = False
2021-11-12 16:22:01.039 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 16:22:01.039 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:22:01.039 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:22:01.040 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 16:22:01.040 type = bert-base-cased
2021-11-12 16:22:01.040 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 16:22:01.040 dataset_reader.tokenizer.max_length = 128
2021-11-12 16:22:01.040 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 16:22:01.042 dataset_reader.text_token_indexers.type = ref
2021-11-12 16:22:01.044 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:22:01.044 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:22:01.045 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 16:22:01.045 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 16:22:01.045 type = bert-base-cased
2021-11-12 16:22:01.046 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 16:22:01.046 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 16:22:01.046 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 16:22:01.048 dataset_reader.to_index = 6
2021-11-12 16:22:01.048 type = from_instances
2021-11-12 16:22:01.049 Loading token dictionary from /tmp/tmpupg214al/vocabulary.
2021-11-12 16:22:01.049 model.type = sentence_level_classifier
2021-11-12 16:22:01.050 model.embedder.type = ref
2021-11-12 16:22:01.052 model.embedder.type = basic
2021-11-12 16:22:01.052 model.embedder.token_embedders.type = ref
2021-11-12 16:22:01.056 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 16:22:01.056 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 16:22:01.057 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 16:22:01.058 type = bert-base-cased
2021-11-12 16:22:01.058 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 16:22:01.058 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 16:22:01.058 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 16:22:01.058 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 16:22:01.059 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 16:22:01.059 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 16:22:01.059 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 16:22:01.059 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 16:22:01.060 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 16:22:01.060 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 16:22:01.060 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 16:22:01.107 model.encoder.type = bert_pooler
2021-11-12 16:22:01.107 model.encoder.type = bert_pooler
2021-11-12 16:22:01.107 model.encoder.pretrained_model = bert-base-cased
2021-11-12 16:22:01.108 type = bert-base-cased
2021-11-12 16:22:01.108 model.encoder.override_weights_file = None
2021-11-12 16:22:01.108 model.encoder.override_weights_strip_prefix = None
2021-11-12 16:22:01.108 model.encoder.load_weights = True
2021-11-12 16:22:01.108 model.encoder.requires_grad = True
2021-11-12 16:22:01.108 model.encoder.dropout = 0.0
2021-11-12 16:22:01.109 model.encoder.transformer_kwargs = None
2021-11-12 16:22:01.297 removing temporary unarchived model dir at /tmp/tmpupg214al
2021-11-12 16:35:05.243 Plugin allennlp_models available
2021-11-12 16:35:05.254 Plugin allennlp_server available
2021-11-12 16:35:05.255 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 16:35:05.255 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp06ffi5gj
2021-11-12 16:35:07.402 dataset_reader.type = example_reader
2021-11-12 16:35:07.402 dataset_reader.max_instances = None
2021-11-12 16:35:07.402 dataset_reader.manual_distributed_sharding = False
2021-11-12 16:35:07.402 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 16:35:07.402 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:35:07.403 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:35:07.403 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 16:35:07.403 type = bert-base-cased
2021-11-12 16:35:07.403 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 16:35:07.403 dataset_reader.tokenizer.max_length = 128
2021-11-12 16:35:07.404 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 16:35:07.404 dataset_reader.text_token_indexers.type = ref
2021-11-12 16:35:07.405 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:35:07.406 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:35:07.406 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 16:35:07.406 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 16:35:07.406 type = bert-base-cased
2021-11-12 16:35:07.406 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 16:35:07.406 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 16:35:07.406 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 16:35:07.407 dataset_reader.to_index = 6
2021-11-12 16:35:07.407 dataset_reader.type = example_reader
2021-11-12 16:35:07.408 dataset_reader.max_instances = None
2021-11-12 16:35:07.408 dataset_reader.manual_distributed_sharding = False
2021-11-12 16:35:07.408 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 16:35:07.408 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:35:07.408 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 16:35:07.408 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 16:35:07.408 type = bert-base-cased
2021-11-12 16:35:07.408 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 16:35:07.408 dataset_reader.tokenizer.max_length = 128
2021-11-12 16:35:07.408 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 16:35:07.409 dataset_reader.text_token_indexers.type = ref
2021-11-12 16:35:07.410 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:35:07.410 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 16:35:07.410 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 16:35:07.410 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 16:35:07.410 type = bert-base-cased
2021-11-12 16:35:07.410 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 16:35:07.410 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 16:35:07.410 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 16:35:07.411 dataset_reader.to_index = 6
2021-11-12 16:35:07.411 type = from_instances
2021-11-12 16:35:07.412 Loading token dictionary from /tmp/tmp06ffi5gj/vocabulary.
2021-11-12 16:35:07.412 model.type = sentence_level_classifier
2021-11-12 16:35:07.412 model.embedder.type = ref
2021-11-12 16:35:07.414 model.embedder.type = basic
2021-11-12 16:35:07.415 model.embedder.token_embedders.type = ref
2021-11-12 16:35:07.417 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 16:35:07.417 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 16:35:07.418 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 16:35:07.418 type = bert-base-cased
2021-11-12 16:35:07.419 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 16:35:07.419 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 16:35:07.419 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 16:35:07.419 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 16:35:07.420 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 16:35:07.420 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 16:35:07.420 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 16:35:07.420 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 16:35:07.420 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 16:35:07.421 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 16:35:07.421 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 16:35:07.469 model.encoder.type = bert_pooler
2021-11-12 16:35:07.470 model.encoder.type = bert_pooler
2021-11-12 16:35:07.470 model.encoder.pretrained_model = bert-base-cased
2021-11-12 16:35:07.470 type = bert-base-cased
2021-11-12 16:35:07.470 model.encoder.override_weights_file = None
2021-11-12 16:35:07.470 model.encoder.override_weights_strip_prefix = None
2021-11-12 16:35:07.471 model.encoder.load_weights = True
2021-11-12 16:35:07.471 model.encoder.requires_grad = True
2021-11-12 16:35:07.471 model.encoder.dropout = 0.0
2021-11-12 16:35:07.471 model.encoder.transformer_kwargs = None
2021-11-12 16:35:07.661 removing temporary unarchived model dir at /tmp/tmp06ffi5gj
2021-11-12 17:06:11.466 Plugin allennlp_models available
2021-11-12 17:06:11.478 Plugin allennlp_server available
2021-11-12 17:06:11.479 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 17:06:11.479 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpmcue1pe6
2021-11-12 17:06:13.602 dataset_reader.type = example_reader
2021-11-12 17:06:13.602 dataset_reader.max_instances = None
2021-11-12 17:06:13.602 dataset_reader.manual_distributed_sharding = False
2021-11-12 17:06:13.602 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 17:06:13.602 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 17:06:13.603 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 17:06:13.603 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 17:06:13.603 type = bert-base-cased
2021-11-12 17:06:13.604 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 17:06:13.604 dataset_reader.tokenizer.max_length = 128
2021-11-12 17:06:13.604 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 17:06:13.605 dataset_reader.text_token_indexers.type = ref
2021-11-12 17:06:13.606 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 17:06:13.606 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 17:06:13.606 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 17:06:13.607 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 17:06:13.607 type = bert-base-cased
2021-11-12 17:06:13.607 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 17:06:13.607 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 17:06:13.607 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 17:06:13.608 dataset_reader.to_index = 6
2021-11-12 17:06:13.608 dataset_reader.type = example_reader
2021-11-12 17:06:13.608 dataset_reader.max_instances = None
2021-11-12 17:06:13.609 dataset_reader.manual_distributed_sharding = False
2021-11-12 17:06:13.609 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 17:06:13.609 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 17:06:13.609 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 17:06:13.609 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 17:06:13.609 type = bert-base-cased
2021-11-12 17:06:13.609 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 17:06:13.609 dataset_reader.tokenizer.max_length = 128
2021-11-12 17:06:13.609 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 17:06:13.610 dataset_reader.text_token_indexers.type = ref
2021-11-12 17:06:13.611 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 17:06:13.611 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 17:06:13.611 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 17:06:13.611 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 17:06:13.612 type = bert-base-cased
2021-11-12 17:06:13.612 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 17:06:13.612 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 17:06:13.612 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 17:06:13.613 dataset_reader.to_index = 6
2021-11-12 17:06:13.613 type = from_instances
2021-11-12 17:06:13.613 Loading token dictionary from /tmp/tmpmcue1pe6/vocabulary.
2021-11-12 17:06:13.613 model.type = sentence_level_classifier
2021-11-12 17:06:13.614 model.embedder.type = ref
2021-11-12 17:06:13.615 model.embedder.type = basic
2021-11-12 17:06:13.615 model.embedder.token_embedders.type = ref
2021-11-12 17:06:13.616 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 17:06:13.616 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 17:06:13.616 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 17:06:13.617 type = bert-base-cased
2021-11-12 17:06:13.617 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 17:06:13.617 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 17:06:13.617 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 17:06:13.617 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 17:06:13.617 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 17:06:13.617 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 17:06:13.617 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 17:06:13.617 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 17:06:13.617 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 17:06:13.617 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 17:06:13.617 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 17:06:13.668 model.encoder.type = bert_pooler
2021-11-12 17:06:13.668 model.encoder.type = bert_pooler
2021-11-12 17:06:13.668 model.encoder.pretrained_model = bert-base-cased
2021-11-12 17:06:13.669 type = bert-base-cased
2021-11-12 17:06:13.669 model.encoder.override_weights_file = None
2021-11-12 17:06:13.669 model.encoder.override_weights_strip_prefix = None
2021-11-12 17:06:13.669 model.encoder.load_weights = True
2021-11-12 17:06:13.669 model.encoder.requires_grad = True
2021-11-12 17:06:13.669 model.encoder.dropout = 0.0
2021-11-12 17:06:13.669 model.encoder.transformer_kwargs = None
2021-11-12 17:06:13.862 removing temporary unarchived model dir at /tmp/tmpmcue1pe6
2021-11-12 18:39:01.596 Plugin allennlp_models available
2021-11-12 18:39:01.604 Plugin allennlp_server available
2021-11-12 18:39:01.605 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 18:39:01.605 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpdveiytbd
2021-11-12 18:39:03.731 dataset_reader.type = example_reader
2021-11-12 18:39:03.731 dataset_reader.max_instances = None
2021-11-12 18:39:03.732 dataset_reader.manual_distributed_sharding = False
2021-11-12 18:39:03.732 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 18:39:03.733 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 18:39:03.733 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 18:39:03.733 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 18:39:03.734 type = bert-base-cased
2021-11-12 18:39:03.734 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 18:39:03.734 dataset_reader.tokenizer.max_length = 128
2021-11-12 18:39:03.734 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 18:39:03.736 dataset_reader.text_token_indexers.type = ref
2021-11-12 18:39:03.737 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 18:39:03.737 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 18:39:03.738 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 18:39:03.738 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 18:39:03.738 type = bert-base-cased
2021-11-12 18:39:03.738 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 18:39:03.738 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 18:39:03.739 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 18:39:03.739 dataset_reader.to_index = 6
2021-11-12 18:39:03.740 dataset_reader.type = example_reader
2021-11-12 18:39:03.740 dataset_reader.max_instances = None
2021-11-12 18:39:03.740 dataset_reader.manual_distributed_sharding = False
2021-11-12 18:39:03.740 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 18:39:03.741 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 18:39:03.741 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 18:39:03.741 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 18:39:03.741 type = bert-base-cased
2021-11-12 18:39:03.742 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 18:39:03.742 dataset_reader.tokenizer.max_length = 128
2021-11-12 18:39:03.742 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 18:39:03.743 dataset_reader.text_token_indexers.type = ref
2021-11-12 18:39:03.744 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 18:39:03.744 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 18:39:03.745 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 18:39:03.745 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 18:39:03.745 type = bert-base-cased
2021-11-12 18:39:03.745 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 18:39:03.746 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 18:39:03.746 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 18:39:03.746 dataset_reader.to_index = 6
2021-11-12 18:39:03.747 type = from_instances
2021-11-12 18:39:03.747 Loading token dictionary from /tmp/tmpdveiytbd/vocabulary.
2021-11-12 18:39:03.747 model.type = sentence_level_classifier
2021-11-12 18:39:03.748 model.embedder.type = ref
2021-11-12 18:39:03.749 model.embedder.type = basic
2021-11-12 18:39:03.749 model.embedder.token_embedders.type = ref
2021-11-12 18:39:03.750 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 18:39:03.750 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 18:39:03.750 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 18:39:03.751 type = bert-base-cased
2021-11-12 18:39:03.751 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 18:39:03.751 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 18:39:03.751 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 18:39:03.751 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 18:39:03.751 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 18:39:03.751 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 18:39:03.751 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 18:39:03.751 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 18:39:03.751 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 18:39:03.751 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 18:39:03.751 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 18:39:03.800 model.encoder.type = bert_pooler
2021-11-12 18:39:03.800 model.encoder.type = bert_pooler
2021-11-12 18:39:03.800 model.encoder.pretrained_model = bert-base-cased
2021-11-12 18:39:03.800 type = bert-base-cased
2021-11-12 18:39:03.801 model.encoder.override_weights_file = None
2021-11-12 18:39:03.801 model.encoder.override_weights_strip_prefix = None
2021-11-12 18:39:03.801 model.encoder.load_weights = True
2021-11-12 18:39:03.801 model.encoder.requires_grad = True
2021-11-12 18:39:03.801 model.encoder.dropout = 0.0
2021-11-12 18:39:03.801 model.encoder.transformer_kwargs = None
2021-11-12 18:39:03.991 removing temporary unarchived model dir at /tmp/tmpdveiytbd
2021-11-12 18:39:11.348 Plugin allennlp_models available
2021-11-12 18:39:11.359 Plugin allennlp_server available
2021-11-12 18:39:11.359 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 18:39:11.360 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp75lsbnjm
2021-11-12 18:39:13.466 dataset_reader.type = example_reader
2021-11-12 18:39:13.466 dataset_reader.max_instances = None
2021-11-12 18:39:13.466 dataset_reader.manual_distributed_sharding = False
2021-11-12 18:39:13.466 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 18:39:13.467 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 18:39:13.467 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 18:39:13.467 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 18:39:13.467 type = bert-base-cased
2021-11-12 18:39:13.468 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 18:39:13.468 dataset_reader.tokenizer.max_length = 128
2021-11-12 18:39:13.468 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 18:39:13.469 dataset_reader.text_token_indexers.type = ref
2021-11-12 18:39:13.470 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 18:39:13.470 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 18:39:13.470 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 18:39:13.470 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 18:39:13.470 type = bert-base-cased
2021-11-12 18:39:13.470 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 18:39:13.470 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 18:39:13.471 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 18:39:13.471 dataset_reader.to_index = 6
2021-11-12 18:39:13.472 dataset_reader.type = example_reader
2021-11-12 18:39:13.472 dataset_reader.max_instances = None
2021-11-12 18:39:13.472 dataset_reader.manual_distributed_sharding = False
2021-11-12 18:39:13.472 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 18:39:13.472 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 18:39:13.472 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 18:39:13.473 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 18:39:13.473 type = bert-base-cased
2021-11-12 18:39:13.473 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 18:39:13.473 dataset_reader.tokenizer.max_length = 128
2021-11-12 18:39:13.473 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 18:39:13.474 dataset_reader.text_token_indexers.type = ref
2021-11-12 18:39:13.475 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 18:39:13.475 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 18:39:13.475 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 18:39:13.475 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 18:39:13.475 type = bert-base-cased
2021-11-12 18:39:13.475 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 18:39:13.476 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 18:39:13.476 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 18:39:13.477 dataset_reader.to_index = 6
2021-11-12 18:39:13.477 type = from_instances
2021-11-12 18:39:13.477 Loading token dictionary from /tmp/tmp75lsbnjm/vocabulary.
2021-11-12 18:39:13.477 model.type = sentence_level_classifier
2021-11-12 18:39:13.478 model.embedder.type = ref
2021-11-12 18:39:13.478 model.embedder.type = basic
2021-11-12 18:39:13.479 model.embedder.token_embedders.type = ref
2021-11-12 18:39:13.479 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 18:39:13.480 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 18:39:13.480 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 18:39:13.480 type = bert-base-cased
2021-11-12 18:39:13.480 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 18:39:13.480 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 18:39:13.480 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 18:39:13.480 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 18:39:13.480 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 18:39:13.480 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 18:39:13.481 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 18:39:13.481 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 18:39:13.481 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 18:39:13.481 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 18:39:13.481 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 18:39:13.532 model.encoder.type = bert_pooler
2021-11-12 18:39:13.532 model.encoder.type = bert_pooler
2021-11-12 18:39:13.532 model.encoder.pretrained_model = bert-base-cased
2021-11-12 18:39:13.532 type = bert-base-cased
2021-11-12 18:39:13.532 model.encoder.override_weights_file = None
2021-11-12 18:39:13.532 model.encoder.override_weights_strip_prefix = None
2021-11-12 18:39:13.533 model.encoder.load_weights = True
2021-11-12 18:39:13.533 model.encoder.requires_grad = True
2021-11-12 18:39:13.533 model.encoder.dropout = 0.0
2021-11-12 18:39:13.533 model.encoder.transformer_kwargs = None
2021-11-12 18:39:13.729 removing temporary unarchived model dir at /tmp/tmp75lsbnjm
2021-11-12 18:54:47.052 Plugin allennlp_models available
2021-11-12 18:54:47.059 Plugin allennlp_server available
2021-11-12 18:54:47.060 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 18:54:47.060 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpavempxgo
2021-11-12 18:54:49.160 dataset_reader.type = example_reader
2021-11-12 18:54:49.161 dataset_reader.max_instances = None
2021-11-12 18:54:49.161 dataset_reader.manual_distributed_sharding = False
2021-11-12 18:54:49.161 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 18:54:49.161 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 18:54:49.162 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 18:54:49.162 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 18:54:49.162 type = bert-base-cased
2021-11-12 18:54:49.162 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 18:54:49.162 dataset_reader.tokenizer.max_length = 128
2021-11-12 18:54:49.163 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 18:54:49.164 dataset_reader.text_token_indexers.type = ref
2021-11-12 18:54:49.165 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 18:54:49.165 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 18:54:49.165 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 18:54:49.165 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 18:54:49.166 type = bert-base-cased
2021-11-12 18:54:49.166 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 18:54:49.166 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 18:54:49.166 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 18:54:49.167 dataset_reader.to_index = 6
2021-11-12 18:54:49.167 dataset_reader.type = example_reader
2021-11-12 18:54:49.167 dataset_reader.max_instances = None
2021-11-12 18:54:49.167 dataset_reader.manual_distributed_sharding = False
2021-11-12 18:54:49.167 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 18:54:49.168 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 18:54:49.168 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 18:54:49.168 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 18:54:49.168 type = bert-base-cased
2021-11-12 18:54:49.168 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 18:54:49.169 dataset_reader.tokenizer.max_length = 128
2021-11-12 18:54:49.169 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 18:54:49.170 dataset_reader.text_token_indexers.type = ref
2021-11-12 18:54:49.170 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 18:54:49.171 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 18:54:49.171 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 18:54:49.171 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 18:54:49.171 type = bert-base-cased
2021-11-12 18:54:49.171 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 18:54:49.172 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 18:54:49.172 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 18:54:49.172 dataset_reader.to_index = 6
2021-11-12 18:54:49.173 type = from_instances
2021-11-12 18:54:49.173 Loading token dictionary from /tmp/tmpavempxgo/vocabulary.
2021-11-12 18:54:49.173 model.type = sentence_level_classifier
2021-11-12 18:54:49.173 model.embedder.type = ref
2021-11-12 18:54:49.174 model.embedder.type = basic
2021-11-12 18:54:49.174 model.embedder.token_embedders.type = ref
2021-11-12 18:54:49.175 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 18:54:49.175 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 18:54:49.176 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 18:54:49.176 type = bert-base-cased
2021-11-12 18:54:49.176 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 18:54:49.176 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 18:54:49.176 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 18:54:49.176 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 18:54:49.176 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 18:54:49.176 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 18:54:49.177 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 18:54:49.177 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 18:54:49.177 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 18:54:49.177 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 18:54:49.177 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 18:54:49.225 model.encoder.type = bert_pooler
2021-11-12 18:54:49.226 model.encoder.type = bert_pooler
2021-11-12 18:54:49.226 model.encoder.pretrained_model = bert-base-cased
2021-11-12 18:54:49.226 type = bert-base-cased
2021-11-12 18:54:49.226 model.encoder.override_weights_file = None
2021-11-12 18:54:49.226 model.encoder.override_weights_strip_prefix = None
2021-11-12 18:54:49.227 model.encoder.load_weights = True
2021-11-12 18:54:49.227 model.encoder.requires_grad = True
2021-11-12 18:54:49.227 model.encoder.dropout = 0.0
2021-11-12 18:54:49.227 model.encoder.transformer_kwargs = None
2021-11-12 18:54:49.417 removing temporary unarchived model dir at /tmp/tmpavempxgo
2021-11-12 19:03:50.139 Plugin allennlp_models available
2021-11-12 19:03:50.151 Plugin allennlp_server available
2021-11-12 19:03:50.151 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 19:03:50.152 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmply2jlw9a
2021-11-12 19:03:52.277 dataset_reader.type = example_reader
2021-11-12 19:03:52.277 dataset_reader.max_instances = None
2021-11-12 19:03:52.278 dataset_reader.manual_distributed_sharding = False
2021-11-12 19:03:52.278 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 19:03:52.278 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 19:03:52.278 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 19:03:52.278 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 19:03:52.278 type = bert-base-cased
2021-11-12 19:03:52.279 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 19:03:52.279 dataset_reader.tokenizer.max_length = 128
2021-11-12 19:03:52.279 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 19:03:52.280 dataset_reader.text_token_indexers.type = ref
2021-11-12 19:03:52.281 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 19:03:52.281 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 19:03:52.281 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 19:03:52.282 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 19:03:52.282 type = bert-base-cased
2021-11-12 19:03:52.282 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 19:03:52.282 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 19:03:52.282 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 19:03:52.283 dataset_reader.to_index = 6
2021-11-12 19:03:52.283 dataset_reader.type = example_reader
2021-11-12 19:03:52.283 dataset_reader.max_instances = None
2021-11-12 19:03:52.283 dataset_reader.manual_distributed_sharding = False
2021-11-12 19:03:52.283 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 19:03:52.284 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 19:03:52.284 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 19:03:52.284 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 19:03:52.284 type = bert-base-cased
2021-11-12 19:03:52.284 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 19:03:52.284 dataset_reader.tokenizer.max_length = 128
2021-11-12 19:03:52.284 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 19:03:52.285 dataset_reader.text_token_indexers.type = ref
2021-11-12 19:03:52.285 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 19:03:52.286 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 19:03:52.286 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 19:03:52.286 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 19:03:52.286 type = bert-base-cased
2021-11-12 19:03:52.286 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 19:03:52.286 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 19:03:52.286 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 19:03:52.287 dataset_reader.to_index = 6
2021-11-12 19:03:52.287 type = from_instances
2021-11-12 19:03:52.287 Loading token dictionary from /tmp/tmply2jlw9a/vocabulary.
2021-11-12 19:03:52.288 model.type = sentence_level_classifier
2021-11-12 19:03:52.288 model.embedder.type = ref
2021-11-12 19:03:52.289 model.embedder.type = basic
2021-11-12 19:03:52.289 model.embedder.token_embedders.type = ref
2021-11-12 19:03:52.290 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 19:03:52.290 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 19:03:52.290 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 19:03:52.290 type = bert-base-cased
2021-11-12 19:03:52.290 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 19:03:52.290 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 19:03:52.290 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 19:03:52.290 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 19:03:52.290 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 19:03:52.291 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 19:03:52.291 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 19:03:52.291 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 19:03:52.291 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 19:03:52.291 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 19:03:52.291 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 19:03:52.338 model.encoder.type = bert_pooler
2021-11-12 19:03:52.338 model.encoder.type = bert_pooler
2021-11-12 19:03:52.339 model.encoder.pretrained_model = bert-base-cased
2021-11-12 19:03:52.339 type = bert-base-cased
2021-11-12 19:03:52.339 model.encoder.override_weights_file = None
2021-11-12 19:03:52.339 model.encoder.override_weights_strip_prefix = None
2021-11-12 19:03:52.339 model.encoder.load_weights = True
2021-11-12 19:03:52.339 model.encoder.requires_grad = True
2021-11-12 19:03:52.340 model.encoder.dropout = 0.0
2021-11-12 19:03:52.340 model.encoder.transformer_kwargs = None
2021-11-12 19:03:52.528 removing temporary unarchived model dir at /tmp/tmply2jlw9a
2021-11-12 19:03:59.915 Plugin allennlp_models available
2021-11-12 19:03:59.927 Plugin allennlp_server available
2021-11-12 19:03:59.927 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 19:03:59.928 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp1qmrnbr5
2021-11-12 19:04:02.026 dataset_reader.type = example_reader
2021-11-12 19:04:02.026 dataset_reader.max_instances = None
2021-11-12 19:04:02.026 dataset_reader.manual_distributed_sharding = False
2021-11-12 19:04:02.026 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 19:04:02.027 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 19:04:02.027 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 19:04:02.027 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 19:04:02.028 type = bert-base-cased
2021-11-12 19:04:02.028 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 19:04:02.028 dataset_reader.tokenizer.max_length = 128
2021-11-12 19:04:02.028 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 19:04:02.029 dataset_reader.text_token_indexers.type = ref
2021-11-12 19:04:02.030 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 19:04:02.030 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 19:04:02.030 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 19:04:02.031 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 19:04:02.031 type = bert-base-cased
2021-11-12 19:04:02.031 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 19:04:02.031 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 19:04:02.031 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 19:04:02.032 dataset_reader.to_index = 6
2021-11-12 19:04:02.032 dataset_reader.type = example_reader
2021-11-12 19:04:02.032 dataset_reader.max_instances = None
2021-11-12 19:04:02.032 dataset_reader.manual_distributed_sharding = False
2021-11-12 19:04:02.032 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 19:04:02.032 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 19:04:02.032 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 19:04:02.033 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 19:04:02.033 type = bert-base-cased
2021-11-12 19:04:02.033 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 19:04:02.033 dataset_reader.tokenizer.max_length = 128
2021-11-12 19:04:02.033 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 19:04:02.034 dataset_reader.text_token_indexers.type = ref
2021-11-12 19:04:02.035 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 19:04:02.035 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 19:04:02.035 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 19:04:02.035 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 19:04:02.035 type = bert-base-cased
2021-11-12 19:04:02.036 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 19:04:02.036 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 19:04:02.036 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 19:04:02.037 dataset_reader.to_index = 6
2021-11-12 19:04:02.037 type = from_instances
2021-11-12 19:04:02.037 Loading token dictionary from /tmp/tmp1qmrnbr5/vocabulary.
2021-11-12 19:04:02.038 model.type = sentence_level_classifier
2021-11-12 19:04:02.038 model.embedder.type = ref
2021-11-12 19:04:02.039 model.embedder.type = basic
2021-11-12 19:04:02.039 model.embedder.token_embedders.type = ref
2021-11-12 19:04:02.040 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 19:04:02.040 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 19:04:02.041 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 19:04:02.041 type = bert-base-cased
2021-11-12 19:04:02.041 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 19:04:02.041 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 19:04:02.041 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 19:04:02.041 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 19:04:02.041 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 19:04:02.041 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 19:04:02.041 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 19:04:02.041 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 19:04:02.041 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 19:04:02.041 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 19:04:02.041 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 19:04:02.090 model.encoder.type = bert_pooler
2021-11-12 19:04:02.090 model.encoder.type = bert_pooler
2021-11-12 19:04:02.091 model.encoder.pretrained_model = bert-base-cased
2021-11-12 19:04:02.091 type = bert-base-cased
2021-11-12 19:04:02.091 model.encoder.override_weights_file = None
2021-11-12 19:04:02.091 model.encoder.override_weights_strip_prefix = None
2021-11-12 19:04:02.091 model.encoder.load_weights = True
2021-11-12 19:04:02.091 model.encoder.requires_grad = True
2021-11-12 19:04:02.091 model.encoder.dropout = 0.0
2021-11-12 19:04:02.092 model.encoder.transformer_kwargs = None
2021-11-12 19:04:02.283 removing temporary unarchived model dir at /tmp/tmp1qmrnbr5
2021-11-12 19:08:08.096 Plugin allennlp_models available
2021-11-12 19:08:08.101 Plugin allennlp_server available
2021-11-12 19:08:08.102 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 19:08:08.102 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpeyzo_01g
2021-11-12 19:08:10.197 dataset_reader.type = example_reader
2021-11-12 19:08:10.197 dataset_reader.max_instances = None
2021-11-12 19:08:10.197 dataset_reader.manual_distributed_sharding = False
2021-11-12 19:08:10.197 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 19:08:10.197 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 19:08:10.198 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 19:08:10.198 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 19:08:10.198 type = bert-base-cased
2021-11-12 19:08:10.198 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 19:08:10.199 dataset_reader.tokenizer.max_length = 128
2021-11-12 19:08:10.199 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 19:08:10.200 dataset_reader.text_token_indexers.type = ref
2021-11-12 19:08:10.201 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 19:08:10.201 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 19:08:10.201 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 19:08:10.201 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 19:08:10.201 type = bert-base-cased
2021-11-12 19:08:10.202 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 19:08:10.202 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 19:08:10.202 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 19:08:10.202 dataset_reader.to_index = 6
2021-11-12 19:08:10.203 dataset_reader.type = example_reader
2021-11-12 19:08:10.203 dataset_reader.max_instances = None
2021-11-12 19:08:10.203 dataset_reader.manual_distributed_sharding = False
2021-11-12 19:08:10.203 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 19:08:10.203 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 19:08:10.203 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 19:08:10.204 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 19:08:10.204 type = bert-base-cased
2021-11-12 19:08:10.204 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 19:08:10.204 dataset_reader.tokenizer.max_length = 128
2021-11-12 19:08:10.204 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 19:08:10.205 dataset_reader.text_token_indexers.type = ref
2021-11-12 19:08:10.206 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 19:08:10.206 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 19:08:10.206 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 19:08:10.206 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 19:08:10.206 type = bert-base-cased
2021-11-12 19:08:10.207 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 19:08:10.207 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 19:08:10.207 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 19:08:10.208 dataset_reader.to_index = 6
2021-11-12 19:08:10.208 type = from_instances
2021-11-12 19:08:10.208 Loading token dictionary from /tmp/tmpeyzo_01g/vocabulary.
2021-11-12 19:08:10.209 model.type = sentence_level_classifier
2021-11-12 19:08:10.209 model.embedder.type = ref
2021-11-12 19:08:10.210 model.embedder.type = basic
2021-11-12 19:08:10.210 model.embedder.token_embedders.type = ref
2021-11-12 19:08:10.211 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 19:08:10.211 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 19:08:10.211 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 19:08:10.211 type = bert-base-cased
2021-11-12 19:08:10.211 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 19:08:10.211 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 19:08:10.211 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 19:08:10.211 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 19:08:10.211 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 19:08:10.211 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 19:08:10.212 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 19:08:10.212 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 19:08:10.212 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 19:08:10.212 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 19:08:10.212 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 19:08:10.262 model.encoder.type = bert_pooler
2021-11-12 19:08:10.262 model.encoder.type = bert_pooler
2021-11-12 19:08:10.263 model.encoder.pretrained_model = bert-base-cased
2021-11-12 19:08:10.263 type = bert-base-cased
2021-11-12 19:08:10.263 model.encoder.override_weights_file = None
2021-11-12 19:08:10.263 model.encoder.override_weights_strip_prefix = None
2021-11-12 19:08:10.263 model.encoder.load_weights = True
2021-11-12 19:08:10.264 model.encoder.requires_grad = True
2021-11-12 19:08:10.264 model.encoder.dropout = 0.0
2021-11-12 19:08:10.264 model.encoder.transformer_kwargs = None
2021-11-12 19:08:10.452 removing temporary unarchived model dir at /tmp/tmpeyzo_01g
2021-11-12 19:29:04.081 Plugin allennlp_models available
2021-11-12 19:29:04.087 Plugin allennlp_server available
2021-11-12 19:29:04.087 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 19:29:04.088 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpwcht_1w7
2021-11-12 19:29:06.179 dataset_reader.type = example_reader
2021-11-12 19:29:06.180 dataset_reader.max_instances = None
2021-11-12 19:29:06.180 dataset_reader.manual_distributed_sharding = False
2021-11-12 19:29:06.180 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 19:29:06.181 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 19:29:06.181 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 19:29:06.181 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 19:29:06.182 type = bert-base-cased
2021-11-12 19:29:06.182 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 19:29:06.182 dataset_reader.tokenizer.max_length = 128
2021-11-12 19:29:06.182 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 19:29:06.184 dataset_reader.text_token_indexers.type = ref
2021-11-12 19:29:06.185 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 19:29:06.185 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 19:29:06.185 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 19:29:06.185 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 19:29:06.186 type = bert-base-cased
2021-11-12 19:29:06.186 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 19:29:06.186 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 19:29:06.186 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 19:29:06.187 dataset_reader.to_index = 6
2021-11-12 19:29:06.187 dataset_reader.type = example_reader
2021-11-12 19:29:06.188 dataset_reader.max_instances = None
2021-11-12 19:29:06.188 dataset_reader.manual_distributed_sharding = False
2021-11-12 19:29:06.188 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 19:29:06.188 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 19:29:06.189 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 19:29:06.189 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 19:29:06.189 type = bert-base-cased
2021-11-12 19:29:06.189 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 19:29:06.190 dataset_reader.tokenizer.max_length = 128
2021-11-12 19:29:06.190 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 19:29:06.191 dataset_reader.text_token_indexers.type = ref
2021-11-12 19:29:06.192 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 19:29:06.192 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 19:29:06.192 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 19:29:06.193 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 19:29:06.193 type = bert-base-cased
2021-11-12 19:29:06.193 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 19:29:06.193 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 19:29:06.193 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 19:29:06.194 dataset_reader.to_index = 6
2021-11-12 19:29:06.195 type = from_instances
2021-11-12 19:29:06.195 Loading token dictionary from /tmp/tmpwcht_1w7/vocabulary.
2021-11-12 19:29:06.195 model.type = sentence_level_classifier
2021-11-12 19:29:06.196 model.embedder.type = ref
2021-11-12 19:29:06.197 model.embedder.type = basic
2021-11-12 19:29:06.197 model.embedder.token_embedders.type = ref
2021-11-12 19:29:06.198 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 19:29:06.198 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 19:29:06.199 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 19:29:06.199 type = bert-base-cased
2021-11-12 19:29:06.199 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 19:29:06.199 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 19:29:06.200 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 19:29:06.200 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 19:29:06.200 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 19:29:06.200 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 19:29:06.200 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 19:29:06.201 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 19:29:06.201 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 19:29:06.201 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 19:29:06.202 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 19:29:06.252 model.encoder.type = bert_pooler
2021-11-12 19:29:06.252 model.encoder.type = bert_pooler
2021-11-12 19:29:06.253 model.encoder.pretrained_model = bert-base-cased
2021-11-12 19:29:06.253 type = bert-base-cased
2021-11-12 19:29:06.254 model.encoder.override_weights_file = None
2021-11-12 19:29:06.254 model.encoder.override_weights_strip_prefix = None
2021-11-12 19:29:06.254 model.encoder.load_weights = True
2021-11-12 19:29:06.254 model.encoder.requires_grad = True
2021-11-12 19:29:06.254 model.encoder.dropout = 0.0
2021-11-12 19:29:06.255 model.encoder.transformer_kwargs = None
2021-11-12 19:29:06.452 removing temporary unarchived model dir at /tmp/tmpwcht_1w7
2021-11-12 19:29:19.965 Plugin allennlp_models available
2021-11-12 19:29:19.974 Plugin allennlp_server available
2021-11-12 19:29:19.975 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 19:29:19.975 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp77i84tmc
2021-11-12 19:29:22.083 dataset_reader.type = example_reader
2021-11-12 19:29:22.084 dataset_reader.max_instances = None
2021-11-12 19:29:22.084 dataset_reader.manual_distributed_sharding = False
2021-11-12 19:29:22.084 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 19:29:22.084 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 19:29:22.084 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 19:29:22.085 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 19:29:22.085 type = bert-base-cased
2021-11-12 19:29:22.085 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 19:29:22.085 dataset_reader.tokenizer.max_length = 128
2021-11-12 19:29:22.086 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 19:29:22.087 dataset_reader.text_token_indexers.type = ref
2021-11-12 19:29:22.088 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 19:29:22.088 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 19:29:22.088 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 19:29:22.088 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 19:29:22.089 type = bert-base-cased
2021-11-12 19:29:22.089 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 19:29:22.089 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 19:29:22.089 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 19:29:22.090 dataset_reader.to_index = 6
2021-11-12 19:29:22.090 dataset_reader.type = example_reader
2021-11-12 19:29:22.090 dataset_reader.max_instances = None
2021-11-12 19:29:22.090 dataset_reader.manual_distributed_sharding = False
2021-11-12 19:29:22.090 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 19:29:22.090 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 19:29:22.091 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 19:29:22.091 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 19:29:22.091 type = bert-base-cased
2021-11-12 19:29:22.091 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 19:29:22.091 dataset_reader.tokenizer.max_length = 128
2021-11-12 19:29:22.091 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 19:29:22.092 dataset_reader.text_token_indexers.type = ref
2021-11-12 19:29:22.093 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 19:29:22.093 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 19:29:22.093 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 19:29:22.093 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 19:29:22.094 type = bert-base-cased
2021-11-12 19:29:22.094 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 19:29:22.094 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 19:29:22.094 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 19:29:22.095 dataset_reader.to_index = 6
2021-11-12 19:29:22.095 type = from_instances
2021-11-12 19:29:22.095 Loading token dictionary from /tmp/tmp77i84tmc/vocabulary.
2021-11-12 19:29:22.096 model.type = sentence_level_classifier
2021-11-12 19:29:22.096 model.embedder.type = ref
2021-11-12 19:29:22.096 model.embedder.type = basic
2021-11-12 19:29:22.097 model.embedder.token_embedders.type = ref
2021-11-12 19:29:22.098 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 19:29:22.098 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 19:29:22.098 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 19:29:22.099 type = bert-base-cased
2021-11-12 19:29:22.099 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 19:29:22.099 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 19:29:22.099 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 19:29:22.099 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 19:29:22.099 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 19:29:22.099 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 19:29:22.099 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 19:29:22.099 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 19:29:22.099 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 19:29:22.099 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 19:29:22.099 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 19:29:22.147 model.encoder.type = bert_pooler
2021-11-12 19:29:22.147 model.encoder.type = bert_pooler
2021-11-12 19:29:22.147 model.encoder.pretrained_model = bert-base-cased
2021-11-12 19:29:22.148 type = bert-base-cased
2021-11-12 19:29:22.148 model.encoder.override_weights_file = None
2021-11-12 19:29:22.148 model.encoder.override_weights_strip_prefix = None
2021-11-12 19:29:22.149 model.encoder.load_weights = True
2021-11-12 19:29:22.149 model.encoder.requires_grad = True
2021-11-12 19:29:22.149 model.encoder.dropout = 0.0
2021-11-12 19:29:22.149 model.encoder.transformer_kwargs = None
2021-11-12 19:29:22.358 removing temporary unarchived model dir at /tmp/tmp77i84tmc
2021-11-12 19:29:29.973 Plugin allennlp_models available
2021-11-12 19:29:29.981 Plugin allennlp_server available
2021-11-12 19:29:29.982 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 19:29:29.982 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp_oan9w0p
2021-11-12 19:29:32.084 dataset_reader.type = example_reader
2021-11-12 19:29:32.084 dataset_reader.max_instances = None
2021-11-12 19:29:32.084 dataset_reader.manual_distributed_sharding = False
2021-11-12 19:29:32.084 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 19:29:32.085 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 19:29:32.085 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 19:29:32.085 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 19:29:32.085 type = bert-base-cased
2021-11-12 19:29:32.085 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 19:29:32.085 dataset_reader.tokenizer.max_length = 128
2021-11-12 19:29:32.086 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 19:29:32.087 dataset_reader.text_token_indexers.type = ref
2021-11-12 19:29:32.088 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 19:29:32.088 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 19:29:32.088 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 19:29:32.088 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 19:29:32.089 type = bert-base-cased
2021-11-12 19:29:32.089 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 19:29:32.089 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 19:29:32.090 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 19:29:32.091 dataset_reader.to_index = 6
2021-11-12 19:29:32.092 dataset_reader.type = example_reader
2021-11-12 19:29:32.092 dataset_reader.max_instances = None
2021-11-12 19:29:32.093 dataset_reader.manual_distributed_sharding = False
2021-11-12 19:29:32.093 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 19:29:32.093 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 19:29:32.094 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 19:29:32.094 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 19:29:32.095 type = bert-base-cased
2021-11-12 19:29:32.095 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 19:29:32.095 dataset_reader.tokenizer.max_length = 128
2021-11-12 19:29:32.096 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 19:29:32.097 dataset_reader.text_token_indexers.type = ref
2021-11-12 19:29:32.099 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 19:29:32.100 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 19:29:32.100 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 19:29:32.100 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 19:29:32.101 type = bert-base-cased
2021-11-12 19:29:32.101 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 19:29:32.101 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 19:29:32.101 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 19:29:32.103 dataset_reader.to_index = 6
2021-11-12 19:29:32.104 type = from_instances
2021-11-12 19:29:32.104 Loading token dictionary from /tmp/tmp_oan9w0p/vocabulary.
2021-11-12 19:29:32.105 model.type = sentence_level_classifier
2021-11-12 19:29:32.105 model.embedder.type = ref
2021-11-12 19:29:32.107 model.embedder.type = basic
2021-11-12 19:29:32.107 model.embedder.token_embedders.type = ref
2021-11-12 19:29:32.109 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 19:29:32.110 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 19:29:32.110 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 19:29:32.111 type = bert-base-cased
2021-11-12 19:29:32.111 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 19:29:32.111 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 19:29:32.111 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 19:29:32.111 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 19:29:32.112 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 19:29:32.112 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 19:29:32.112 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 19:29:32.112 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 19:29:32.113 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 19:29:32.113 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 19:29:32.113 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 19:29:32.164 model.encoder.type = bert_pooler
2021-11-12 19:29:32.164 model.encoder.type = bert_pooler
2021-11-12 19:29:32.165 model.encoder.pretrained_model = bert-base-cased
2021-11-12 19:29:32.165 type = bert-base-cased
2021-11-12 19:29:32.165 model.encoder.override_weights_file = None
2021-11-12 19:29:32.165 model.encoder.override_weights_strip_prefix = None
2021-11-12 19:29:32.165 model.encoder.load_weights = True
2021-11-12 19:29:32.166 model.encoder.requires_grad = True
2021-11-12 19:29:32.166 model.encoder.dropout = 0.0
2021-11-12 19:29:32.166 model.encoder.transformer_kwargs = None
2021-11-12 19:29:32.355 removing temporary unarchived model dir at /tmp/tmp_oan9w0p
2021-11-12 19:33:17.226 Plugin allennlp_models available
2021-11-12 19:33:17.233 Plugin allennlp_server available
2021-11-12 19:33:17.234 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 19:33:17.234 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmptv8n1ysy
2021-11-12 19:33:19.333 dataset_reader.type = example_reader
2021-11-12 19:33:19.334 dataset_reader.max_instances = None
2021-11-12 19:33:19.334 dataset_reader.manual_distributed_sharding = False
2021-11-12 19:33:19.334 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 19:33:19.334 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 19:33:19.334 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 19:33:19.335 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 19:33:19.335 type = bert-base-cased
2021-11-12 19:33:19.335 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 19:33:19.335 dataset_reader.tokenizer.max_length = 128
2021-11-12 19:33:19.335 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 19:33:19.336 dataset_reader.text_token_indexers.type = ref
2021-11-12 19:33:19.337 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 19:33:19.337 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 19:33:19.338 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 19:33:19.338 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 19:33:19.338 type = bert-base-cased
2021-11-12 19:33:19.338 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 19:33:19.338 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 19:33:19.338 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 19:33:19.339 dataset_reader.to_index = 6
2021-11-12 19:33:19.339 dataset_reader.type = example_reader
2021-11-12 19:33:19.339 dataset_reader.max_instances = None
2021-11-12 19:33:19.339 dataset_reader.manual_distributed_sharding = False
2021-11-12 19:33:19.340 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 19:33:19.340 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 19:33:19.340 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 19:33:19.340 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 19:33:19.340 type = bert-base-cased
2021-11-12 19:33:19.340 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 19:33:19.340 dataset_reader.tokenizer.max_length = 128
2021-11-12 19:33:19.340 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 19:33:19.342 dataset_reader.text_token_indexers.type = ref
2021-11-12 19:33:19.344 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 19:33:19.344 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 19:33:19.345 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 19:33:19.345 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 19:33:19.346 type = bert-base-cased
2021-11-12 19:33:19.346 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 19:33:19.346 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 19:33:19.346 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 19:33:19.348 dataset_reader.to_index = 6
2021-11-12 19:33:19.349 type = from_instances
2021-11-12 19:33:19.349 Loading token dictionary from /tmp/tmptv8n1ysy/vocabulary.
2021-11-12 19:33:19.350 model.type = sentence_level_classifier
2021-11-12 19:33:19.350 model.embedder.type = ref
2021-11-12 19:33:19.352 model.embedder.type = basic
2021-11-12 19:33:19.353 model.embedder.token_embedders.type = ref
2021-11-12 19:33:19.355 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 19:33:19.355 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 19:33:19.356 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 19:33:19.356 type = bert-base-cased
2021-11-12 19:33:19.357 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 19:33:19.357 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 19:33:19.357 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 19:33:19.357 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 19:33:19.357 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 19:33:19.357 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 19:33:19.358 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 19:33:19.358 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 19:33:19.358 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 19:33:19.358 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 19:33:19.358 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 19:33:19.412 model.encoder.type = bert_pooler
2021-11-12 19:33:19.412 model.encoder.type = bert_pooler
2021-11-12 19:33:19.413 model.encoder.pretrained_model = bert-base-cased
2021-11-12 19:33:19.413 type = bert-base-cased
2021-11-12 19:33:19.413 model.encoder.override_weights_file = None
2021-11-12 19:33:19.413 model.encoder.override_weights_strip_prefix = None
2021-11-12 19:33:19.413 model.encoder.load_weights = True
2021-11-12 19:33:19.413 model.encoder.requires_grad = True
2021-11-12 19:33:19.413 model.encoder.dropout = 0.0
2021-11-12 19:33:19.414 model.encoder.transformer_kwargs = None
2021-11-12 19:33:19.601 removing temporary unarchived model dir at /tmp/tmptv8n1ysy
2021-11-12 19:33:32.361 Plugin allennlp_models available
2021-11-12 19:33:32.366 Plugin allennlp_server available
2021-11-12 19:33:32.367 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 19:33:32.367 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpf5trhy7c
2021-11-12 19:33:34.453 dataset_reader.type = example_reader
2021-11-12 19:33:34.453 dataset_reader.max_instances = None
2021-11-12 19:33:34.453 dataset_reader.manual_distributed_sharding = False
2021-11-12 19:33:34.453 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 19:33:34.453 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 19:33:34.454 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 19:33:34.454 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 19:33:34.454 type = bert-base-cased
2021-11-12 19:33:34.454 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 19:33:34.454 dataset_reader.tokenizer.max_length = 128
2021-11-12 19:33:34.454 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 19:33:34.455 dataset_reader.text_token_indexers.type = ref
2021-11-12 19:33:34.456 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 19:33:34.457 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 19:33:34.457 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 19:33:34.457 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 19:33:34.457 type = bert-base-cased
2021-11-12 19:33:34.457 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 19:33:34.457 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 19:33:34.457 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 19:33:34.458 dataset_reader.to_index = 6
2021-11-12 19:33:34.458 dataset_reader.type = example_reader
2021-11-12 19:33:34.458 dataset_reader.max_instances = None
2021-11-12 19:33:34.459 dataset_reader.manual_distributed_sharding = False
2021-11-12 19:33:34.459 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 19:33:34.459 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 19:33:34.459 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 19:33:34.459 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 19:33:34.459 type = bert-base-cased
2021-11-12 19:33:34.459 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 19:33:34.459 dataset_reader.tokenizer.max_length = 128
2021-11-12 19:33:34.459 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 19:33:34.460 dataset_reader.text_token_indexers.type = ref
2021-11-12 19:33:34.461 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 19:33:34.461 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 19:33:34.461 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 19:33:34.461 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 19:33:34.462 type = bert-base-cased
2021-11-12 19:33:34.462 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 19:33:34.462 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 19:33:34.462 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 19:33:34.462 dataset_reader.to_index = 6
2021-11-12 19:33:34.463 type = from_instances
2021-11-12 19:33:34.463 Loading token dictionary from /tmp/tmpf5trhy7c/vocabulary.
2021-11-12 19:33:34.463 model.type = sentence_level_classifier
2021-11-12 19:33:34.463 model.embedder.type = ref
2021-11-12 19:33:34.464 model.embedder.type = basic
2021-11-12 19:33:34.464 model.embedder.token_embedders.type = ref
2021-11-12 19:33:34.466 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 19:33:34.466 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 19:33:34.466 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 19:33:34.467 type = bert-base-cased
2021-11-12 19:33:34.467 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 19:33:34.467 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 19:33:34.467 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 19:33:34.467 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 19:33:34.467 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 19:33:34.467 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 19:33:34.467 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 19:33:34.467 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 19:33:34.467 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 19:33:34.467 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 19:33:34.467 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 19:33:34.507 model.encoder.type = bert_pooler
2021-11-12 19:33:34.507 model.encoder.type = bert_pooler
2021-11-12 19:33:34.507 model.encoder.pretrained_model = bert-base-cased
2021-11-12 19:33:34.507 type = bert-base-cased
2021-11-12 19:33:34.507 model.encoder.override_weights_file = None
2021-11-12 19:33:34.508 model.encoder.override_weights_strip_prefix = None
2021-11-12 19:33:34.508 model.encoder.load_weights = True
2021-11-12 19:33:34.508 model.encoder.requires_grad = True
2021-11-12 19:33:34.508 model.encoder.dropout = 0.0
2021-11-12 19:33:34.508 model.encoder.transformer_kwargs = None
2021-11-12 19:33:34.692 removing temporary unarchived model dir at /tmp/tmpf5trhy7c
2021-11-12 19:34:57.047 Plugin allennlp_models available
2021-11-12 19:34:57.057 Plugin allennlp_server available
2021-11-12 19:34:57.058 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 19:34:57.058 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp93mg1pc6
2021-11-12 19:34:59.166 dataset_reader.type = example_reader
2021-11-12 19:34:59.166 dataset_reader.max_instances = None
2021-11-12 19:34:59.166 dataset_reader.manual_distributed_sharding = False
2021-11-12 19:34:59.167 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 19:34:59.167 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 19:34:59.167 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 19:34:59.167 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 19:34:59.167 type = bert-base-cased
2021-11-12 19:34:59.168 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 19:34:59.168 dataset_reader.tokenizer.max_length = 128
2021-11-12 19:34:59.168 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 19:34:59.170 dataset_reader.text_token_indexers.type = ref
2021-11-12 19:34:59.173 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 19:34:59.173 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 19:34:59.174 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 19:34:59.174 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 19:34:59.175 type = bert-base-cased
2021-11-12 19:34:59.175 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 19:34:59.175 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 19:34:59.175 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 19:34:59.177 dataset_reader.to_index = 6
2021-11-12 19:34:59.177 dataset_reader.type = example_reader
2021-11-12 19:34:59.178 dataset_reader.max_instances = None
2021-11-12 19:34:59.178 dataset_reader.manual_distributed_sharding = False
2021-11-12 19:34:59.179 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 19:34:59.179 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 19:34:59.179 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 19:34:59.180 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 19:34:59.180 type = bert-base-cased
2021-11-12 19:34:59.180 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 19:34:59.181 dataset_reader.tokenizer.max_length = 128
2021-11-12 19:34:59.181 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 19:34:59.183 dataset_reader.text_token_indexers.type = ref
2021-11-12 19:34:59.184 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 19:34:59.185 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 19:34:59.185 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 19:34:59.185 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 19:34:59.186 type = bert-base-cased
2021-11-12 19:34:59.186 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 19:34:59.186 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 19:34:59.186 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 19:34:59.188 dataset_reader.to_index = 6
2021-11-12 19:34:59.188 type = from_instances
2021-11-12 19:34:59.188 Loading token dictionary from /tmp/tmp93mg1pc6/vocabulary.
2021-11-12 19:34:59.189 model.type = sentence_level_classifier
2021-11-12 19:34:59.190 model.embedder.type = ref
2021-11-12 19:34:59.191 model.embedder.type = basic
2021-11-12 19:34:59.192 model.embedder.token_embedders.type = ref
2021-11-12 19:34:59.193 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 19:34:59.193 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 19:34:59.194 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 19:34:59.194 type = bert-base-cased
2021-11-12 19:34:59.194 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 19:34:59.195 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 19:34:59.195 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 19:34:59.195 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 19:34:59.195 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 19:34:59.195 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 19:34:59.195 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 19:34:59.195 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 19:34:59.196 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 19:34:59.196 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 19:34:59.196 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 19:34:59.246 model.encoder.type = bert_pooler
2021-11-12 19:34:59.246 model.encoder.type = bert_pooler
2021-11-12 19:34:59.246 model.encoder.pretrained_model = bert-base-cased
2021-11-12 19:34:59.247 type = bert-base-cased
2021-11-12 19:34:59.247 model.encoder.override_weights_file = None
2021-11-12 19:34:59.247 model.encoder.override_weights_strip_prefix = None
2021-11-12 19:34:59.247 model.encoder.load_weights = True
2021-11-12 19:34:59.247 model.encoder.requires_grad = True
2021-11-12 19:34:59.247 model.encoder.dropout = 0.0
2021-11-12 19:34:59.248 model.encoder.transformer_kwargs = None
2021-11-12 19:34:59.432 removing temporary unarchived model dir at /tmp/tmp93mg1pc6
2021-11-12 20:14:35.534 Plugin allennlp_models available
2021-11-12 20:14:35.544 Plugin allennlp_server available
2021-11-12 20:14:35.545 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 20:14:35.546 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp_clmpsnk
2021-11-12 20:14:37.659 dataset_reader.type = example_reader
2021-11-12 20:14:37.659 dataset_reader.max_instances = None
2021-11-12 20:14:37.659 dataset_reader.manual_distributed_sharding = False
2021-11-12 20:14:37.659 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 20:14:37.660 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 20:14:37.660 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 20:14:37.660 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 20:14:37.660 type = bert-base-cased
2021-11-12 20:14:37.660 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 20:14:37.660 dataset_reader.tokenizer.max_length = 128
2021-11-12 20:14:37.660 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 20:14:37.661 dataset_reader.text_token_indexers.type = ref
2021-11-12 20:14:37.662 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 20:14:37.662 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 20:14:37.662 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 20:14:37.662 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 20:14:37.663 type = bert-base-cased
2021-11-12 20:14:37.663 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 20:14:37.663 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 20:14:37.663 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 20:14:37.664 dataset_reader.to_index = 6
2021-11-12 20:14:37.664 dataset_reader.type = example_reader
2021-11-12 20:14:37.664 dataset_reader.max_instances = None
2021-11-12 20:14:37.664 dataset_reader.manual_distributed_sharding = False
2021-11-12 20:14:37.664 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 20:14:37.664 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 20:14:37.664 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 20:14:37.664 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 20:14:37.664 type = bert-base-cased
2021-11-12 20:14:37.665 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 20:14:37.665 dataset_reader.tokenizer.max_length = 128
2021-11-12 20:14:37.665 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 20:14:37.665 dataset_reader.text_token_indexers.type = ref
2021-11-12 20:14:37.666 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 20:14:37.666 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 20:14:37.666 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 20:14:37.666 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 20:14:37.666 type = bert-base-cased
2021-11-12 20:14:37.666 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 20:14:37.666 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 20:14:37.666 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 20:14:37.667 dataset_reader.to_index = 6
2021-11-12 20:14:37.668 type = from_instances
2021-11-12 20:14:37.668 Loading token dictionary from /tmp/tmp_clmpsnk/vocabulary.
2021-11-12 20:14:37.668 model.type = sentence_level_classifier
2021-11-12 20:14:37.669 model.embedder.type = ref
2021-11-12 20:14:37.669 model.embedder.type = basic
2021-11-12 20:14:37.669 model.embedder.token_embedders.type = ref
2021-11-12 20:14:37.670 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 20:14:37.670 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 20:14:37.671 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 20:14:37.671 type = bert-base-cased
2021-11-12 20:14:37.671 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 20:14:37.671 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 20:14:37.671 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 20:14:37.671 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 20:14:37.671 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 20:14:37.671 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 20:14:37.671 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 20:14:37.671 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 20:14:37.671 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 20:14:37.671 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 20:14:37.671 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 20:14:37.719 model.encoder.type = bert_pooler
2021-11-12 20:14:37.719 model.encoder.type = bert_pooler
2021-11-12 20:14:37.720 model.encoder.pretrained_model = bert-base-cased
2021-11-12 20:14:37.720 type = bert-base-cased
2021-11-12 20:14:37.720 model.encoder.override_weights_file = None
2021-11-12 20:14:37.720 model.encoder.override_weights_strip_prefix = None
2021-11-12 20:14:37.720 model.encoder.load_weights = True
2021-11-12 20:14:37.720 model.encoder.requires_grad = True
2021-11-12 20:14:37.720 model.encoder.dropout = 0.0
2021-11-12 20:14:37.720 model.encoder.transformer_kwargs = None
2021-11-12 20:14:37.924 removing temporary unarchived model dir at /tmp/tmp_clmpsnk
2021-11-12 20:15:02.308 Plugin allennlp_models available
2021-11-12 20:15:02.313 Plugin allennlp_server available
2021-11-12 20:15:02.314 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 20:15:02.314 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpnhvt2vak
2021-11-12 20:15:04.419 dataset_reader.type = example_reader
2021-11-12 20:15:04.419 dataset_reader.max_instances = None
2021-11-12 20:15:04.419 dataset_reader.manual_distributed_sharding = False
2021-11-12 20:15:04.419 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 20:15:04.419 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 20:15:04.419 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 20:15:04.419 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 20:15:04.419 type = bert-base-cased
2021-11-12 20:15:04.420 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 20:15:04.420 dataset_reader.tokenizer.max_length = 128
2021-11-12 20:15:04.420 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 20:15:04.421 dataset_reader.text_token_indexers.type = ref
2021-11-12 20:15:04.422 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 20:15:04.422 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 20:15:04.422 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 20:15:04.422 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 20:15:04.422 type = bert-base-cased
2021-11-12 20:15:04.422 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 20:15:04.422 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 20:15:04.422 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 20:15:04.423 dataset_reader.to_index = 6
2021-11-12 20:15:04.424 dataset_reader.type = example_reader
2021-11-12 20:15:04.424 dataset_reader.max_instances = None
2021-11-12 20:15:04.424 dataset_reader.manual_distributed_sharding = False
2021-11-12 20:15:04.424 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 20:15:04.424 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 20:15:04.424 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 20:15:04.424 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 20:15:04.424 type = bert-base-cased
2021-11-12 20:15:04.424 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 20:15:04.424 dataset_reader.tokenizer.max_length = 128
2021-11-12 20:15:04.425 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 20:15:04.425 dataset_reader.text_token_indexers.type = ref
2021-11-12 20:15:04.426 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 20:15:04.426 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 20:15:04.426 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 20:15:04.426 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 20:15:04.426 type = bert-base-cased
2021-11-12 20:15:04.426 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 20:15:04.426 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 20:15:04.426 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 20:15:04.427 dataset_reader.to_index = 6
2021-11-12 20:15:04.427 type = from_instances
2021-11-12 20:15:04.427 Loading token dictionary from /tmp/tmpnhvt2vak/vocabulary.
2021-11-12 20:15:04.428 model.type = sentence_level_classifier
2021-11-12 20:15:04.428 model.embedder.type = ref
2021-11-12 20:15:04.429 model.embedder.type = basic
2021-11-12 20:15:04.429 model.embedder.token_embedders.type = ref
2021-11-12 20:15:04.430 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 20:15:04.430 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 20:15:04.430 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 20:15:04.430 type = bert-base-cased
2021-11-12 20:15:04.430 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 20:15:04.430 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 20:15:04.430 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 20:15:04.431 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 20:15:04.431 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 20:15:04.431 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 20:15:04.431 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 20:15:04.431 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 20:15:04.431 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 20:15:04.431 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 20:15:04.431 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 20:15:04.481 model.encoder.type = bert_pooler
2021-11-12 20:15:04.482 model.encoder.type = bert_pooler
2021-11-12 20:15:04.482 model.encoder.pretrained_model = bert-base-cased
2021-11-12 20:15:04.482 type = bert-base-cased
2021-11-12 20:15:04.482 model.encoder.override_weights_file = None
2021-11-12 20:15:04.482 model.encoder.override_weights_strip_prefix = None
2021-11-12 20:15:04.482 model.encoder.load_weights = True
2021-11-12 20:15:04.483 model.encoder.requires_grad = True
2021-11-12 20:15:04.483 model.encoder.dropout = 0.0
2021-11-12 20:15:04.483 model.encoder.transformer_kwargs = None
2021-11-12 20:15:04.685 removing temporary unarchived model dir at /tmp/tmpnhvt2vak
2021-11-12 20:15:14.621 Plugin allennlp_models available
2021-11-12 20:15:14.630 Plugin allennlp_server available
2021-11-12 20:15:14.631 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 20:15:14.631 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpz9vqtr08
2021-11-12 20:15:16.735 dataset_reader.type = example_reader
2021-11-12 20:15:16.736 dataset_reader.max_instances = None
2021-11-12 20:15:16.736 dataset_reader.manual_distributed_sharding = False
2021-11-12 20:15:16.736 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 20:15:16.736 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 20:15:16.736 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 20:15:16.736 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 20:15:16.736 type = bert-base-cased
2021-11-12 20:15:16.736 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 20:15:16.736 dataset_reader.tokenizer.max_length = 128
2021-11-12 20:15:16.736 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 20:15:16.738 dataset_reader.text_token_indexers.type = ref
2021-11-12 20:15:16.739 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 20:15:16.739 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 20:15:16.739 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 20:15:16.739 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 20:15:16.739 type = bert-base-cased
2021-11-12 20:15:16.739 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 20:15:16.739 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 20:15:16.739 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 20:15:16.740 dataset_reader.to_index = 6
2021-11-12 20:15:16.740 dataset_reader.type = example_reader
2021-11-12 20:15:16.740 dataset_reader.max_instances = None
2021-11-12 20:15:16.741 dataset_reader.manual_distributed_sharding = False
2021-11-12 20:15:16.741 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 20:15:16.741 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 20:15:16.741 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 20:15:16.741 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 20:15:16.741 type = bert-base-cased
2021-11-12 20:15:16.741 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 20:15:16.742 dataset_reader.tokenizer.max_length = 128
2021-11-12 20:15:16.742 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 20:15:16.742 dataset_reader.text_token_indexers.type = ref
2021-11-12 20:15:16.744 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 20:15:16.744 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 20:15:16.744 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 20:15:16.744 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 20:15:16.744 type = bert-base-cased
2021-11-12 20:15:16.744 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 20:15:16.745 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 20:15:16.745 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 20:15:16.745 dataset_reader.to_index = 6
2021-11-12 20:15:16.746 type = from_instances
2021-11-12 20:15:16.746 Loading token dictionary from /tmp/tmpz9vqtr08/vocabulary.
2021-11-12 20:15:16.746 model.type = sentence_level_classifier
2021-11-12 20:15:16.746 model.embedder.type = ref
2021-11-12 20:15:16.747 model.embedder.type = basic
2021-11-12 20:15:16.747 model.embedder.token_embedders.type = ref
2021-11-12 20:15:16.748 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 20:15:16.748 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 20:15:16.748 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 20:15:16.749 type = bert-base-cased
2021-11-12 20:15:16.749 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 20:15:16.749 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 20:15:16.749 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 20:15:16.749 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 20:15:16.749 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 20:15:16.749 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 20:15:16.749 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 20:15:16.749 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 20:15:16.749 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 20:15:16.749 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 20:15:16.749 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 20:15:16.800 model.encoder.type = bert_pooler
2021-11-12 20:15:16.800 model.encoder.type = bert_pooler
2021-11-12 20:15:16.800 model.encoder.pretrained_model = bert-base-cased
2021-11-12 20:15:16.800 type = bert-base-cased
2021-11-12 20:15:16.801 model.encoder.override_weights_file = None
2021-11-12 20:15:16.801 model.encoder.override_weights_strip_prefix = None
2021-11-12 20:15:16.801 model.encoder.load_weights = True
2021-11-12 20:15:16.801 model.encoder.requires_grad = True
2021-11-12 20:15:16.801 model.encoder.dropout = 0.0
2021-11-12 20:15:16.801 model.encoder.transformer_kwargs = None
2021-11-12 20:15:17.005 removing temporary unarchived model dir at /tmp/tmpz9vqtr08
2021-11-12 20:15:33.565 Plugin allennlp_models available
2021-11-12 20:15:33.570 Plugin allennlp_server available
2021-11-12 20:15:33.570 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 20:15:33.570 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpf3ye1_4j
2021-11-12 20:15:35.672 dataset_reader.type = example_reader
2021-11-12 20:15:35.672 dataset_reader.max_instances = None
2021-11-12 20:15:35.672 dataset_reader.manual_distributed_sharding = False
2021-11-12 20:15:35.672 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 20:15:35.672 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 20:15:35.672 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 20:15:35.672 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 20:15:35.672 type = bert-base-cased
2021-11-12 20:15:35.672 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 20:15:35.672 dataset_reader.tokenizer.max_length = 128
2021-11-12 20:15:35.673 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 20:15:35.674 dataset_reader.text_token_indexers.type = ref
2021-11-12 20:15:35.675 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 20:15:35.675 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 20:15:35.675 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 20:15:35.675 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 20:15:35.675 type = bert-base-cased
2021-11-12 20:15:35.675 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 20:15:35.676 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 20:15:35.676 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 20:15:35.676 dataset_reader.to_index = 6
2021-11-12 20:15:35.677 dataset_reader.type = example_reader
2021-11-12 20:15:35.677 dataset_reader.max_instances = None
2021-11-12 20:15:35.677 dataset_reader.manual_distributed_sharding = False
2021-11-12 20:15:35.677 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 20:15:35.677 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 20:15:35.677 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 20:15:35.677 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 20:15:35.677 type = bert-base-cased
2021-11-12 20:15:35.677 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 20:15:35.677 dataset_reader.tokenizer.max_length = 128
2021-11-12 20:15:35.677 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 20:15:35.678 dataset_reader.text_token_indexers.type = ref
2021-11-12 20:15:35.679 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 20:15:35.679 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 20:15:35.679 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 20:15:35.679 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 20:15:35.679 type = bert-base-cased
2021-11-12 20:15:35.679 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 20:15:35.679 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 20:15:35.679 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 20:15:35.680 dataset_reader.to_index = 6
2021-11-12 20:15:35.680 type = from_instances
2021-11-12 20:15:35.680 Loading token dictionary from /tmp/tmpf3ye1_4j/vocabulary.
2021-11-12 20:15:35.681 model.type = sentence_level_classifier
2021-11-12 20:15:35.681 model.embedder.type = ref
2021-11-12 20:15:35.682 model.embedder.type = basic
2021-11-12 20:15:35.692 model.embedder.token_embedders.type = ref
2021-11-12 20:15:35.693 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 20:15:35.693 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 20:15:35.694 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 20:15:35.694 type = bert-base-cased
2021-11-12 20:15:35.694 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 20:15:35.694 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 20:15:35.694 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 20:15:35.694 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 20:15:35.694 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 20:15:35.694 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 20:15:35.694 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 20:15:35.694 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 20:15:35.694 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 20:15:35.694 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 20:15:35.694 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 20:15:35.744 model.encoder.type = bert_pooler
2021-11-12 20:15:35.744 model.encoder.type = bert_pooler
2021-11-12 20:15:35.745 model.encoder.pretrained_model = bert-base-cased
2021-11-12 20:15:35.745 type = bert-base-cased
2021-11-12 20:15:35.745 model.encoder.override_weights_file = None
2021-11-12 20:15:35.745 model.encoder.override_weights_strip_prefix = None
2021-11-12 20:15:35.745 model.encoder.load_weights = True
2021-11-12 20:15:35.745 model.encoder.requires_grad = True
2021-11-12 20:15:35.745 model.encoder.dropout = 0.0
2021-11-12 20:15:35.746 model.encoder.transformer_kwargs = None
2021-11-12 20:15:35.947 removing temporary unarchived model dir at /tmp/tmpf3ye1_4j
2021-11-12 20:17:39.382 Plugin allennlp_models available
2021-11-12 20:17:39.390 Plugin allennlp_server available
2021-11-12 20:17:39.390 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 20:17:39.391 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp415pu_1p
2021-11-12 20:17:41.512 dataset_reader.type = example_reader
2021-11-12 20:17:41.513 dataset_reader.max_instances = None
2021-11-12 20:17:41.513 dataset_reader.manual_distributed_sharding = False
2021-11-12 20:17:41.513 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 20:17:41.513 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 20:17:41.513 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 20:17:41.513 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 20:17:41.513 type = bert-base-cased
2021-11-12 20:17:41.513 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 20:17:41.514 dataset_reader.tokenizer.max_length = 128
2021-11-12 20:17:41.514 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 20:17:41.515 dataset_reader.text_token_indexers.type = ref
2021-11-12 20:17:41.516 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 20:17:41.516 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 20:17:41.516 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 20:17:41.516 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 20:17:41.516 type = bert-base-cased
2021-11-12 20:17:41.516 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 20:17:41.516 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 20:17:41.516 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 20:17:41.517 dataset_reader.to_index = 6
2021-11-12 20:17:41.517 dataset_reader.type = example_reader
2021-11-12 20:17:41.517 dataset_reader.max_instances = None
2021-11-12 20:17:41.517 dataset_reader.manual_distributed_sharding = False
2021-11-12 20:17:41.517 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 20:17:41.517 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 20:17:41.518 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 20:17:41.518 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 20:17:41.518 type = bert-base-cased
2021-11-12 20:17:41.518 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 20:17:41.518 dataset_reader.tokenizer.max_length = 128
2021-11-12 20:17:41.518 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 20:17:41.519 dataset_reader.text_token_indexers.type = ref
2021-11-12 20:17:41.519 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 20:17:41.520 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 20:17:41.520 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 20:17:41.520 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 20:17:41.520 type = bert-base-cased
2021-11-12 20:17:41.520 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 20:17:41.520 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 20:17:41.520 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 20:17:41.521 dataset_reader.to_index = 6
2021-11-12 20:17:41.521 type = from_instances
2021-11-12 20:17:41.521 Loading token dictionary from /tmp/tmp415pu_1p/vocabulary.
2021-11-12 20:17:41.522 model.type = sentence_level_classifier
2021-11-12 20:17:41.522 model.embedder.type = ref
2021-11-12 20:17:41.523 model.embedder.type = basic
2021-11-12 20:17:41.523 model.embedder.token_embedders.type = ref
2021-11-12 20:17:41.524 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 20:17:41.524 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 20:17:41.524 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 20:17:41.524 type = bert-base-cased
2021-11-12 20:17:41.524 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 20:17:41.524 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 20:17:41.524 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 20:17:41.524 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 20:17:41.524 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 20:17:41.524 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 20:17:41.524 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 20:17:41.524 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 20:17:41.524 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 20:17:41.525 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 20:17:41.525 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 20:17:41.574 model.encoder.type = bert_pooler
2021-11-12 20:17:41.574 model.encoder.type = bert_pooler
2021-11-12 20:17:41.574 model.encoder.pretrained_model = bert-base-cased
2021-11-12 20:17:41.575 type = bert-base-cased
2021-11-12 20:17:41.575 model.encoder.override_weights_file = None
2021-11-12 20:17:41.575 model.encoder.override_weights_strip_prefix = None
2021-11-12 20:17:41.575 model.encoder.load_weights = True
2021-11-12 20:17:41.575 model.encoder.requires_grad = True
2021-11-12 20:17:41.575 model.encoder.dropout = 0.0
2021-11-12 20:17:41.575 model.encoder.transformer_kwargs = None
2021-11-12 20:17:41.778 removing temporary unarchived model dir at /tmp/tmp415pu_1p
2021-11-12 20:17:46.442 Plugin allennlp_models available
2021-11-12 20:17:46.451 Plugin allennlp_server available
2021-11-12 20:17:46.452 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 20:17:46.452 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmprx_qk1m_
2021-11-12 20:17:48.562 dataset_reader.type = example_reader
2021-11-12 20:17:48.562 dataset_reader.max_instances = None
2021-11-12 20:17:48.562 dataset_reader.manual_distributed_sharding = False
2021-11-12 20:17:48.562 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 20:17:48.562 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 20:17:48.562 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 20:17:48.562 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 20:17:48.562 type = bert-base-cased
2021-11-12 20:17:48.562 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 20:17:48.563 dataset_reader.tokenizer.max_length = 128
2021-11-12 20:17:48.563 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 20:17:48.564 dataset_reader.text_token_indexers.type = ref
2021-11-12 20:17:48.565 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 20:17:48.565 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 20:17:48.565 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 20:17:48.565 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 20:17:48.565 type = bert-base-cased
2021-11-12 20:17:48.565 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 20:17:48.565 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 20:17:48.565 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 20:17:48.566 dataset_reader.to_index = 6
2021-11-12 20:17:48.566 dataset_reader.type = example_reader
2021-11-12 20:17:48.566 dataset_reader.max_instances = None
2021-11-12 20:17:48.567 dataset_reader.manual_distributed_sharding = False
2021-11-12 20:17:48.567 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 20:17:48.567 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 20:17:48.567 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 20:17:48.567 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 20:17:48.567 type = bert-base-cased
2021-11-12 20:17:48.567 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 20:17:48.567 dataset_reader.tokenizer.max_length = 128
2021-11-12 20:17:48.567 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 20:17:48.568 dataset_reader.text_token_indexers.type = ref
2021-11-12 20:17:48.569 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 20:17:48.569 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 20:17:48.569 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 20:17:48.569 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 20:17:48.569 type = bert-base-cased
2021-11-12 20:17:48.569 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 20:17:48.569 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 20:17:48.569 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 20:17:48.570 dataset_reader.to_index = 6
2021-11-12 20:17:48.570 type = from_instances
2021-11-12 20:17:48.570 Loading token dictionary from /tmp/tmprx_qk1m_/vocabulary.
2021-11-12 20:17:48.571 model.type = sentence_level_classifier
2021-11-12 20:17:48.571 model.embedder.type = ref
2021-11-12 20:17:48.572 model.embedder.type = basic
2021-11-12 20:17:48.572 model.embedder.token_embedders.type = ref
2021-11-12 20:17:48.573 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 20:17:48.573 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 20:17:48.574 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 20:17:48.574 type = bert-base-cased
2021-11-12 20:17:48.574 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 20:17:48.574 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 20:17:48.574 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 20:17:48.574 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 20:17:48.574 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 20:17:48.574 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 20:17:48.574 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 20:17:48.574 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 20:17:48.574 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 20:17:48.574 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 20:17:48.574 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 20:17:48.622 model.encoder.type = bert_pooler
2021-11-12 20:17:48.622 model.encoder.type = bert_pooler
2021-11-12 20:17:48.622 model.encoder.pretrained_model = bert-base-cased
2021-11-12 20:17:48.622 type = bert-base-cased
2021-11-12 20:17:48.622 model.encoder.override_weights_file = None
2021-11-12 20:17:48.622 model.encoder.override_weights_strip_prefix = None
2021-11-12 20:17:48.622 model.encoder.load_weights = True
2021-11-12 20:17:48.622 model.encoder.requires_grad = True
2021-11-12 20:17:48.622 model.encoder.dropout = 0.0
2021-11-12 20:17:48.622 model.encoder.transformer_kwargs = None
2021-11-12 20:17:48.817 removing temporary unarchived model dir at /tmp/tmprx_qk1m_
2021-11-12 20:19:21.635 Plugin allennlp_models available
2021-11-12 20:19:21.643 Plugin allennlp_server available
2021-11-12 20:19:21.643 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 20:19:21.643 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpfa6d16o2
2021-11-12 20:19:23.749 dataset_reader.type = example_reader
2021-11-12 20:19:23.749 dataset_reader.max_instances = None
2021-11-12 20:19:23.749 dataset_reader.manual_distributed_sharding = False
2021-11-12 20:19:23.749 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 20:19:23.749 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 20:19:23.749 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 20:19:23.749 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 20:19:23.749 type = bert-base-cased
2021-11-12 20:19:23.749 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 20:19:23.749 dataset_reader.tokenizer.max_length = 128
2021-11-12 20:19:23.750 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 20:19:23.751 dataset_reader.text_token_indexers.type = ref
2021-11-12 20:19:23.752 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 20:19:23.752 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 20:19:23.752 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 20:19:23.752 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 20:19:23.752 type = bert-base-cased
2021-11-12 20:19:23.752 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 20:19:23.752 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 20:19:23.752 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 20:19:23.753 dataset_reader.to_index = 6
2021-11-12 20:19:23.753 dataset_reader.type = example_reader
2021-11-12 20:19:23.754 dataset_reader.max_instances = None
2021-11-12 20:19:23.754 dataset_reader.manual_distributed_sharding = False
2021-11-12 20:19:23.754 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 20:19:23.754 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 20:19:23.754 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 20:19:23.754 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 20:19:23.754 type = bert-base-cased
2021-11-12 20:19:23.754 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 20:19:23.754 dataset_reader.tokenizer.max_length = 128
2021-11-12 20:19:23.754 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 20:19:23.755 dataset_reader.text_token_indexers.type = ref
2021-11-12 20:19:23.756 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 20:19:23.756 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 20:19:23.756 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 20:19:23.756 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 20:19:23.756 type = bert-base-cased
2021-11-12 20:19:23.756 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 20:19:23.756 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 20:19:23.756 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 20:19:23.758 dataset_reader.to_index = 6
2021-11-12 20:19:23.758 type = from_instances
2021-11-12 20:19:23.758 Loading token dictionary from /tmp/tmpfa6d16o2/vocabulary.
2021-11-12 20:19:23.759 model.type = sentence_level_classifier
2021-11-12 20:19:23.760 model.embedder.type = ref
2021-11-12 20:19:23.761 model.embedder.type = basic
2021-11-12 20:19:23.762 model.embedder.token_embedders.type = ref
2021-11-12 20:19:23.764 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 20:19:23.764 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 20:19:23.765 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 20:19:23.765 type = bert-base-cased
2021-11-12 20:19:23.765 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 20:19:23.766 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 20:19:23.766 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 20:19:23.766 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 20:19:23.766 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 20:19:23.766 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 20:19:23.766 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 20:19:23.767 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 20:19:23.767 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 20:19:23.767 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 20:19:23.767 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 20:19:23.816 model.encoder.type = bert_pooler
2021-11-12 20:19:23.817 model.encoder.type = bert_pooler
2021-11-12 20:19:23.817 model.encoder.pretrained_model = bert-base-cased
2021-11-12 20:19:23.817 type = bert-base-cased
2021-11-12 20:19:23.817 model.encoder.override_weights_file = None
2021-11-12 20:19:23.817 model.encoder.override_weights_strip_prefix = None
2021-11-12 20:19:23.817 model.encoder.load_weights = True
2021-11-12 20:19:23.817 model.encoder.requires_grad = True
2021-11-12 20:19:23.817 model.encoder.dropout = 0.0
2021-11-12 20:19:23.817 model.encoder.transformer_kwargs = None
2021-11-12 20:19:24.018 removing temporary unarchived model dir at /tmp/tmpfa6d16o2
2021-11-12 21:46:05.531 Plugin allennlp_models available
2021-11-12 21:46:05.539 Plugin allennlp_server available
2021-11-12 21:46:05.540 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 21:46:05.540 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmppxqatgur
2021-11-12 21:46:07.649 dataset_reader.type = example_reader
2021-11-12 21:46:07.650 dataset_reader.max_instances = None
2021-11-12 21:46:07.650 dataset_reader.manual_distributed_sharding = False
2021-11-12 21:46:07.650 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 21:46:07.650 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 21:46:07.650 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 21:46:07.650 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 21:46:07.650 type = bert-base-cased
2021-11-12 21:46:07.650 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 21:46:07.650 dataset_reader.tokenizer.max_length = 128
2021-11-12 21:46:07.650 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 21:46:07.652 dataset_reader.text_token_indexers.type = ref
2021-11-12 21:46:07.653 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 21:46:07.653 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 21:46:07.653 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 21:46:07.653 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 21:46:07.653 type = bert-base-cased
2021-11-12 21:46:07.653 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 21:46:07.653 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 21:46:07.653 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 21:46:07.654 dataset_reader.to_index = 6
2021-11-12 21:46:07.654 dataset_reader.type = example_reader
2021-11-12 21:46:07.654 dataset_reader.max_instances = None
2021-11-12 21:46:07.654 dataset_reader.manual_distributed_sharding = False
2021-11-12 21:46:07.655 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 21:46:07.655 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 21:46:07.655 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 21:46:07.655 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 21:46:07.655 type = bert-base-cased
2021-11-12 21:46:07.655 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 21:46:07.655 dataset_reader.tokenizer.max_length = 128
2021-11-12 21:46:07.655 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 21:46:07.656 dataset_reader.text_token_indexers.type = ref
2021-11-12 21:46:07.656 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 21:46:07.657 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 21:46:07.657 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 21:46:07.657 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 21:46:07.657 type = bert-base-cased
2021-11-12 21:46:07.657 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 21:46:07.657 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 21:46:07.657 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 21:46:07.658 dataset_reader.to_index = 6
2021-11-12 21:46:07.658 type = from_instances
2021-11-12 21:46:07.658 Loading token dictionary from /tmp/tmppxqatgur/vocabulary.
2021-11-12 21:46:07.659 model.type = sentence_level_classifier
2021-11-12 21:46:07.659 model.embedder.type = ref
2021-11-12 21:46:07.660 model.embedder.type = basic
2021-11-12 21:46:07.660 model.embedder.token_embedders.type = ref
2021-11-12 21:46:07.661 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 21:46:07.661 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 21:46:07.661 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 21:46:07.662 type = bert-base-cased
2021-11-12 21:46:07.662 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 21:46:07.662 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 21:46:07.662 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 21:46:07.662 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 21:46:07.662 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 21:46:07.662 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 21:46:07.662 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 21:46:07.662 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 21:46:07.662 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 21:46:07.662 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 21:46:07.662 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 21:46:07.711 model.encoder.type = bert_pooler
2021-11-12 21:46:07.711 model.encoder.type = bert_pooler
2021-11-12 21:46:07.712 model.encoder.pretrained_model = bert-base-cased
2021-11-12 21:46:07.712 type = bert-base-cased
2021-11-12 21:46:07.712 model.encoder.override_weights_file = None
2021-11-12 21:46:07.712 model.encoder.override_weights_strip_prefix = None
2021-11-12 21:46:07.712 model.encoder.load_weights = True
2021-11-12 21:46:07.712 model.encoder.requires_grad = True
2021-11-12 21:46:07.712 model.encoder.dropout = 0.0
2021-11-12 21:46:07.712 model.encoder.transformer_kwargs = None
2021-11-12 21:46:07.904 removing temporary unarchived model dir at /tmp/tmppxqatgur
2021-11-12 21:58:34.229 Plugin allennlp_models available
2021-11-12 21:58:34.238 Plugin allennlp_server available
2021-11-12 21:58:34.239 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 21:58:34.239 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpzwjvcb_1
2021-11-12 21:58:36.344 dataset_reader.type = example_reader
2021-11-12 21:58:36.345 dataset_reader.max_instances = None
2021-11-12 21:58:36.345 dataset_reader.manual_distributed_sharding = False
2021-11-12 21:58:36.345 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 21:58:36.345 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 21:58:36.345 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 21:58:36.345 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 21:58:36.345 type = bert-base-cased
2021-11-12 21:58:36.345 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 21:58:36.345 dataset_reader.tokenizer.max_length = 128
2021-11-12 21:58:36.345 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 21:58:36.347 dataset_reader.text_token_indexers.type = ref
2021-11-12 21:58:36.348 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 21:58:36.348 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 21:58:36.348 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 21:58:36.348 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 21:58:36.348 type = bert-base-cased
2021-11-12 21:58:36.348 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 21:58:36.348 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 21:58:36.348 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 21:58:36.349 dataset_reader.to_index = 6
2021-11-12 21:58:36.349 dataset_reader.type = example_reader
2021-11-12 21:58:36.350 dataset_reader.max_instances = None
2021-11-12 21:58:36.350 dataset_reader.manual_distributed_sharding = False
2021-11-12 21:58:36.350 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 21:58:36.350 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 21:58:36.350 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 21:58:36.350 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 21:58:36.350 type = bert-base-cased
2021-11-12 21:58:36.350 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 21:58:36.350 dataset_reader.tokenizer.max_length = 128
2021-11-12 21:58:36.350 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 21:58:36.351 dataset_reader.text_token_indexers.type = ref
2021-11-12 21:58:36.352 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 21:58:36.352 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 21:58:36.352 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 21:58:36.352 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 21:58:36.352 type = bert-base-cased
2021-11-12 21:58:36.352 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 21:58:36.352 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 21:58:36.352 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 21:58:36.353 dataset_reader.to_index = 6
2021-11-12 21:58:36.353 type = from_instances
2021-11-12 21:58:36.353 Loading token dictionary from /tmp/tmpzwjvcb_1/vocabulary.
2021-11-12 21:58:36.354 model.type = sentence_level_classifier
2021-11-12 21:58:36.354 model.embedder.type = ref
2021-11-12 21:58:36.355 model.embedder.type = basic
2021-11-12 21:58:36.355 model.embedder.token_embedders.type = ref
2021-11-12 21:58:36.356 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 21:58:36.356 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 21:58:36.356 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 21:58:36.356 type = bert-base-cased
2021-11-12 21:58:36.356 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 21:58:36.356 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 21:58:36.356 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 21:58:36.356 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 21:58:36.356 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 21:58:36.356 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 21:58:36.356 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 21:58:36.356 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 21:58:36.356 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 21:58:36.356 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 21:58:36.356 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 21:58:36.409 model.encoder.type = bert_pooler
2021-11-12 21:58:36.409 model.encoder.type = bert_pooler
2021-11-12 21:58:36.409 model.encoder.pretrained_model = bert-base-cased
2021-11-12 21:58:36.410 type = bert-base-cased
2021-11-12 21:58:36.410 model.encoder.override_weights_file = None
2021-11-12 21:58:36.410 model.encoder.override_weights_strip_prefix = None
2021-11-12 21:58:36.410 model.encoder.load_weights = True
2021-11-12 21:58:36.410 model.encoder.requires_grad = True
2021-11-12 21:58:36.410 model.encoder.dropout = 0.0
2021-11-12 21:58:36.410 model.encoder.transformer_kwargs = None
2021-11-12 21:58:36.609 removing temporary unarchived model dir at /tmp/tmpzwjvcb_1
2021-11-12 22:37:50.816 Plugin allennlp_models available
2021-11-12 22:37:50.828 Plugin allennlp_server available
2021-11-12 22:37:50.829 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 22:37:50.830 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpv89xy6sc
2021-11-12 22:37:52.960 dataset_reader.type = example_reader
2021-11-12 22:37:52.960 dataset_reader.max_instances = None
2021-11-12 22:37:52.960 dataset_reader.manual_distributed_sharding = False
2021-11-12 22:37:52.960 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 22:37:52.960 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 22:37:52.960 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 22:37:52.960 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 22:37:52.960 type = bert-base-cased
2021-11-12 22:37:52.960 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 22:37:52.960 dataset_reader.tokenizer.max_length = 128
2021-11-12 22:37:52.960 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 22:37:52.962 dataset_reader.text_token_indexers.type = ref
2021-11-12 22:37:52.963 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 22:37:52.963 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 22:37:52.963 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 22:37:52.963 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 22:37:52.963 type = bert-base-cased
2021-11-12 22:37:52.963 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 22:37:52.963 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 22:37:52.963 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 22:37:52.964 dataset_reader.to_index = 6
2021-11-12 22:37:52.965 dataset_reader.type = example_reader
2021-11-12 22:37:52.965 dataset_reader.max_instances = None
2021-11-12 22:37:52.965 dataset_reader.manual_distributed_sharding = False
2021-11-12 22:37:52.965 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 22:37:52.965 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 22:37:52.965 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 22:37:52.965 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 22:37:52.965 type = bert-base-cased
2021-11-12 22:37:52.965 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 22:37:52.965 dataset_reader.tokenizer.max_length = 128
2021-11-12 22:37:52.965 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 22:37:52.966 dataset_reader.text_token_indexers.type = ref
2021-11-12 22:37:52.967 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 22:37:52.967 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 22:37:52.967 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 22:37:52.967 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 22:37:52.967 type = bert-base-cased
2021-11-12 22:37:52.967 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 22:37:52.967 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 22:37:52.967 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 22:37:52.968 dataset_reader.to_index = 6
2021-11-12 22:37:52.968 type = from_instances
2021-11-12 22:37:52.968 Loading token dictionary from /tmp/tmpv89xy6sc/vocabulary.
2021-11-12 22:37:52.969 model.type = sentence_level_classifier
2021-11-12 22:37:52.969 model.embedder.type = ref
2021-11-12 22:37:52.970 model.embedder.type = basic
2021-11-12 22:37:52.970 model.embedder.token_embedders.type = ref
2021-11-12 22:37:52.971 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 22:37:52.971 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 22:37:52.971 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 22:37:52.971 type = bert-base-cased
2021-11-12 22:37:52.971 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 22:37:52.971 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 22:37:52.971 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 22:37:52.971 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 22:37:52.971 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 22:37:52.971 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 22:37:52.971 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 22:37:52.971 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 22:37:52.971 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 22:37:52.972 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 22:37:52.972 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 22:37:53.021 model.encoder.type = bert_pooler
2021-11-12 22:37:53.021 model.encoder.type = bert_pooler
2021-11-12 22:37:53.022 model.encoder.pretrained_model = bert-base-cased
2021-11-12 22:37:53.022 type = bert-base-cased
2021-11-12 22:37:53.022 model.encoder.override_weights_file = None
2021-11-12 22:37:53.022 model.encoder.override_weights_strip_prefix = None
2021-11-12 22:37:53.022 model.encoder.load_weights = True
2021-11-12 22:37:53.022 model.encoder.requires_grad = True
2021-11-12 22:37:53.022 model.encoder.dropout = 0.0
2021-11-12 22:37:53.022 model.encoder.transformer_kwargs = None
2021-11-12 22:37:53.223 removing temporary unarchived model dir at /tmp/tmpv89xy6sc
2021-11-12 23:08:57.805 Plugin allennlp_models available
2021-11-12 23:08:57.814 Plugin allennlp_server available
2021-11-12 23:08:57.815 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 23:08:57.815 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpkx_n9n_s
2021-11-12 23:08:59.921 dataset_reader.type = example_reader
2021-11-12 23:08:59.921 dataset_reader.max_instances = None
2021-11-12 23:08:59.921 dataset_reader.manual_distributed_sharding = False
2021-11-12 23:08:59.921 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 23:08:59.922 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 23:08:59.922 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 23:08:59.922 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 23:08:59.922 type = bert-base-cased
2021-11-12 23:08:59.922 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 23:08:59.922 dataset_reader.tokenizer.max_length = 128
2021-11-12 23:08:59.922 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 23:08:59.923 dataset_reader.text_token_indexers.type = ref
2021-11-12 23:08:59.925 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 23:08:59.925 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 23:08:59.925 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 23:08:59.925 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 23:08:59.925 type = bert-base-cased
2021-11-12 23:08:59.925 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 23:08:59.925 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 23:08:59.925 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 23:08:59.926 dataset_reader.to_index = 6
2021-11-12 23:08:59.926 dataset_reader.type = example_reader
2021-11-12 23:08:59.927 dataset_reader.max_instances = None
2021-11-12 23:08:59.927 dataset_reader.manual_distributed_sharding = False
2021-11-12 23:08:59.927 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 23:08:59.927 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 23:08:59.927 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 23:08:59.927 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 23:08:59.927 type = bert-base-cased
2021-11-12 23:08:59.927 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 23:08:59.927 dataset_reader.tokenizer.max_length = 128
2021-11-12 23:08:59.927 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 23:08:59.928 dataset_reader.text_token_indexers.type = ref
2021-11-12 23:08:59.929 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 23:08:59.929 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 23:08:59.929 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 23:08:59.929 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 23:08:59.929 type = bert-base-cased
2021-11-12 23:08:59.929 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 23:08:59.929 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 23:08:59.929 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 23:08:59.930 dataset_reader.to_index = 6
2021-11-12 23:08:59.930 type = from_instances
2021-11-12 23:08:59.930 Loading token dictionary from /tmp/tmpkx_n9n_s/vocabulary.
2021-11-12 23:08:59.931 model.type = sentence_level_classifier
2021-11-12 23:08:59.931 model.embedder.type = ref
2021-11-12 23:08:59.932 model.embedder.type = basic
2021-11-12 23:08:59.932 model.embedder.token_embedders.type = ref
2021-11-12 23:08:59.933 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 23:08:59.933 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 23:08:59.933 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 23:08:59.934 type = bert-base-cased
2021-11-12 23:08:59.934 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 23:08:59.934 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 23:08:59.934 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 23:08:59.934 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 23:08:59.934 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 23:08:59.934 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 23:08:59.934 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 23:08:59.934 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 23:08:59.934 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 23:08:59.934 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 23:08:59.934 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 23:08:59.983 model.encoder.type = bert_pooler
2021-11-12 23:08:59.983 model.encoder.type = bert_pooler
2021-11-12 23:08:59.984 model.encoder.pretrained_model = bert-base-cased
2021-11-12 23:08:59.984 type = bert-base-cased
2021-11-12 23:08:59.984 model.encoder.override_weights_file = None
2021-11-12 23:08:59.984 model.encoder.override_weights_strip_prefix = None
2021-11-12 23:08:59.985 model.encoder.load_weights = True
2021-11-12 23:08:59.985 model.encoder.requires_grad = True
2021-11-12 23:08:59.985 model.encoder.dropout = 0.0
2021-11-12 23:08:59.985 model.encoder.transformer_kwargs = None
2021-11-12 23:09:00.190 removing temporary unarchived model dir at /tmp/tmpkx_n9n_s
2021-11-12 23:14:19.959 Plugin allennlp_models available
2021-11-12 23:14:19.970 Plugin allennlp_server available
2021-11-12 23:14:19.971 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 23:14:19.972 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmppwiecf4i
2021-11-12 23:14:22.105 dataset_reader.type = example_reader
2021-11-12 23:14:22.105 dataset_reader.max_instances = None
2021-11-12 23:14:22.105 dataset_reader.manual_distributed_sharding = False
2021-11-12 23:14:22.105 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 23:14:22.105 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 23:14:22.105 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 23:14:22.105 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 23:14:22.106 type = bert-base-cased
2021-11-12 23:14:22.106 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 23:14:22.106 dataset_reader.tokenizer.max_length = 128
2021-11-12 23:14:22.106 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 23:14:22.107 dataset_reader.text_token_indexers.type = ref
2021-11-12 23:14:22.108 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 23:14:22.108 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 23:14:22.108 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 23:14:22.108 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 23:14:22.108 type = bert-base-cased
2021-11-12 23:14:22.108 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 23:14:22.108 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 23:14:22.109 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 23:14:22.110 dataset_reader.to_index = 6
2021-11-12 23:14:22.110 dataset_reader.type = example_reader
2021-11-12 23:14:22.110 dataset_reader.max_instances = None
2021-11-12 23:14:22.110 dataset_reader.manual_distributed_sharding = False
2021-11-12 23:14:22.110 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 23:14:22.110 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 23:14:22.110 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 23:14:22.110 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 23:14:22.110 type = bert-base-cased
2021-11-12 23:14:22.110 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 23:14:22.110 dataset_reader.tokenizer.max_length = 128
2021-11-12 23:14:22.110 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 23:14:22.111 dataset_reader.text_token_indexers.type = ref
2021-11-12 23:14:22.112 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 23:14:22.112 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 23:14:22.112 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 23:14:22.112 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 23:14:22.112 type = bert-base-cased
2021-11-12 23:14:22.112 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 23:14:22.112 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 23:14:22.112 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 23:14:22.113 dataset_reader.to_index = 6
2021-11-12 23:14:22.113 type = from_instances
2021-11-12 23:14:22.113 Loading token dictionary from /tmp/tmppwiecf4i/vocabulary.
2021-11-12 23:14:22.114 model.type = sentence_level_classifier
2021-11-12 23:14:22.114 model.embedder.type = ref
2021-11-12 23:14:22.115 model.embedder.type = basic
2021-11-12 23:14:22.115 model.embedder.token_embedders.type = ref
2021-11-12 23:14:22.116 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 23:14:22.116 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 23:14:22.116 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 23:14:22.116 type = bert-base-cased
2021-11-12 23:14:22.116 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 23:14:22.116 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 23:14:22.116 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 23:14:22.117 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 23:14:22.117 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 23:14:22.117 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 23:14:22.117 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 23:14:22.117 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 23:14:22.117 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 23:14:22.117 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 23:14:22.117 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 23:14:22.166 model.encoder.type = bert_pooler
2021-11-12 23:14:22.167 model.encoder.type = bert_pooler
2021-11-12 23:14:22.167 model.encoder.pretrained_model = bert-base-cased
2021-11-12 23:14:22.167 type = bert-base-cased
2021-11-12 23:14:22.167 model.encoder.override_weights_file = None
2021-11-12 23:14:22.167 model.encoder.override_weights_strip_prefix = None
2021-11-12 23:14:22.167 model.encoder.load_weights = True
2021-11-12 23:14:22.167 model.encoder.requires_grad = True
2021-11-12 23:14:22.167 model.encoder.dropout = 0.0
2021-11-12 23:14:22.167 model.encoder.transformer_kwargs = None
2021-11-12 23:14:22.368 removing temporary unarchived model dir at /tmp/tmppwiecf4i
2021-11-12 23:14:29.288 Plugin allennlp_models available
2021-11-12 23:14:29.298 Plugin allennlp_server available
2021-11-12 23:14:29.299 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 23:14:29.299 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmplx6r4kjv
2021-11-12 23:14:31.436 dataset_reader.type = example_reader
2021-11-12 23:14:31.436 dataset_reader.max_instances = None
2021-11-12 23:14:31.436 dataset_reader.manual_distributed_sharding = False
2021-11-12 23:14:31.436 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 23:14:31.436 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 23:14:31.436 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 23:14:31.436 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 23:14:31.437 type = bert-base-cased
2021-11-12 23:14:31.437 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 23:14:31.437 dataset_reader.tokenizer.max_length = 128
2021-11-12 23:14:31.437 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 23:14:31.438 dataset_reader.text_token_indexers.type = ref
2021-11-12 23:14:31.439 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 23:14:31.439 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 23:14:31.439 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 23:14:31.439 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 23:14:31.439 type = bert-base-cased
2021-11-12 23:14:31.439 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 23:14:31.439 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 23:14:31.439 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 23:14:31.440 dataset_reader.to_index = 6
2021-11-12 23:14:31.440 dataset_reader.type = example_reader
2021-11-12 23:14:31.440 dataset_reader.max_instances = None
2021-11-12 23:14:31.441 dataset_reader.manual_distributed_sharding = False
2021-11-12 23:14:31.441 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 23:14:31.441 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 23:14:31.441 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 23:14:31.441 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 23:14:31.441 type = bert-base-cased
2021-11-12 23:14:31.441 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 23:14:31.441 dataset_reader.tokenizer.max_length = 128
2021-11-12 23:14:31.441 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 23:14:31.442 dataset_reader.text_token_indexers.type = ref
2021-11-12 23:14:31.443 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 23:14:31.443 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 23:14:31.444 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 23:14:31.444 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 23:14:31.444 type = bert-base-cased
2021-11-12 23:14:31.444 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 23:14:31.444 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 23:14:31.444 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 23:14:31.445 dataset_reader.to_index = 6
2021-11-12 23:14:31.445 type = from_instances
2021-11-12 23:14:31.445 Loading token dictionary from /tmp/tmplx6r4kjv/vocabulary.
2021-11-12 23:14:31.445 model.type = sentence_level_classifier
2021-11-12 23:14:31.446 model.embedder.type = ref
2021-11-12 23:14:31.446 model.embedder.type = basic
2021-11-12 23:14:31.446 model.embedder.token_embedders.type = ref
2021-11-12 23:14:31.447 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 23:14:31.447 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 23:14:31.448 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 23:14:31.448 type = bert-base-cased
2021-11-12 23:14:31.448 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 23:14:31.448 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 23:14:31.448 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 23:14:31.448 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 23:14:31.448 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 23:14:31.448 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 23:14:31.448 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 23:14:31.448 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 23:14:31.448 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 23:14:31.448 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 23:14:31.448 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 23:14:31.499 model.encoder.type = bert_pooler
2021-11-12 23:14:31.499 model.encoder.type = bert_pooler
2021-11-12 23:14:31.499 model.encoder.pretrained_model = bert-base-cased
2021-11-12 23:14:31.500 type = bert-base-cased
2021-11-12 23:14:31.500 model.encoder.override_weights_file = None
2021-11-12 23:14:31.500 model.encoder.override_weights_strip_prefix = None
2021-11-12 23:14:31.500 model.encoder.load_weights = True
2021-11-12 23:14:31.500 model.encoder.requires_grad = True
2021-11-12 23:14:31.500 model.encoder.dropout = 0.0
2021-11-12 23:14:31.500 model.encoder.transformer_kwargs = None
2021-11-12 23:14:31.699 removing temporary unarchived model dir at /tmp/tmplx6r4kjv
2021-11-12 23:15:02.658 Plugin allennlp_models available
2021-11-12 23:15:02.665 Plugin allennlp_server available
2021-11-12 23:15:02.665 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 23:15:02.666 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpr26or92b
2021-11-12 23:15:04.770 dataset_reader.type = example_reader
2021-11-12 23:15:04.770 dataset_reader.max_instances = None
2021-11-12 23:15:04.770 dataset_reader.manual_distributed_sharding = False
2021-11-12 23:15:04.770 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 23:15:04.770 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 23:15:04.771 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 23:15:04.771 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 23:15:04.771 type = bert-base-cased
2021-11-12 23:15:04.771 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 23:15:04.771 dataset_reader.tokenizer.max_length = 128
2021-11-12 23:15:04.771 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 23:15:04.772 dataset_reader.text_token_indexers.type = ref
2021-11-12 23:15:04.773 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 23:15:04.774 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 23:15:04.774 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 23:15:04.774 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 23:15:04.774 type = bert-base-cased
2021-11-12 23:15:04.774 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 23:15:04.774 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 23:15:04.774 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 23:15:04.775 dataset_reader.to_index = 6
2021-11-12 23:15:04.775 dataset_reader.type = example_reader
2021-11-12 23:15:04.775 dataset_reader.max_instances = None
2021-11-12 23:15:04.775 dataset_reader.manual_distributed_sharding = False
2021-11-12 23:15:04.776 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 23:15:04.776 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 23:15:04.776 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 23:15:04.776 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 23:15:04.776 type = bert-base-cased
2021-11-12 23:15:04.776 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 23:15:04.776 dataset_reader.tokenizer.max_length = 128
2021-11-12 23:15:04.777 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 23:15:04.777 dataset_reader.text_token_indexers.type = ref
2021-11-12 23:15:04.778 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 23:15:04.778 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 23:15:04.778 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 23:15:04.778 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 23:15:04.779 type = bert-base-cased
2021-11-12 23:15:04.779 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 23:15:04.779 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 23:15:04.779 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 23:15:04.779 dataset_reader.to_index = 6
2021-11-12 23:15:04.780 type = from_instances
2021-11-12 23:15:04.780 Loading token dictionary from /tmp/tmpr26or92b/vocabulary.
2021-11-12 23:15:04.780 model.type = sentence_level_classifier
2021-11-12 23:15:04.780 model.embedder.type = ref
2021-11-12 23:15:04.781 model.embedder.type = basic
2021-11-12 23:15:04.781 model.embedder.token_embedders.type = ref
2021-11-12 23:15:04.783 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 23:15:04.783 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 23:15:04.783 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 23:15:04.784 type = bert-base-cased
2021-11-12 23:15:04.784 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 23:15:04.784 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 23:15:04.784 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 23:15:04.784 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 23:15:04.784 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 23:15:04.784 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 23:15:04.784 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 23:15:04.785 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 23:15:04.785 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 23:15:04.785 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 23:15:04.785 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 23:15:04.834 model.encoder.type = bert_pooler
2021-11-12 23:15:04.834 model.encoder.type = bert_pooler
2021-11-12 23:15:04.835 model.encoder.pretrained_model = bert-base-cased
2021-11-12 23:15:04.835 type = bert-base-cased
2021-11-12 23:15:04.835 model.encoder.override_weights_file = None
2021-11-12 23:15:04.835 model.encoder.override_weights_strip_prefix = None
2021-11-12 23:15:04.835 model.encoder.load_weights = True
2021-11-12 23:15:04.835 model.encoder.requires_grad = True
2021-11-12 23:15:04.835 model.encoder.dropout = 0.0
2021-11-12 23:15:04.836 model.encoder.transformer_kwargs = None
2021-11-12 23:15:05.033 removing temporary unarchived model dir at /tmp/tmpr26or92b
2021-11-12 23:15:10.894 Plugin allennlp_models available
2021-11-12 23:15:10.904 Plugin allennlp_server available
2021-11-12 23:15:10.905 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 23:15:10.905 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpccbobvus
2021-11-12 23:15:13.041 dataset_reader.type = example_reader
2021-11-12 23:15:13.041 dataset_reader.max_instances = None
2021-11-12 23:15:13.041 dataset_reader.manual_distributed_sharding = False
2021-11-12 23:15:13.041 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 23:15:13.042 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 23:15:13.042 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 23:15:13.042 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 23:15:13.042 type = bert-base-cased
2021-11-12 23:15:13.042 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 23:15:13.042 dataset_reader.tokenizer.max_length = 128
2021-11-12 23:15:13.042 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 23:15:13.043 dataset_reader.text_token_indexers.type = ref
2021-11-12 23:15:13.044 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 23:15:13.044 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 23:15:13.045 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 23:15:13.045 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 23:15:13.045 type = bert-base-cased
2021-11-12 23:15:13.045 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 23:15:13.045 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 23:15:13.045 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 23:15:13.046 dataset_reader.to_index = 6
2021-11-12 23:15:13.046 dataset_reader.type = example_reader
2021-11-12 23:15:13.046 dataset_reader.max_instances = None
2021-11-12 23:15:13.046 dataset_reader.manual_distributed_sharding = False
2021-11-12 23:15:13.047 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 23:15:13.047 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 23:15:13.047 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 23:15:13.047 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 23:15:13.047 type = bert-base-cased
2021-11-12 23:15:13.047 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 23:15:13.048 dataset_reader.tokenizer.max_length = 128
2021-11-12 23:15:13.048 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 23:15:13.048 dataset_reader.text_token_indexers.type = ref
2021-11-12 23:15:13.049 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 23:15:13.049 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 23:15:13.049 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 23:15:13.050 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 23:15:13.050 type = bert-base-cased
2021-11-12 23:15:13.050 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 23:15:13.050 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 23:15:13.050 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 23:15:13.051 dataset_reader.to_index = 6
2021-11-12 23:15:13.051 type = from_instances
2021-11-12 23:15:13.051 Loading token dictionary from /tmp/tmpccbobvus/vocabulary.
2021-11-12 23:15:13.051 model.type = sentence_level_classifier
2021-11-12 23:15:13.052 model.embedder.type = ref
2021-11-12 23:15:13.052 model.embedder.type = basic
2021-11-12 23:15:13.052 model.embedder.token_embedders.type = ref
2021-11-12 23:15:13.053 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 23:15:13.054 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 23:15:13.054 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 23:15:13.054 type = bert-base-cased
2021-11-12 23:15:13.054 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 23:15:13.054 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 23:15:13.055 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 23:15:13.055 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 23:15:13.055 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 23:15:13.055 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 23:15:13.055 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 23:15:13.055 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 23:15:13.055 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 23:15:13.055 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 23:15:13.055 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 23:15:13.104 model.encoder.type = bert_pooler
2021-11-12 23:15:13.104 model.encoder.type = bert_pooler
2021-11-12 23:15:13.105 model.encoder.pretrained_model = bert-base-cased
2021-11-12 23:15:13.105 type = bert-base-cased
2021-11-12 23:15:13.105 model.encoder.override_weights_file = None
2021-11-12 23:15:13.105 model.encoder.override_weights_strip_prefix = None
2021-11-12 23:15:13.105 model.encoder.load_weights = True
2021-11-12 23:15:13.105 model.encoder.requires_grad = True
2021-11-12 23:15:13.105 model.encoder.dropout = 0.0
2021-11-12 23:15:13.106 model.encoder.transformer_kwargs = None
2021-11-12 23:15:13.308 removing temporary unarchived model dir at /tmp/tmpccbobvus
2021-11-12 23:15:22.607 Plugin allennlp_models available
2021-11-12 23:15:22.612 Plugin allennlp_server available
2021-11-12 23:15:22.612 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 23:15:22.613 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpctucw1n6
2021-11-12 23:15:24.717 dataset_reader.type = example_reader
2021-11-12 23:15:24.717 dataset_reader.max_instances = None
2021-11-12 23:15:24.717 dataset_reader.manual_distributed_sharding = False
2021-11-12 23:15:24.718 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 23:15:24.718 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 23:15:24.718 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 23:15:24.718 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 23:15:24.718 type = bert-base-cased
2021-11-12 23:15:24.718 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 23:15:24.718 dataset_reader.tokenizer.max_length = 128
2021-11-12 23:15:24.718 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 23:15:24.719 dataset_reader.text_token_indexers.type = ref
2021-11-12 23:15:24.720 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 23:15:24.720 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 23:15:24.721 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 23:15:24.721 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 23:15:24.721 type = bert-base-cased
2021-11-12 23:15:24.721 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 23:15:24.721 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 23:15:24.721 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 23:15:24.722 dataset_reader.to_index = 6
2021-11-12 23:15:24.722 dataset_reader.type = example_reader
2021-11-12 23:15:24.722 dataset_reader.max_instances = None
2021-11-12 23:15:24.722 dataset_reader.manual_distributed_sharding = False
2021-11-12 23:15:24.722 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 23:15:24.723 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 23:15:24.723 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 23:15:24.723 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 23:15:24.723 type = bert-base-cased
2021-11-12 23:15:24.723 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 23:15:24.723 dataset_reader.tokenizer.max_length = 128
2021-11-12 23:15:24.723 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 23:15:24.724 dataset_reader.text_token_indexers.type = ref
2021-11-12 23:15:24.724 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 23:15:24.725 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 23:15:24.725 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 23:15:24.725 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 23:15:24.725 type = bert-base-cased
2021-11-12 23:15:24.725 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 23:15:24.725 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 23:15:24.725 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 23:15:24.726 dataset_reader.to_index = 6
2021-11-12 23:15:24.726 type = from_instances
2021-11-12 23:15:24.726 Loading token dictionary from /tmp/tmpctucw1n6/vocabulary.
2021-11-12 23:15:24.727 model.type = sentence_level_classifier
2021-11-12 23:15:24.727 model.embedder.type = ref
2021-11-12 23:15:24.728 model.embedder.type = basic
2021-11-12 23:15:24.728 model.embedder.token_embedders.type = ref
2021-11-12 23:15:24.729 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 23:15:24.729 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 23:15:24.729 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 23:15:24.730 type = bert-base-cased
2021-11-12 23:15:24.730 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 23:15:24.730 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 23:15:24.730 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 23:15:24.730 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 23:15:24.730 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 23:15:24.730 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 23:15:24.730 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 23:15:24.730 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 23:15:24.730 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 23:15:24.730 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 23:15:24.730 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 23:15:24.777 model.encoder.type = bert_pooler
2021-11-12 23:15:24.778 model.encoder.type = bert_pooler
2021-11-12 23:15:24.778 model.encoder.pretrained_model = bert-base-cased
2021-11-12 23:15:24.778 type = bert-base-cased
2021-11-12 23:15:24.778 model.encoder.override_weights_file = None
2021-11-12 23:15:24.778 model.encoder.override_weights_strip_prefix = None
2021-11-12 23:15:24.778 model.encoder.load_weights = True
2021-11-12 23:15:24.779 model.encoder.requires_grad = True
2021-11-12 23:15:24.779 model.encoder.dropout = 0.0
2021-11-12 23:15:24.779 model.encoder.transformer_kwargs = None
2021-11-12 23:15:24.980 removing temporary unarchived model dir at /tmp/tmpctucw1n6
2021-11-12 23:15:35.833 Plugin allennlp_models available
2021-11-12 23:15:35.841 Plugin allennlp_server available
2021-11-12 23:15:35.842 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 23:15:35.842 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp9wkrest6
2021-11-12 23:15:37.956 dataset_reader.type = example_reader
2021-11-12 23:15:37.956 dataset_reader.max_instances = None
2021-11-12 23:15:37.956 dataset_reader.manual_distributed_sharding = False
2021-11-12 23:15:37.956 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 23:15:37.956 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 23:15:37.956 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 23:15:37.956 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 23:15:37.957 type = bert-base-cased
2021-11-12 23:15:37.957 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 23:15:37.957 dataset_reader.tokenizer.max_length = 128
2021-11-12 23:15:37.957 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 23:15:37.958 dataset_reader.text_token_indexers.type = ref
2021-11-12 23:15:37.959 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 23:15:37.959 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 23:15:37.959 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 23:15:37.959 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 23:15:37.959 type = bert-base-cased
2021-11-12 23:15:37.959 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 23:15:37.959 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 23:15:37.959 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 23:15:37.960 dataset_reader.to_index = 6
2021-11-12 23:15:37.961 dataset_reader.type = example_reader
2021-11-12 23:15:37.961 dataset_reader.max_instances = None
2021-11-12 23:15:37.961 dataset_reader.manual_distributed_sharding = False
2021-11-12 23:15:37.961 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 23:15:37.961 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 23:15:37.961 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 23:15:37.961 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 23:15:37.961 type = bert-base-cased
2021-11-12 23:15:37.961 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 23:15:37.961 dataset_reader.tokenizer.max_length = 128
2021-11-12 23:15:37.961 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 23:15:37.962 dataset_reader.text_token_indexers.type = ref
2021-11-12 23:15:37.963 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 23:15:37.963 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 23:15:37.963 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 23:15:37.963 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 23:15:37.963 type = bert-base-cased
2021-11-12 23:15:37.963 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 23:15:37.963 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 23:15:37.963 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 23:15:37.964 dataset_reader.to_index = 6
2021-11-12 23:15:37.964 type = from_instances
2021-11-12 23:15:37.965 Loading token dictionary from /tmp/tmp9wkrest6/vocabulary.
2021-11-12 23:15:37.965 model.type = sentence_level_classifier
2021-11-12 23:15:37.965 model.embedder.type = ref
2021-11-12 23:15:37.966 model.embedder.type = basic
2021-11-12 23:15:37.967 model.embedder.token_embedders.type = ref
2021-11-12 23:15:37.967 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 23:15:37.968 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 23:15:37.968 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 23:15:37.968 type = bert-base-cased
2021-11-12 23:15:37.968 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 23:15:37.968 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 23:15:37.968 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 23:15:37.968 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 23:15:37.968 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 23:15:37.968 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 23:15:37.968 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 23:15:37.968 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 23:15:37.968 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 23:15:37.968 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 23:15:37.968 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 23:15:38.016 model.encoder.type = bert_pooler
2021-11-12 23:15:38.016 model.encoder.type = bert_pooler
2021-11-12 23:15:38.017 model.encoder.pretrained_model = bert-base-cased
2021-11-12 23:15:38.017 type = bert-base-cased
2021-11-12 23:15:38.017 model.encoder.override_weights_file = None
2021-11-12 23:15:38.017 model.encoder.override_weights_strip_prefix = None
2021-11-12 23:15:38.017 model.encoder.load_weights = True
2021-11-12 23:15:38.017 model.encoder.requires_grad = True
2021-11-12 23:15:38.017 model.encoder.dropout = 0.0
2021-11-12 23:15:38.017 model.encoder.transformer_kwargs = None
2021-11-12 23:15:38.220 removing temporary unarchived model dir at /tmp/tmp9wkrest6
2021-11-12 23:15:56.372 Plugin allennlp_models available
2021-11-12 23:15:56.378 Plugin allennlp_server available
2021-11-12 23:15:56.378 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 23:15:56.378 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp7y_id23v
2021-11-12 23:15:58.478 dataset_reader.type = example_reader
2021-11-12 23:15:58.478 dataset_reader.max_instances = None
2021-11-12 23:15:58.478 dataset_reader.manual_distributed_sharding = False
2021-11-12 23:15:58.478 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 23:15:58.478 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 23:15:58.479 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 23:15:58.479 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 23:15:58.479 type = bert-base-cased
2021-11-12 23:15:58.479 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 23:15:58.479 dataset_reader.tokenizer.max_length = 128
2021-11-12 23:15:58.479 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 23:15:58.480 dataset_reader.text_token_indexers.type = ref
2021-11-12 23:15:58.481 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 23:15:58.481 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 23:15:58.481 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 23:15:58.481 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 23:15:58.482 type = bert-base-cased
2021-11-12 23:15:58.482 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 23:15:58.482 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 23:15:58.482 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 23:15:58.483 dataset_reader.to_index = 6
2021-11-12 23:15:58.483 dataset_reader.type = example_reader
2021-11-12 23:15:58.483 dataset_reader.max_instances = None
2021-11-12 23:15:58.483 dataset_reader.manual_distributed_sharding = False
2021-11-12 23:15:58.483 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 23:15:58.483 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 23:15:58.483 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 23:15:58.483 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 23:15:58.484 type = bert-base-cased
2021-11-12 23:15:58.484 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 23:15:58.484 dataset_reader.tokenizer.max_length = 128
2021-11-12 23:15:58.484 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 23:15:58.485 dataset_reader.text_token_indexers.type = ref
2021-11-12 23:15:58.485 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 23:15:58.485 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 23:15:58.485 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 23:15:58.485 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 23:15:58.486 type = bert-base-cased
2021-11-12 23:15:58.486 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 23:15:58.486 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 23:15:58.486 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 23:15:58.486 dataset_reader.to_index = 6
2021-11-12 23:15:58.487 type = from_instances
2021-11-12 23:15:58.487 Loading token dictionary from /tmp/tmp7y_id23v/vocabulary.
2021-11-12 23:15:58.487 model.type = sentence_level_classifier
2021-11-12 23:15:58.487 model.embedder.type = ref
2021-11-12 23:15:58.488 model.embedder.type = basic
2021-11-12 23:15:58.488 model.embedder.token_embedders.type = ref
2021-11-12 23:15:58.489 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 23:15:58.489 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 23:15:58.490 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 23:15:58.490 type = bert-base-cased
2021-11-12 23:15:58.490 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 23:15:58.490 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 23:15:58.490 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 23:15:58.490 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 23:15:58.490 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 23:15:58.490 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 23:15:58.490 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 23:15:58.490 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 23:15:58.490 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 23:15:58.490 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 23:15:58.490 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 23:15:58.540 model.encoder.type = bert_pooler
2021-11-12 23:15:58.540 model.encoder.type = bert_pooler
2021-11-12 23:15:58.541 model.encoder.pretrained_model = bert-base-cased
2021-11-12 23:15:58.541 type = bert-base-cased
2021-11-12 23:15:58.541 model.encoder.override_weights_file = None
2021-11-12 23:15:58.541 model.encoder.override_weights_strip_prefix = None
2021-11-12 23:15:58.541 model.encoder.load_weights = True
2021-11-12 23:15:58.541 model.encoder.requires_grad = True
2021-11-12 23:15:58.541 model.encoder.dropout = 0.0
2021-11-12 23:15:58.541 model.encoder.transformer_kwargs = None
2021-11-12 23:15:58.744 removing temporary unarchived model dir at /tmp/tmp7y_id23v
2021-11-12 23:16:20.534 Plugin allennlp_models available
2021-11-12 23:16:20.541 Plugin allennlp_server available
2021-11-12 23:16:20.542 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-12 23:16:20.542 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp45msy4fd
2021-11-12 23:16:22.640 dataset_reader.type = example_reader
2021-11-12 23:16:22.641 dataset_reader.max_instances = None
2021-11-12 23:16:22.641 dataset_reader.manual_distributed_sharding = False
2021-11-12 23:16:22.645 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 23:16:22.646 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 23:16:22.646 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 23:16:22.646 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 23:16:22.646 type = bert-base-cased
2021-11-12 23:16:22.646 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 23:16:22.646 dataset_reader.tokenizer.max_length = 128
2021-11-12 23:16:22.646 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 23:16:22.647 dataset_reader.text_token_indexers.type = ref
2021-11-12 23:16:22.648 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 23:16:22.648 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 23:16:22.648 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 23:16:22.648 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 23:16:22.649 type = bert-base-cased
2021-11-12 23:16:22.649 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 23:16:22.649 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 23:16:22.649 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 23:16:22.650 dataset_reader.to_index = 6
2021-11-12 23:16:22.650 dataset_reader.type = example_reader
2021-11-12 23:16:22.650 dataset_reader.max_instances = None
2021-11-12 23:16:22.650 dataset_reader.manual_distributed_sharding = False
2021-11-12 23:16:22.650 dataset_reader.manual_multiprocess_sharding = False
2021-11-12 23:16:22.650 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 23:16:22.650 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-12 23:16:22.650 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-12 23:16:22.650 type = bert-base-cased
2021-11-12 23:16:22.650 dataset_reader.tokenizer.add_special_tokens = True
2021-11-12 23:16:22.650 dataset_reader.tokenizer.max_length = 128
2021-11-12 23:16:22.650 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-12 23:16:22.651 dataset_reader.text_token_indexers.type = ref
2021-11-12 23:16:22.652 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 23:16:22.652 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-12 23:16:22.652 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-12 23:16:22.652 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-12 23:16:22.652 type = bert-base-cased
2021-11-12 23:16:22.652 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-12 23:16:22.652 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-12 23:16:22.652 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-12 23:16:22.653 dataset_reader.to_index = 6
2021-11-12 23:16:22.654 type = from_instances
2021-11-12 23:16:22.654 Loading token dictionary from /tmp/tmp45msy4fd/vocabulary.
2021-11-12 23:16:22.654 model.type = sentence_level_classifier
2021-11-12 23:16:22.655 model.embedder.type = ref
2021-11-12 23:16:22.655 model.embedder.type = basic
2021-11-12 23:16:22.655 model.embedder.token_embedders.type = ref
2021-11-12 23:16:22.657 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 23:16:22.657 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-12 23:16:22.658 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-12 23:16:22.658 type = bert-base-cased
2021-11-12 23:16:22.658 model.embedder.token_embedders.tokens.max_length = None
2021-11-12 23:16:22.658 model.embedder.token_embedders.tokens.sub_module = None
2021-11-12 23:16:22.658 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-12 23:16:22.658 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-12 23:16:22.658 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-12 23:16:22.658 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-12 23:16:22.658 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-12 23:16:22.658 model.embedder.token_embedders.tokens.load_weights = True
2021-11-12 23:16:22.658 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-12 23:16:22.658 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-12 23:16:22.658 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-12 23:16:22.706 model.encoder.type = bert_pooler
2021-11-12 23:16:22.707 model.encoder.type = bert_pooler
2021-11-12 23:16:22.707 model.encoder.pretrained_model = bert-base-cased
2021-11-12 23:16:22.707 type = bert-base-cased
2021-11-12 23:16:22.707 model.encoder.override_weights_file = None
2021-11-12 23:16:22.707 model.encoder.override_weights_strip_prefix = None
2021-11-12 23:16:22.707 model.encoder.load_weights = True
2021-11-12 23:16:22.707 model.encoder.requires_grad = True
2021-11-12 23:16:22.707 model.encoder.dropout = 0.0
2021-11-12 23:16:22.707 model.encoder.transformer_kwargs = None
2021-11-12 23:16:22.909 removing temporary unarchived model dir at /tmp/tmp45msy4fd
2021-11-13 00:02:02.694 Plugin allennlp_models available
2021-11-13 00:02:02.704 Plugin allennlp_server available
2021-11-13 00:02:02.705 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-13 00:02:02.705 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpx3u9nc1s
2021-11-13 00:02:04.850 dataset_reader.type = example_reader
2021-11-13 00:02:04.850 dataset_reader.max_instances = None
2021-11-13 00:02:04.850 dataset_reader.manual_distributed_sharding = False
2021-11-13 00:02:04.850 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 00:02:04.851 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 00:02:04.851 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 00:02:04.851 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 00:02:04.851 type = bert-base-cased
2021-11-13 00:02:04.851 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 00:02:04.851 dataset_reader.tokenizer.max_length = 128
2021-11-13 00:02:04.851 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 00:02:04.852 dataset_reader.text_token_indexers.type = ref
2021-11-13 00:02:04.853 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 00:02:04.853 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 00:02:04.854 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 00:02:04.854 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 00:02:04.854 type = bert-base-cased
2021-11-13 00:02:04.854 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 00:02:04.854 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 00:02:04.854 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 00:02:04.855 dataset_reader.to_index = 6
2021-11-13 00:02:04.855 dataset_reader.type = example_reader
2021-11-13 00:02:04.855 dataset_reader.max_instances = None
2021-11-13 00:02:04.855 dataset_reader.manual_distributed_sharding = False
2021-11-13 00:02:04.855 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 00:02:04.855 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 00:02:04.855 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 00:02:04.855 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 00:02:04.855 type = bert-base-cased
2021-11-13 00:02:04.855 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 00:02:04.855 dataset_reader.tokenizer.max_length = 128
2021-11-13 00:02:04.856 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 00:02:04.856 dataset_reader.text_token_indexers.type = ref
2021-11-13 00:02:04.857 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 00:02:04.857 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 00:02:04.857 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 00:02:04.857 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 00:02:04.858 type = bert-base-cased
2021-11-13 00:02:04.858 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 00:02:04.858 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 00:02:04.858 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 00:02:04.858 dataset_reader.to_index = 6
2021-11-13 00:02:04.859 type = from_instances
2021-11-13 00:02:04.859 Loading token dictionary from /tmp/tmpx3u9nc1s/vocabulary.
2021-11-13 00:02:04.859 model.type = sentence_level_classifier
2021-11-13 00:02:04.859 model.embedder.type = ref
2021-11-13 00:02:04.860 model.embedder.type = basic
2021-11-13 00:02:04.860 model.embedder.token_embedders.type = ref
2021-11-13 00:02:04.861 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 00:02:04.861 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 00:02:04.861 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-13 00:02:04.862 type = bert-base-cased
2021-11-13 00:02:04.862 model.embedder.token_embedders.tokens.max_length = None
2021-11-13 00:02:04.862 model.embedder.token_embedders.tokens.sub_module = None
2021-11-13 00:02:04.862 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-13 00:02:04.862 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-13 00:02:04.862 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-13 00:02:04.862 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-13 00:02:04.862 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-13 00:02:04.862 model.embedder.token_embedders.tokens.load_weights = True
2021-11-13 00:02:04.862 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-13 00:02:04.862 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-13 00:02:04.862 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-13 00:02:04.903 model.encoder.type = bert_pooler
2021-11-13 00:02:04.904 model.encoder.type = bert_pooler
2021-11-13 00:02:04.904 model.encoder.pretrained_model = bert-base-cased
2021-11-13 00:02:04.904 type = bert-base-cased
2021-11-13 00:02:04.904 model.encoder.override_weights_file = None
2021-11-13 00:02:04.904 model.encoder.override_weights_strip_prefix = None
2021-11-13 00:02:04.904 model.encoder.load_weights = True
2021-11-13 00:02:04.904 model.encoder.requires_grad = True
2021-11-13 00:02:04.905 model.encoder.dropout = 0.0
2021-11-13 00:02:04.905 model.encoder.transformer_kwargs = None
2021-11-13 00:02:05.107 removing temporary unarchived model dir at /tmp/tmpx3u9nc1s
2021-11-13 00:15:20.312 Plugin allennlp_models available
2021-11-13 00:15:20.322 Plugin allennlp_server available
2021-11-13 00:15:20.323 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-13 00:15:20.324 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpc58oln2p
2021-11-13 00:15:22.435 dataset_reader.type = example_reader
2021-11-13 00:15:22.436 dataset_reader.max_instances = None
2021-11-13 00:15:22.436 dataset_reader.manual_distributed_sharding = False
2021-11-13 00:15:22.436 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 00:15:22.436 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 00:15:22.436 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 00:15:22.436 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 00:15:22.436 type = bert-base-cased
2021-11-13 00:15:22.436 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 00:15:22.436 dataset_reader.tokenizer.max_length = 128
2021-11-13 00:15:22.436 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 00:15:22.437 dataset_reader.text_token_indexers.type = ref
2021-11-13 00:15:22.438 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 00:15:22.439 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 00:15:22.439 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 00:15:22.439 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 00:15:22.439 type = bert-base-cased
2021-11-13 00:15:22.439 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 00:15:22.439 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 00:15:22.439 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 00:15:22.440 dataset_reader.to_index = 6
2021-11-13 00:15:22.440 dataset_reader.type = example_reader
2021-11-13 00:15:22.440 dataset_reader.max_instances = None
2021-11-13 00:15:22.440 dataset_reader.manual_distributed_sharding = False
2021-11-13 00:15:22.440 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 00:15:22.440 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 00:15:22.440 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 00:15:22.441 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 00:15:22.441 type = bert-base-cased
2021-11-13 00:15:22.441 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 00:15:22.441 dataset_reader.tokenizer.max_length = 128
2021-11-13 00:15:22.441 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 00:15:22.442 dataset_reader.text_token_indexers.type = ref
2021-11-13 00:15:22.443 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 00:15:22.443 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 00:15:22.443 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 00:15:22.443 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 00:15:22.443 type = bert-base-cased
2021-11-13 00:15:22.443 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 00:15:22.443 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 00:15:22.443 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 00:15:22.444 dataset_reader.to_index = 6
2021-11-13 00:15:22.444 type = from_instances
2021-11-13 00:15:22.444 Loading token dictionary from /tmp/tmpc58oln2p/vocabulary.
2021-11-13 00:15:22.445 model.type = sentence_level_classifier
2021-11-13 00:15:22.445 model.embedder.type = ref
2021-11-13 00:15:22.446 model.embedder.type = basic
2021-11-13 00:15:22.446 model.embedder.token_embedders.type = ref
2021-11-13 00:15:22.447 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 00:15:22.447 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 00:15:22.447 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-13 00:15:22.447 type = bert-base-cased
2021-11-13 00:15:22.447 model.embedder.token_embedders.tokens.max_length = None
2021-11-13 00:15:22.447 model.embedder.token_embedders.tokens.sub_module = None
2021-11-13 00:15:22.447 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-13 00:15:22.447 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-13 00:15:22.448 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-13 00:15:22.448 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-13 00:15:22.448 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-13 00:15:22.448 model.embedder.token_embedders.tokens.load_weights = True
2021-11-13 00:15:22.448 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-13 00:15:22.448 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-13 00:15:22.448 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-13 00:15:22.489 model.encoder.type = bert_pooler
2021-11-13 00:15:22.489 model.encoder.type = bert_pooler
2021-11-13 00:15:22.490 model.encoder.pretrained_model = bert-base-cased
2021-11-13 00:15:22.490 type = bert-base-cased
2021-11-13 00:15:22.490 model.encoder.override_weights_file = None
2021-11-13 00:15:22.490 model.encoder.override_weights_strip_prefix = None
2021-11-13 00:15:22.490 model.encoder.load_weights = True
2021-11-13 00:15:22.490 model.encoder.requires_grad = True
2021-11-13 00:15:22.490 model.encoder.dropout = 0.0
2021-11-13 00:15:22.490 model.encoder.transformer_kwargs = None
2021-11-13 00:15:22.692 removing temporary unarchived model dir at /tmp/tmpc58oln2p
2021-11-13 00:44:28.065 Plugin allennlp_models available
2021-11-13 00:44:28.076 Plugin allennlp_server available
2021-11-13 00:44:28.077 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-13 00:44:28.077 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp3o_drasa
2021-11-13 00:44:30.179 dataset_reader.type = example_reader
2021-11-13 00:44:30.179 dataset_reader.max_instances = None
2021-11-13 00:44:30.179 dataset_reader.manual_distributed_sharding = False
2021-11-13 00:44:30.180 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 00:44:30.180 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 00:44:30.180 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 00:44:30.180 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 00:44:30.180 type = bert-base-cased
2021-11-13 00:44:30.180 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 00:44:30.180 dataset_reader.tokenizer.max_length = 128
2021-11-13 00:44:30.180 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 00:44:30.181 dataset_reader.text_token_indexers.type = ref
2021-11-13 00:44:30.182 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 00:44:30.182 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 00:44:30.183 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 00:44:30.183 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 00:44:30.183 type = bert-base-cased
2021-11-13 00:44:30.183 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 00:44:30.183 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 00:44:30.183 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 00:44:30.184 dataset_reader.to_index = 6
2021-11-13 00:44:30.184 dataset_reader.type = example_reader
2021-11-13 00:44:30.184 dataset_reader.max_instances = None
2021-11-13 00:44:30.184 dataset_reader.manual_distributed_sharding = False
2021-11-13 00:44:30.184 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 00:44:30.184 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 00:44:30.184 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 00:44:30.185 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 00:44:30.185 type = bert-base-cased
2021-11-13 00:44:30.185 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 00:44:30.185 dataset_reader.tokenizer.max_length = 128
2021-11-13 00:44:30.185 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 00:44:30.186 dataset_reader.text_token_indexers.type = ref
2021-11-13 00:44:30.186 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 00:44:30.186 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 00:44:30.186 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 00:44:30.187 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 00:44:30.187 type = bert-base-cased
2021-11-13 00:44:30.187 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 00:44:30.187 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 00:44:30.187 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 00:44:30.188 dataset_reader.to_index = 6
2021-11-13 00:44:30.188 type = from_instances
2021-11-13 00:44:30.188 Loading token dictionary from /tmp/tmp3o_drasa/vocabulary.
2021-11-13 00:44:30.188 model.type = sentence_level_classifier
2021-11-13 00:44:30.189 model.embedder.type = ref
2021-11-13 00:44:30.189 model.embedder.type = basic
2021-11-13 00:44:30.189 model.embedder.token_embedders.type = ref
2021-11-13 00:44:30.190 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 00:44:30.190 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 00:44:30.191 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-13 00:44:30.191 type = bert-base-cased
2021-11-13 00:44:30.191 model.embedder.token_embedders.tokens.max_length = None
2021-11-13 00:44:30.191 model.embedder.token_embedders.tokens.sub_module = None
2021-11-13 00:44:30.191 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-13 00:44:30.191 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-13 00:44:30.191 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-13 00:44:30.191 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-13 00:44:30.191 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-13 00:44:30.191 model.embedder.token_embedders.tokens.load_weights = True
2021-11-13 00:44:30.191 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-13 00:44:30.191 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-13 00:44:30.191 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-13 00:44:30.239 model.encoder.type = bert_pooler
2021-11-13 00:44:30.239 model.encoder.type = bert_pooler
2021-11-13 00:44:30.240 model.encoder.pretrained_model = bert-base-cased
2021-11-13 00:44:30.240 type = bert-base-cased
2021-11-13 00:44:30.240 model.encoder.override_weights_file = None
2021-11-13 00:44:30.240 model.encoder.override_weights_strip_prefix = None
2021-11-13 00:44:30.240 model.encoder.load_weights = True
2021-11-13 00:44:30.240 model.encoder.requires_grad = True
2021-11-13 00:44:30.240 model.encoder.dropout = 0.0
2021-11-13 00:44:30.241 model.encoder.transformer_kwargs = None
2021-11-13 00:44:30.448 removing temporary unarchived model dir at /tmp/tmp3o_drasa
2021-11-13 00:45:51.073 Plugin allennlp_models available
2021-11-13 00:45:51.083 Plugin allennlp_server available
2021-11-13 00:45:51.084 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-13 00:45:51.084 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp5yi4h_61
2021-11-13 00:45:53.201 dataset_reader.type = example_reader
2021-11-13 00:45:53.202 dataset_reader.max_instances = None
2021-11-13 00:45:53.202 dataset_reader.manual_distributed_sharding = False
2021-11-13 00:45:53.202 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 00:45:53.202 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 00:45:53.202 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 00:45:53.202 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 00:45:53.202 type = bert-base-cased
2021-11-13 00:45:53.202 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 00:45:53.202 dataset_reader.tokenizer.max_length = 128
2021-11-13 00:45:53.202 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 00:45:53.203 dataset_reader.text_token_indexers.type = ref
2021-11-13 00:45:53.205 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 00:45:53.205 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 00:45:53.205 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 00:45:53.205 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 00:45:53.205 type = bert-base-cased
2021-11-13 00:45:53.205 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 00:45:53.205 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 00:45:53.206 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 00:45:53.207 dataset_reader.to_index = 6
2021-11-13 00:45:53.207 dataset_reader.type = example_reader
2021-11-13 00:45:53.207 dataset_reader.max_instances = None
2021-11-13 00:45:53.207 dataset_reader.manual_distributed_sharding = False
2021-11-13 00:45:53.207 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 00:45:53.207 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 00:45:53.207 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 00:45:53.207 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 00:45:53.207 type = bert-base-cased
2021-11-13 00:45:53.207 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 00:45:53.207 dataset_reader.tokenizer.max_length = 128
2021-11-13 00:45:53.207 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 00:45:53.208 dataset_reader.text_token_indexers.type = ref
2021-11-13 00:45:53.209 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 00:45:53.209 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 00:45:53.209 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 00:45:53.209 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 00:45:53.209 type = bert-base-cased
2021-11-13 00:45:53.209 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 00:45:53.209 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 00:45:53.209 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 00:45:53.210 dataset_reader.to_index = 6
2021-11-13 00:45:53.210 type = from_instances
2021-11-13 00:45:53.211 Loading token dictionary from /tmp/tmp5yi4h_61/vocabulary.
2021-11-13 00:45:53.211 model.type = sentence_level_classifier
2021-11-13 00:45:53.211 model.embedder.type = ref
2021-11-13 00:45:53.212 model.embedder.type = basic
2021-11-13 00:45:53.212 model.embedder.token_embedders.type = ref
2021-11-13 00:45:53.213 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 00:45:53.213 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 00:45:53.213 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-13 00:45:53.213 type = bert-base-cased
2021-11-13 00:45:53.213 model.embedder.token_embedders.tokens.max_length = None
2021-11-13 00:45:53.213 model.embedder.token_embedders.tokens.sub_module = None
2021-11-13 00:45:53.213 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-13 00:45:53.213 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-13 00:45:53.213 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-13 00:45:53.214 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-13 00:45:53.214 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-13 00:45:53.214 model.embedder.token_embedders.tokens.load_weights = True
2021-11-13 00:45:53.214 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-13 00:45:53.214 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-13 00:45:53.214 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-13 00:45:53.261 model.encoder.type = bert_pooler
2021-11-13 00:45:53.262 model.encoder.type = bert_pooler
2021-11-13 00:45:53.262 model.encoder.pretrained_model = bert-base-cased
2021-11-13 00:45:53.262 type = bert-base-cased
2021-11-13 00:45:53.262 model.encoder.override_weights_file = None
2021-11-13 00:45:53.262 model.encoder.override_weights_strip_prefix = None
2021-11-13 00:45:53.262 model.encoder.load_weights = True
2021-11-13 00:45:53.262 model.encoder.requires_grad = True
2021-11-13 00:45:53.262 model.encoder.dropout = 0.0
2021-11-13 00:45:53.262 model.encoder.transformer_kwargs = None
2021-11-13 00:45:53.464 removing temporary unarchived model dir at /tmp/tmp5yi4h_61
2021-11-13 00:46:59.675 Plugin allennlp_models available
2021-11-13 00:46:59.684 Plugin allennlp_server available
2021-11-13 00:46:59.685 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-13 00:46:59.685 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpalbl7mwu
2021-11-13 00:47:01.800 dataset_reader.type = example_reader
2021-11-13 00:47:01.800 dataset_reader.max_instances = None
2021-11-13 00:47:01.800 dataset_reader.manual_distributed_sharding = False
2021-11-13 00:47:01.800 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 00:47:01.800 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 00:47:01.800 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 00:47:01.800 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 00:47:01.800 type = bert-base-cased
2021-11-13 00:47:01.800 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 00:47:01.800 dataset_reader.tokenizer.max_length = 128
2021-11-13 00:47:01.801 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 00:47:01.802 dataset_reader.text_token_indexers.type = ref
2021-11-13 00:47:01.803 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 00:47:01.803 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 00:47:01.803 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 00:47:01.803 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 00:47:01.803 type = bert-base-cased
2021-11-13 00:47:01.803 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 00:47:01.803 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 00:47:01.803 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 00:47:01.804 dataset_reader.to_index = 6
2021-11-13 00:47:01.804 dataset_reader.type = example_reader
2021-11-13 00:47:01.805 dataset_reader.max_instances = None
2021-11-13 00:47:01.805 dataset_reader.manual_distributed_sharding = False
2021-11-13 00:47:01.805 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 00:47:01.805 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 00:47:01.805 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 00:47:01.805 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 00:47:01.805 type = bert-base-cased
2021-11-13 00:47:01.805 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 00:47:01.805 dataset_reader.tokenizer.max_length = 128
2021-11-13 00:47:01.805 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 00:47:01.806 dataset_reader.text_token_indexers.type = ref
2021-11-13 00:47:01.807 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 00:47:01.807 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 00:47:01.807 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 00:47:01.807 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 00:47:01.807 type = bert-base-cased
2021-11-13 00:47:01.807 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 00:47:01.807 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 00:47:01.807 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 00:47:01.808 dataset_reader.to_index = 6
2021-11-13 00:47:01.808 type = from_instances
2021-11-13 00:47:01.809 Loading token dictionary from /tmp/tmpalbl7mwu/vocabulary.
2021-11-13 00:47:01.809 model.type = sentence_level_classifier
2021-11-13 00:47:01.810 model.embedder.type = ref
2021-11-13 00:47:01.812 model.embedder.type = basic
2021-11-13 00:47:01.812 model.embedder.token_embedders.type = ref
2021-11-13 00:47:01.814 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 00:47:01.815 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 00:47:01.815 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-13 00:47:01.816 type = bert-base-cased
2021-11-13 00:47:01.816 model.embedder.token_embedders.tokens.max_length = None
2021-11-13 00:47:01.816 model.embedder.token_embedders.tokens.sub_module = None
2021-11-13 00:47:01.816 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-13 00:47:01.816 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-13 00:47:01.816 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-13 00:47:01.817 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-13 00:47:01.817 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-13 00:47:01.817 model.embedder.token_embedders.tokens.load_weights = True
2021-11-13 00:47:01.817 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-13 00:47:01.817 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-13 00:47:01.817 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-13 00:47:01.867 model.encoder.type = bert_pooler
2021-11-13 00:47:01.867 model.encoder.type = bert_pooler
2021-11-13 00:47:01.868 model.encoder.pretrained_model = bert-base-cased
2021-11-13 00:47:01.868 type = bert-base-cased
2021-11-13 00:47:01.868 model.encoder.override_weights_file = None
2021-11-13 00:47:01.868 model.encoder.override_weights_strip_prefix = None
2021-11-13 00:47:01.868 model.encoder.load_weights = True
2021-11-13 00:47:01.868 model.encoder.requires_grad = True
2021-11-13 00:47:01.868 model.encoder.dropout = 0.0
2021-11-13 00:47:01.868 model.encoder.transformer_kwargs = None
2021-11-13 00:47:02.067 removing temporary unarchived model dir at /tmp/tmpalbl7mwu
2021-11-13 01:04:06.654 Plugin allennlp_models available
2021-11-13 01:04:06.659 Plugin allennlp_server available
2021-11-13 01:04:06.659 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-13 01:04:06.659 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpeq0k4g1f
2021-11-13 01:04:08.760 dataset_reader.type = example_reader
2021-11-13 01:04:08.760 dataset_reader.max_instances = None
2021-11-13 01:04:08.760 dataset_reader.manual_distributed_sharding = False
2021-11-13 01:04:08.760 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 01:04:08.760 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 01:04:08.760 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 01:04:08.761 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 01:04:08.761 type = bert-base-cased
2021-11-13 01:04:08.761 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 01:04:08.761 dataset_reader.tokenizer.max_length = 128
2021-11-13 01:04:08.761 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 01:04:08.762 dataset_reader.text_token_indexers.type = ref
2021-11-13 01:04:08.763 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 01:04:08.763 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 01:04:08.763 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 01:04:08.763 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 01:04:08.763 type = bert-base-cased
2021-11-13 01:04:08.763 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 01:04:08.763 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 01:04:08.763 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 01:04:08.764 dataset_reader.to_index = 6
2021-11-13 01:04:08.764 dataset_reader.type = example_reader
2021-11-13 01:04:08.765 dataset_reader.max_instances = None
2021-11-13 01:04:08.765 dataset_reader.manual_distributed_sharding = False
2021-11-13 01:04:08.765 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 01:04:08.765 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 01:04:08.765 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 01:04:08.766 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 01:04:08.766 type = bert-base-cased
2021-11-13 01:04:08.766 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 01:04:08.766 dataset_reader.tokenizer.max_length = 128
2021-11-13 01:04:08.766 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 01:04:08.767 dataset_reader.text_token_indexers.type = ref
2021-11-13 01:04:08.768 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 01:04:08.768 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 01:04:08.768 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 01:04:08.768 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 01:04:08.768 type = bert-base-cased
2021-11-13 01:04:08.768 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 01:04:08.768 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 01:04:08.768 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 01:04:08.769 dataset_reader.to_index = 6
2021-11-13 01:04:08.769 type = from_instances
2021-11-13 01:04:08.769 Loading token dictionary from /tmp/tmpeq0k4g1f/vocabulary.
2021-11-13 01:04:08.770 model.type = sentence_level_classifier
2021-11-13 01:04:08.770 model.embedder.type = ref
2021-11-13 01:04:08.771 model.embedder.type = basic
2021-11-13 01:04:08.771 model.embedder.token_embedders.type = ref
2021-11-13 01:04:08.772 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 01:04:08.772 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 01:04:08.773 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-13 01:04:08.773 type = bert-base-cased
2021-11-13 01:04:08.773 model.embedder.token_embedders.tokens.max_length = None
2021-11-13 01:04:08.773 model.embedder.token_embedders.tokens.sub_module = None
2021-11-13 01:04:08.773 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-13 01:04:08.773 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-13 01:04:08.773 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-13 01:04:08.773 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-13 01:04:08.773 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-13 01:04:08.773 model.embedder.token_embedders.tokens.load_weights = True
2021-11-13 01:04:08.773 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-13 01:04:08.773 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-13 01:04:08.773 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-13 01:04:08.820 model.encoder.type = bert_pooler
2021-11-13 01:04:08.820 model.encoder.type = bert_pooler
2021-11-13 01:04:08.820 model.encoder.pretrained_model = bert-base-cased
2021-11-13 01:04:08.820 type = bert-base-cased
2021-11-13 01:04:08.820 model.encoder.override_weights_file = None
2021-11-13 01:04:08.821 model.encoder.override_weights_strip_prefix = None
2021-11-13 01:04:08.821 model.encoder.load_weights = True
2021-11-13 01:04:08.821 model.encoder.requires_grad = True
2021-11-13 01:04:08.821 model.encoder.dropout = 0.0
2021-11-13 01:04:08.821 model.encoder.transformer_kwargs = None
2021-11-13 01:04:09.021 removing temporary unarchived model dir at /tmp/tmpeq0k4g1f
2021-11-13 01:50:54.296 Plugin allennlp_models available
2021-11-13 01:50:54.305 Plugin allennlp_server available
2021-11-13 01:50:54.306 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-13 01:50:54.306 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpd38vfj58
2021-11-13 01:50:56.414 dataset_reader.type = example_reader
2021-11-13 01:50:56.415 dataset_reader.max_instances = None
2021-11-13 01:50:56.415 dataset_reader.manual_distributed_sharding = False
2021-11-13 01:50:56.415 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 01:50:56.415 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 01:50:56.415 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 01:50:56.415 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 01:50:56.415 type = bert-base-cased
2021-11-13 01:50:56.415 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 01:50:56.415 dataset_reader.tokenizer.max_length = 128
2021-11-13 01:50:56.415 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 01:50:56.417 dataset_reader.text_token_indexers.type = ref
2021-11-13 01:50:56.418 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 01:50:56.418 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 01:50:56.418 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 01:50:56.418 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 01:50:56.418 type = bert-base-cased
2021-11-13 01:50:56.418 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 01:50:56.418 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 01:50:56.419 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 01:50:56.419 dataset_reader.to_index = 6
2021-11-13 01:50:56.420 dataset_reader.type = example_reader
2021-11-13 01:50:56.420 dataset_reader.max_instances = None
2021-11-13 01:50:56.420 dataset_reader.manual_distributed_sharding = False
2021-11-13 01:50:56.420 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 01:50:56.420 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 01:50:56.420 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 01:50:56.420 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 01:50:56.420 type = bert-base-cased
2021-11-13 01:50:56.420 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 01:50:56.420 dataset_reader.tokenizer.max_length = 128
2021-11-13 01:50:56.421 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 01:50:56.421 dataset_reader.text_token_indexers.type = ref
2021-11-13 01:50:56.422 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 01:50:56.422 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 01:50:56.422 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 01:50:56.422 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 01:50:56.422 type = bert-base-cased
2021-11-13 01:50:56.422 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 01:50:56.422 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 01:50:56.422 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 01:50:56.423 dataset_reader.to_index = 6
2021-11-13 01:50:56.423 type = from_instances
2021-11-13 01:50:56.423 Loading token dictionary from /tmp/tmpd38vfj58/vocabulary.
2021-11-13 01:50:56.424 model.type = sentence_level_classifier
2021-11-13 01:50:56.424 model.embedder.type = ref
2021-11-13 01:50:56.425 model.embedder.type = basic
2021-11-13 01:50:56.425 model.embedder.token_embedders.type = ref
2021-11-13 01:50:56.426 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 01:50:56.426 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 01:50:56.426 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-13 01:50:56.426 type = bert-base-cased
2021-11-13 01:50:56.426 model.embedder.token_embedders.tokens.max_length = None
2021-11-13 01:50:56.427 model.embedder.token_embedders.tokens.sub_module = None
2021-11-13 01:50:56.427 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-13 01:50:56.427 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-13 01:50:56.427 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-13 01:50:56.427 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-13 01:50:56.427 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-13 01:50:56.427 model.embedder.token_embedders.tokens.load_weights = True
2021-11-13 01:50:56.427 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-13 01:50:56.427 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-13 01:50:56.427 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-13 01:50:56.477 model.encoder.type = bert_pooler
2021-11-13 01:50:56.477 model.encoder.type = bert_pooler
2021-11-13 01:50:56.478 model.encoder.pretrained_model = bert-base-cased
2021-11-13 01:50:56.478 type = bert-base-cased
2021-11-13 01:50:56.478 model.encoder.override_weights_file = None
2021-11-13 01:50:56.478 model.encoder.override_weights_strip_prefix = None
2021-11-13 01:50:56.478 model.encoder.load_weights = True
2021-11-13 01:50:56.478 model.encoder.requires_grad = True
2021-11-13 01:50:56.478 model.encoder.dropout = 0.0
2021-11-13 01:50:56.478 model.encoder.transformer_kwargs = None
2021-11-13 01:50:56.678 removing temporary unarchived model dir at /tmp/tmpd38vfj58
2021-11-13 02:19:41.319 Plugin allennlp_models available
2021-11-13 02:19:41.328 Plugin allennlp_server available
2021-11-13 02:19:41.328 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-13 02:19:41.328 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpbe9bw8au
2021-11-13 02:19:43.453 dataset_reader.type = example_reader
2021-11-13 02:19:43.453 dataset_reader.max_instances = None
2021-11-13 02:19:43.453 dataset_reader.manual_distributed_sharding = False
2021-11-13 02:19:43.453 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 02:19:43.453 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 02:19:43.453 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 02:19:43.454 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 02:19:43.454 type = bert-base-cased
2021-11-13 02:19:43.454 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 02:19:43.454 dataset_reader.tokenizer.max_length = 128
2021-11-13 02:19:43.454 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 02:19:43.455 dataset_reader.text_token_indexers.type = ref
2021-11-13 02:19:43.456 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 02:19:43.456 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 02:19:43.457 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 02:19:43.457 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 02:19:43.457 type = bert-base-cased
2021-11-13 02:19:43.457 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 02:19:43.457 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 02:19:43.457 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 02:19:43.458 dataset_reader.to_index = 6
2021-11-13 02:19:43.458 dataset_reader.type = example_reader
2021-11-13 02:19:43.458 dataset_reader.max_instances = None
2021-11-13 02:19:43.458 dataset_reader.manual_distributed_sharding = False
2021-11-13 02:19:43.458 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 02:19:43.458 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 02:19:43.458 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 02:19:43.458 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 02:19:43.458 type = bert-base-cased
2021-11-13 02:19:43.459 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 02:19:43.459 dataset_reader.tokenizer.max_length = 128
2021-11-13 02:19:43.459 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 02:19:43.459 dataset_reader.text_token_indexers.type = ref
2021-11-13 02:19:43.460 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 02:19:43.460 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 02:19:43.460 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 02:19:43.460 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 02:19:43.460 type = bert-base-cased
2021-11-13 02:19:43.461 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 02:19:43.461 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 02:19:43.461 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 02:19:43.461 dataset_reader.to_index = 6
2021-11-13 02:19:43.462 type = from_instances
2021-11-13 02:19:43.462 Loading token dictionary from /tmp/tmpbe9bw8au/vocabulary.
2021-11-13 02:19:43.462 model.type = sentence_level_classifier
2021-11-13 02:19:43.462 model.embedder.type = ref
2021-11-13 02:19:43.463 model.embedder.type = basic
2021-11-13 02:19:43.463 model.embedder.token_embedders.type = ref
2021-11-13 02:19:43.464 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 02:19:43.464 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 02:19:43.464 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-13 02:19:43.464 type = bert-base-cased
2021-11-13 02:19:43.464 model.embedder.token_embedders.tokens.max_length = None
2021-11-13 02:19:43.464 model.embedder.token_embedders.tokens.sub_module = None
2021-11-13 02:19:43.464 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-13 02:19:43.465 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-13 02:19:43.465 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-13 02:19:43.465 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-13 02:19:43.465 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-13 02:19:43.465 model.embedder.token_embedders.tokens.load_weights = True
2021-11-13 02:19:43.465 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-13 02:19:43.465 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-13 02:19:43.465 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-13 02:19:43.514 model.encoder.type = bert_pooler
2021-11-13 02:19:43.515 model.encoder.type = bert_pooler
2021-11-13 02:19:43.515 model.encoder.pretrained_model = bert-base-cased
2021-11-13 02:19:43.515 type = bert-base-cased
2021-11-13 02:19:43.515 model.encoder.override_weights_file = None
2021-11-13 02:19:43.515 model.encoder.override_weights_strip_prefix = None
2021-11-13 02:19:43.515 model.encoder.load_weights = True
2021-11-13 02:19:43.515 model.encoder.requires_grad = True
2021-11-13 02:19:43.515 model.encoder.dropout = 0.0
2021-11-13 02:19:43.515 model.encoder.transformer_kwargs = None
2021-11-13 02:19:43.715 removing temporary unarchived model dir at /tmp/tmpbe9bw8au
2021-11-13 02:37:28.411 Plugin allennlp_models available
2021-11-13 02:37:28.418 Plugin allennlp_server available
2021-11-13 02:37:28.419 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-13 02:37:28.419 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmplthw7zx2
2021-11-13 02:37:30.535 dataset_reader.type = example_reader
2021-11-13 02:37:30.536 dataset_reader.max_instances = None
2021-11-13 02:37:30.536 dataset_reader.manual_distributed_sharding = False
2021-11-13 02:37:30.536 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 02:37:30.536 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 02:37:30.536 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 02:37:30.536 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 02:37:30.536 type = bert-base-cased
2021-11-13 02:37:30.536 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 02:37:30.536 dataset_reader.tokenizer.max_length = 128
2021-11-13 02:37:30.536 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 02:37:30.537 dataset_reader.text_token_indexers.type = ref
2021-11-13 02:37:30.539 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 02:37:30.539 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 02:37:30.539 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 02:37:30.539 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 02:37:30.539 type = bert-base-cased
2021-11-13 02:37:30.539 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 02:37:30.539 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 02:37:30.539 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 02:37:30.540 dataset_reader.to_index = 6
2021-11-13 02:37:30.541 dataset_reader.type = example_reader
2021-11-13 02:37:30.541 dataset_reader.max_instances = None
2021-11-13 02:37:30.541 dataset_reader.manual_distributed_sharding = False
2021-11-13 02:37:30.541 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 02:37:30.541 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 02:37:30.541 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 02:37:30.541 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 02:37:30.541 type = bert-base-cased
2021-11-13 02:37:30.541 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 02:37:30.541 dataset_reader.tokenizer.max_length = 128
2021-11-13 02:37:30.541 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 02:37:30.542 dataset_reader.text_token_indexers.type = ref
2021-11-13 02:37:30.543 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 02:37:30.543 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 02:37:30.543 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 02:37:30.543 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 02:37:30.543 type = bert-base-cased
2021-11-13 02:37:30.543 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 02:37:30.543 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 02:37:30.543 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 02:37:30.544 dataset_reader.to_index = 6
2021-11-13 02:37:30.545 type = from_instances
2021-11-13 02:37:30.545 Loading token dictionary from /tmp/tmplthw7zx2/vocabulary.
2021-11-13 02:37:30.545 model.type = sentence_level_classifier
2021-11-13 02:37:30.545 model.embedder.type = ref
2021-11-13 02:37:30.546 model.embedder.type = basic
2021-11-13 02:37:30.546 model.embedder.token_embedders.type = ref
2021-11-13 02:37:30.547 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 02:37:30.547 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 02:37:30.547 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-13 02:37:30.547 type = bert-base-cased
2021-11-13 02:37:30.547 model.embedder.token_embedders.tokens.max_length = None
2021-11-13 02:37:30.547 model.embedder.token_embedders.tokens.sub_module = None
2021-11-13 02:37:30.547 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-13 02:37:30.547 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-13 02:37:30.547 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-13 02:37:30.547 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-13 02:37:30.547 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-13 02:37:30.547 model.embedder.token_embedders.tokens.load_weights = True
2021-11-13 02:37:30.548 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-13 02:37:30.548 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-13 02:37:30.548 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-13 02:37:30.596 model.encoder.type = bert_pooler
2021-11-13 02:37:30.596 model.encoder.type = bert_pooler
2021-11-13 02:37:30.596 model.encoder.pretrained_model = bert-base-cased
2021-11-13 02:37:30.596 type = bert-base-cased
2021-11-13 02:37:30.596 model.encoder.override_weights_file = None
2021-11-13 02:37:30.596 model.encoder.override_weights_strip_prefix = None
2021-11-13 02:37:30.596 model.encoder.load_weights = True
2021-11-13 02:37:30.597 model.encoder.requires_grad = True
2021-11-13 02:37:30.597 model.encoder.dropout = 0.0
2021-11-13 02:37:30.597 model.encoder.transformer_kwargs = None
2021-11-13 02:37:30.797 removing temporary unarchived model dir at /tmp/tmplthw7zx2
2021-11-13 02:48:15.424 Plugin allennlp_models available
2021-11-13 02:48:15.431 Plugin allennlp_server available
2021-11-13 02:48:15.431 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-13 02:48:15.431 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmptef_76zo
2021-11-13 02:48:17.554 dataset_reader.type = example_reader
2021-11-13 02:48:17.555 dataset_reader.max_instances = None
2021-11-13 02:48:17.555 dataset_reader.manual_distributed_sharding = False
2021-11-13 02:48:17.555 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 02:48:17.555 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 02:48:17.555 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 02:48:17.555 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 02:48:17.555 type = bert-base-cased
2021-11-13 02:48:17.555 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 02:48:17.555 dataset_reader.tokenizer.max_length = 128
2021-11-13 02:48:17.555 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 02:48:17.557 dataset_reader.text_token_indexers.type = ref
2021-11-13 02:48:17.558 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 02:48:17.558 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 02:48:17.558 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 02:48:17.558 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 02:48:17.558 type = bert-base-cased
2021-11-13 02:48:17.558 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 02:48:17.558 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 02:48:17.558 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 02:48:17.559 dataset_reader.to_index = 6
2021-11-13 02:48:17.560 dataset_reader.type = example_reader
2021-11-13 02:48:17.560 dataset_reader.max_instances = None
2021-11-13 02:48:17.560 dataset_reader.manual_distributed_sharding = False
2021-11-13 02:48:17.560 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 02:48:17.560 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 02:48:17.560 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 02:48:17.560 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 02:48:17.560 type = bert-base-cased
2021-11-13 02:48:17.560 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 02:48:17.560 dataset_reader.tokenizer.max_length = 128
2021-11-13 02:48:17.560 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 02:48:17.561 dataset_reader.text_token_indexers.type = ref
2021-11-13 02:48:17.562 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 02:48:17.562 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 02:48:17.562 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 02:48:17.562 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 02:48:17.562 type = bert-base-cased
2021-11-13 02:48:17.562 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 02:48:17.562 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 02:48:17.563 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 02:48:17.563 dataset_reader.to_index = 6
2021-11-13 02:48:17.563 type = from_instances
2021-11-13 02:48:17.564 Loading token dictionary from /tmp/tmptef_76zo/vocabulary.
2021-11-13 02:48:17.564 model.type = sentence_level_classifier
2021-11-13 02:48:17.564 model.embedder.type = ref
2021-11-13 02:48:17.565 model.embedder.type = basic
2021-11-13 02:48:17.565 model.embedder.token_embedders.type = ref
2021-11-13 02:48:17.566 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 02:48:17.566 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 02:48:17.566 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-13 02:48:17.566 type = bert-base-cased
2021-11-13 02:48:17.566 model.embedder.token_embedders.tokens.max_length = None
2021-11-13 02:48:17.566 model.embedder.token_embedders.tokens.sub_module = None
2021-11-13 02:48:17.566 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-13 02:48:17.566 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-13 02:48:17.566 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-13 02:48:17.566 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-13 02:48:17.566 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-13 02:48:17.567 model.embedder.token_embedders.tokens.load_weights = True
2021-11-13 02:48:17.567 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-13 02:48:17.567 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-13 02:48:17.567 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-13 02:48:17.609 model.encoder.type = bert_pooler
2021-11-13 02:48:17.609 model.encoder.type = bert_pooler
2021-11-13 02:48:17.609 model.encoder.pretrained_model = bert-base-cased
2021-11-13 02:48:17.609 type = bert-base-cased
2021-11-13 02:48:17.609 model.encoder.override_weights_file = None
2021-11-13 02:48:17.609 model.encoder.override_weights_strip_prefix = None
2021-11-13 02:48:17.609 model.encoder.load_weights = True
2021-11-13 02:48:17.609 model.encoder.requires_grad = True
2021-11-13 02:48:17.609 model.encoder.dropout = 0.0
2021-11-13 02:48:17.609 model.encoder.transformer_kwargs = None
2021-11-13 02:48:17.806 removing temporary unarchived model dir at /tmp/tmptef_76zo
2021-11-13 02:56:02.343 Plugin allennlp_models available
2021-11-13 02:56:02.354 Plugin allennlp_server available
2021-11-13 02:56:02.354 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-13 02:56:02.355 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpkr0eunzm
2021-11-13 02:56:04.491 dataset_reader.type = example_reader
2021-11-13 02:56:04.491 dataset_reader.max_instances = None
2021-11-13 02:56:04.491 dataset_reader.manual_distributed_sharding = False
2021-11-13 02:56:04.491 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 02:56:04.491 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 02:56:04.491 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 02:56:04.491 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 02:56:04.491 type = bert-base-cased
2021-11-13 02:56:04.491 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 02:56:04.491 dataset_reader.tokenizer.max_length = 128
2021-11-13 02:56:04.491 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 02:56:04.493 dataset_reader.text_token_indexers.type = ref
2021-11-13 02:56:04.494 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 02:56:04.494 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 02:56:04.494 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 02:56:04.494 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 02:56:04.494 type = bert-base-cased
2021-11-13 02:56:04.494 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 02:56:04.495 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 02:56:04.495 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 02:56:04.495 dataset_reader.to_index = 6
2021-11-13 02:56:04.496 dataset_reader.type = example_reader
2021-11-13 02:56:04.496 dataset_reader.max_instances = None
2021-11-13 02:56:04.496 dataset_reader.manual_distributed_sharding = False
2021-11-13 02:56:04.496 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 02:56:04.496 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 02:56:04.496 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 02:56:04.496 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 02:56:04.496 type = bert-base-cased
2021-11-13 02:56:04.497 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 02:56:04.497 dataset_reader.tokenizer.max_length = 128
2021-11-13 02:56:04.497 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 02:56:04.497 dataset_reader.text_token_indexers.type = ref
2021-11-13 02:56:04.498 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 02:56:04.498 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 02:56:04.498 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 02:56:04.498 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 02:56:04.498 type = bert-base-cased
2021-11-13 02:56:04.498 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 02:56:04.498 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 02:56:04.498 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 02:56:04.499 dataset_reader.to_index = 6
2021-11-13 02:56:04.500 type = from_instances
2021-11-13 02:56:04.500 Loading token dictionary from /tmp/tmpkr0eunzm/vocabulary.
2021-11-13 02:56:04.500 model.type = sentence_level_classifier
2021-11-13 02:56:04.500 model.embedder.type = ref
2021-11-13 02:56:04.501 model.embedder.type = basic
2021-11-13 02:56:04.501 model.embedder.token_embedders.type = ref
2021-11-13 02:56:04.502 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 02:56:04.502 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 02:56:04.502 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-13 02:56:04.502 type = bert-base-cased
2021-11-13 02:56:04.502 model.embedder.token_embedders.tokens.max_length = None
2021-11-13 02:56:04.502 model.embedder.token_embedders.tokens.sub_module = None
2021-11-13 02:56:04.502 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-13 02:56:04.502 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-13 02:56:04.503 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-13 02:56:04.503 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-13 02:56:04.503 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-13 02:56:04.503 model.embedder.token_embedders.tokens.load_weights = True
2021-11-13 02:56:04.503 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-13 02:56:04.503 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-13 02:56:04.503 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-13 02:56:04.554 model.encoder.type = bert_pooler
2021-11-13 02:56:04.554 model.encoder.type = bert_pooler
2021-11-13 02:56:04.554 model.encoder.pretrained_model = bert-base-cased
2021-11-13 02:56:04.554 type = bert-base-cased
2021-11-13 02:56:04.554 model.encoder.override_weights_file = None
2021-11-13 02:56:04.554 model.encoder.override_weights_strip_prefix = None
2021-11-13 02:56:04.555 model.encoder.load_weights = True
2021-11-13 02:56:04.555 model.encoder.requires_grad = True
2021-11-13 02:56:04.555 model.encoder.dropout = 0.0
2021-11-13 02:56:04.555 model.encoder.transformer_kwargs = None
2021-11-13 02:56:04.757 removing temporary unarchived model dir at /tmp/tmpkr0eunzm
2021-11-13 03:01:49.489 Plugin allennlp_models available
2021-11-13 03:01:49.499 Plugin allennlp_server available
2021-11-13 03:01:49.499 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-13 03:01:49.500 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpbfbecscv
2021-11-13 03:01:51.632 dataset_reader.type = example_reader
2021-11-13 03:01:51.632 dataset_reader.max_instances = None
2021-11-13 03:01:51.632 dataset_reader.manual_distributed_sharding = False
2021-11-13 03:01:51.632 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 03:01:51.633 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 03:01:51.633 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 03:01:51.633 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 03:01:51.633 type = bert-base-cased
2021-11-13 03:01:51.633 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 03:01:51.633 dataset_reader.tokenizer.max_length = 128
2021-11-13 03:01:51.633 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 03:01:51.634 dataset_reader.text_token_indexers.type = ref
2021-11-13 03:01:51.636 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 03:01:51.636 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 03:01:51.636 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 03:01:51.636 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 03:01:51.636 type = bert-base-cased
2021-11-13 03:01:51.636 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 03:01:51.636 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 03:01:51.636 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 03:01:51.637 dataset_reader.to_index = 6
2021-11-13 03:01:51.637 dataset_reader.type = example_reader
2021-11-13 03:01:51.637 dataset_reader.max_instances = None
2021-11-13 03:01:51.637 dataset_reader.manual_distributed_sharding = False
2021-11-13 03:01:51.637 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 03:01:51.638 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 03:01:51.638 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 03:01:51.638 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 03:01:51.638 type = bert-base-cased
2021-11-13 03:01:51.638 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 03:01:51.638 dataset_reader.tokenizer.max_length = 128
2021-11-13 03:01:51.638 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 03:01:51.639 dataset_reader.text_token_indexers.type = ref
2021-11-13 03:01:51.639 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 03:01:51.640 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 03:01:51.640 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 03:01:51.640 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 03:01:51.640 type = bert-base-cased
2021-11-13 03:01:51.640 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 03:01:51.640 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 03:01:51.640 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 03:01:51.641 dataset_reader.to_index = 6
2021-11-13 03:01:51.641 type = from_instances
2021-11-13 03:01:51.641 Loading token dictionary from /tmp/tmpbfbecscv/vocabulary.
2021-11-13 03:01:51.642 model.type = sentence_level_classifier
2021-11-13 03:01:51.642 model.embedder.type = ref
2021-11-13 03:01:51.643 model.embedder.type = basic
2021-11-13 03:01:51.643 model.embedder.token_embedders.type = ref
2021-11-13 03:01:51.644 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 03:01:51.644 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 03:01:51.644 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-13 03:01:51.644 type = bert-base-cased
2021-11-13 03:01:51.644 model.embedder.token_embedders.tokens.max_length = None
2021-11-13 03:01:51.644 model.embedder.token_embedders.tokens.sub_module = None
2021-11-13 03:01:51.644 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-13 03:01:51.644 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-13 03:01:51.644 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-13 03:01:51.644 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-13 03:01:51.644 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-13 03:01:51.644 model.embedder.token_embedders.tokens.load_weights = True
2021-11-13 03:01:51.644 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-13 03:01:51.644 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-13 03:01:51.644 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-13 03:01:51.686 model.encoder.type = bert_pooler
2021-11-13 03:01:51.686 model.encoder.type = bert_pooler
2021-11-13 03:01:51.686 model.encoder.pretrained_model = bert-base-cased
2021-11-13 03:01:51.686 type = bert-base-cased
2021-11-13 03:01:51.686 model.encoder.override_weights_file = None
2021-11-13 03:01:51.686 model.encoder.override_weights_strip_prefix = None
2021-11-13 03:01:51.686 model.encoder.load_weights = True
2021-11-13 03:01:51.686 model.encoder.requires_grad = True
2021-11-13 03:01:51.686 model.encoder.dropout = 0.0
2021-11-13 03:01:51.686 model.encoder.transformer_kwargs = None
2021-11-13 03:01:51.888 removing temporary unarchived model dir at /tmp/tmpbfbecscv
2021-11-13 03:07:36.388 Plugin allennlp_models available
2021-11-13 03:07:36.397 Plugin allennlp_server available
2021-11-13 03:07:36.397 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-13 03:07:36.398 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpl8icllzl
2021-11-13 03:07:38.515 dataset_reader.type = example_reader
2021-11-13 03:07:38.515 dataset_reader.max_instances = None
2021-11-13 03:07:38.515 dataset_reader.manual_distributed_sharding = False
2021-11-13 03:07:38.515 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 03:07:38.516 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 03:07:38.516 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 03:07:38.516 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 03:07:38.516 type = bert-base-cased
2021-11-13 03:07:38.516 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 03:07:38.516 dataset_reader.tokenizer.max_length = 128
2021-11-13 03:07:38.516 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 03:07:38.517 dataset_reader.text_token_indexers.type = ref
2021-11-13 03:07:38.519 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 03:07:38.519 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 03:07:38.519 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 03:07:38.519 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 03:07:38.519 type = bert-base-cased
2021-11-13 03:07:38.519 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 03:07:38.519 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 03:07:38.519 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 03:07:38.520 dataset_reader.to_index = 6
2021-11-13 03:07:38.520 dataset_reader.type = example_reader
2021-11-13 03:07:38.520 dataset_reader.max_instances = None
2021-11-13 03:07:38.520 dataset_reader.manual_distributed_sharding = False
2021-11-13 03:07:38.520 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 03:07:38.520 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 03:07:38.521 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 03:07:38.521 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 03:07:38.521 type = bert-base-cased
2021-11-13 03:07:38.521 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 03:07:38.521 dataset_reader.tokenizer.max_length = 128
2021-11-13 03:07:38.521 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 03:07:38.522 dataset_reader.text_token_indexers.type = ref
2021-11-13 03:07:38.522 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 03:07:38.522 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 03:07:38.523 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 03:07:38.523 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 03:07:38.523 type = bert-base-cased
2021-11-13 03:07:38.523 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 03:07:38.523 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 03:07:38.523 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 03:07:38.524 dataset_reader.to_index = 6
2021-11-13 03:07:38.524 type = from_instances
2021-11-13 03:07:38.524 Loading token dictionary from /tmp/tmpl8icllzl/vocabulary.
2021-11-13 03:07:38.525 model.type = sentence_level_classifier
2021-11-13 03:07:38.525 model.embedder.type = ref
2021-11-13 03:07:38.525 model.embedder.type = basic
2021-11-13 03:07:38.526 model.embedder.token_embedders.type = ref
2021-11-13 03:07:38.526 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 03:07:38.527 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 03:07:38.527 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-13 03:07:38.527 type = bert-base-cased
2021-11-13 03:07:38.527 model.embedder.token_embedders.tokens.max_length = None
2021-11-13 03:07:38.527 model.embedder.token_embedders.tokens.sub_module = None
2021-11-13 03:07:38.527 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-13 03:07:38.527 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-13 03:07:38.527 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-13 03:07:38.527 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-13 03:07:38.527 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-13 03:07:38.527 model.embedder.token_embedders.tokens.load_weights = True
2021-11-13 03:07:38.527 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-13 03:07:38.527 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-13 03:07:38.527 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-13 03:07:38.579 model.encoder.type = bert_pooler
2021-11-13 03:07:38.579 model.encoder.type = bert_pooler
2021-11-13 03:07:38.579 model.encoder.pretrained_model = bert-base-cased
2021-11-13 03:07:38.579 type = bert-base-cased
2021-11-13 03:07:38.579 model.encoder.override_weights_file = None
2021-11-13 03:07:38.579 model.encoder.override_weights_strip_prefix = None
2021-11-13 03:07:38.579 model.encoder.load_weights = True
2021-11-13 03:07:38.579 model.encoder.requires_grad = True
2021-11-13 03:07:38.579 model.encoder.dropout = 0.0
2021-11-13 03:07:38.579 model.encoder.transformer_kwargs = None
2021-11-13 03:07:38.779 removing temporary unarchived model dir at /tmp/tmpl8icllzl
2021-11-13 05:08:23.386 Plugin allennlp_models available
2021-11-13 05:08:23.415 Plugin allennlp_server available
2021-11-13 05:08:23.416 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-13 05:08:23.416 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpah1urqs7
2021-11-13 05:08:25.533 dataset_reader.type = example_reader
2021-11-13 05:08:25.534 dataset_reader.max_instances = None
2021-11-13 05:08:25.534 dataset_reader.manual_distributed_sharding = False
2021-11-13 05:08:25.534 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 05:08:25.534 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 05:08:25.534 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 05:08:25.534 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 05:08:25.534 type = bert-base-cased
2021-11-13 05:08:25.534 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 05:08:25.534 dataset_reader.tokenizer.max_length = 128
2021-11-13 05:08:25.534 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 05:08:25.536 dataset_reader.text_token_indexers.type = ref
2021-11-13 05:08:25.537 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 05:08:25.537 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 05:08:25.537 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 05:08:25.537 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 05:08:25.537 type = bert-base-cased
2021-11-13 05:08:25.537 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 05:08:25.537 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 05:08:25.537 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 05:08:25.538 dataset_reader.to_index = 6
2021-11-13 05:08:25.538 dataset_reader.type = example_reader
2021-11-13 05:08:25.538 dataset_reader.max_instances = None
2021-11-13 05:08:25.538 dataset_reader.manual_distributed_sharding = False
2021-11-13 05:08:25.538 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 05:08:25.538 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 05:08:25.538 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 05:08:25.538 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 05:08:25.539 type = bert-base-cased
2021-11-13 05:08:25.539 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 05:08:25.539 dataset_reader.tokenizer.max_length = 128
2021-11-13 05:08:25.539 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 05:08:25.540 dataset_reader.text_token_indexers.type = ref
2021-11-13 05:08:25.540 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 05:08:25.540 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 05:08:25.540 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 05:08:25.540 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 05:08:25.541 type = bert-base-cased
2021-11-13 05:08:25.541 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 05:08:25.541 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 05:08:25.541 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 05:08:25.541 dataset_reader.to_index = 6
2021-11-13 05:08:25.541 type = from_instances
2021-11-13 05:08:25.542 Loading token dictionary from /tmp/tmpah1urqs7/vocabulary.
2021-11-13 05:08:25.542 model.type = sentence_level_classifier
2021-11-13 05:08:25.542 model.embedder.type = ref
2021-11-13 05:08:25.543 model.embedder.type = basic
2021-11-13 05:08:25.543 model.embedder.token_embedders.type = ref
2021-11-13 05:08:25.544 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 05:08:25.544 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 05:08:25.544 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-13 05:08:25.544 type = bert-base-cased
2021-11-13 05:08:25.544 model.embedder.token_embedders.tokens.max_length = None
2021-11-13 05:08:25.544 model.embedder.token_embedders.tokens.sub_module = None
2021-11-13 05:08:25.544 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-13 05:08:25.545 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-13 05:08:25.545 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-13 05:08:25.545 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-13 05:08:25.545 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-13 05:08:25.545 model.embedder.token_embedders.tokens.load_weights = True
2021-11-13 05:08:25.545 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-13 05:08:25.545 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-13 05:08:25.545 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-13 05:08:25.593 model.encoder.type = bert_pooler
2021-11-13 05:08:25.594 model.encoder.type = bert_pooler
2021-11-13 05:08:25.594 model.encoder.pretrained_model = bert-base-cased
2021-11-13 05:08:25.594 type = bert-base-cased
2021-11-13 05:08:25.594 model.encoder.override_weights_file = None
2021-11-13 05:08:25.594 model.encoder.override_weights_strip_prefix = None
2021-11-13 05:08:25.594 model.encoder.load_weights = True
2021-11-13 05:08:25.594 model.encoder.requires_grad = True
2021-11-13 05:08:25.594 model.encoder.dropout = 0.0
2021-11-13 05:08:25.594 model.encoder.transformer_kwargs = None
2021-11-13 05:08:25.794 removing temporary unarchived model dir at /tmp/tmpah1urqs7
2021-11-13 06:14:16.305 Plugin allennlp_models available
2021-11-13 06:14:16.314 Plugin allennlp_server available
2021-11-13 06:14:16.315 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-13 06:14:16.315 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp1w3dkeks
2021-11-13 06:14:18.441 dataset_reader.type = example_reader
2021-11-13 06:14:18.441 dataset_reader.max_instances = None
2021-11-13 06:14:18.441 dataset_reader.manual_distributed_sharding = False
2021-11-13 06:14:18.441 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 06:14:18.441 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 06:14:18.442 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 06:14:18.442 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 06:14:18.442 type = bert-base-cased
2021-11-13 06:14:18.442 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 06:14:18.442 dataset_reader.tokenizer.max_length = 128
2021-11-13 06:14:18.442 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 06:14:18.443 dataset_reader.text_token_indexers.type = ref
2021-11-13 06:14:18.444 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 06:14:18.444 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 06:14:18.444 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 06:14:18.444 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 06:14:18.444 type = bert-base-cased
2021-11-13 06:14:18.445 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 06:14:18.445 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 06:14:18.445 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 06:14:18.446 dataset_reader.to_index = 6
2021-11-13 06:14:18.446 dataset_reader.type = example_reader
2021-11-13 06:14:18.446 dataset_reader.max_instances = None
2021-11-13 06:14:18.446 dataset_reader.manual_distributed_sharding = False
2021-11-13 06:14:18.446 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 06:14:18.446 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 06:14:18.446 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 06:14:18.446 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 06:14:18.446 type = bert-base-cased
2021-11-13 06:14:18.446 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 06:14:18.446 dataset_reader.tokenizer.max_length = 128
2021-11-13 06:14:18.447 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 06:14:18.447 dataset_reader.text_token_indexers.type = ref
2021-11-13 06:14:18.448 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 06:14:18.448 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 06:14:18.449 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 06:14:18.449 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 06:14:18.449 type = bert-base-cased
2021-11-13 06:14:18.450 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 06:14:18.450 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 06:14:18.450 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 06:14:18.452 dataset_reader.to_index = 6
2021-11-13 06:14:18.452 type = from_instances
2021-11-13 06:14:18.453 Loading token dictionary from /tmp/tmp1w3dkeks/vocabulary.
2021-11-13 06:14:18.453 model.type = sentence_level_classifier
2021-11-13 06:14:18.454 model.embedder.type = ref
2021-11-13 06:14:18.456 model.embedder.type = basic
2021-11-13 06:14:18.456 model.embedder.token_embedders.type = ref
2021-11-13 06:14:18.459 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 06:14:18.459 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 06:14:18.460 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-13 06:14:18.460 type = bert-base-cased
2021-11-13 06:14:18.460 model.embedder.token_embedders.tokens.max_length = None
2021-11-13 06:14:18.460 model.embedder.token_embedders.tokens.sub_module = None
2021-11-13 06:14:18.461 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-13 06:14:18.461 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-13 06:14:18.461 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-13 06:14:18.461 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-13 06:14:18.461 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-13 06:14:18.462 model.embedder.token_embedders.tokens.load_weights = True
2021-11-13 06:14:18.462 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-13 06:14:18.462 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-13 06:14:18.462 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-13 06:14:18.514 model.encoder.type = bert_pooler
2021-11-13 06:14:18.514 model.encoder.type = bert_pooler
2021-11-13 06:14:18.514 model.encoder.pretrained_model = bert-base-cased
2021-11-13 06:14:18.514 type = bert-base-cased
2021-11-13 06:14:18.514 model.encoder.override_weights_file = None
2021-11-13 06:14:18.514 model.encoder.override_weights_strip_prefix = None
2021-11-13 06:14:18.514 model.encoder.load_weights = True
2021-11-13 06:14:18.514 model.encoder.requires_grad = True
2021-11-13 06:14:18.514 model.encoder.dropout = 0.0
2021-11-13 06:14:18.514 model.encoder.transformer_kwargs = None
2021-11-13 06:14:18.712 removing temporary unarchived model dir at /tmp/tmp1w3dkeks
2021-11-13 07:06:14.349 Plugin allennlp_models available
2021-11-13 07:06:14.358 Plugin allennlp_server available
2021-11-13 07:06:14.359 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-13 07:06:14.359 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpafs1y9xe
2021-11-13 07:06:16.465 dataset_reader.type = example_reader
2021-11-13 07:06:16.465 dataset_reader.max_instances = None
2021-11-13 07:06:16.465 dataset_reader.manual_distributed_sharding = False
2021-11-13 07:06:16.466 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 07:06:16.466 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 07:06:16.466 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 07:06:16.466 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 07:06:16.466 type = bert-base-cased
2021-11-13 07:06:16.466 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 07:06:16.466 dataset_reader.tokenizer.max_length = 128
2021-11-13 07:06:16.466 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 07:06:16.467 dataset_reader.text_token_indexers.type = ref
2021-11-13 07:06:16.469 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 07:06:16.469 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 07:06:16.469 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 07:06:16.469 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 07:06:16.469 type = bert-base-cased
2021-11-13 07:06:16.469 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 07:06:16.469 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 07:06:16.469 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 07:06:16.470 dataset_reader.to_index = 6
2021-11-13 07:06:16.470 dataset_reader.type = example_reader
2021-11-13 07:06:16.470 dataset_reader.max_instances = None
2021-11-13 07:06:16.470 dataset_reader.manual_distributed_sharding = False
2021-11-13 07:06:16.470 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 07:06:16.470 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 07:06:16.470 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 07:06:16.471 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 07:06:16.471 type = bert-base-cased
2021-11-13 07:06:16.471 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 07:06:16.471 dataset_reader.tokenizer.max_length = 128
2021-11-13 07:06:16.471 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 07:06:16.472 dataset_reader.text_token_indexers.type = ref
2021-11-13 07:06:16.472 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 07:06:16.472 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 07:06:16.473 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 07:06:16.473 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 07:06:16.473 type = bert-base-cased
2021-11-13 07:06:16.473 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 07:06:16.473 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 07:06:16.473 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 07:06:16.474 dataset_reader.to_index = 6
2021-11-13 07:06:16.474 type = from_instances
2021-11-13 07:06:16.474 Loading token dictionary from /tmp/tmpafs1y9xe/vocabulary.
2021-11-13 07:06:16.475 model.type = sentence_level_classifier
2021-11-13 07:06:16.475 model.embedder.type = ref
2021-11-13 07:06:16.475 model.embedder.type = basic
2021-11-13 07:06:16.476 model.embedder.token_embedders.type = ref
2021-11-13 07:06:16.476 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 07:06:16.477 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 07:06:16.477 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-13 07:06:16.477 type = bert-base-cased
2021-11-13 07:06:16.477 model.embedder.token_embedders.tokens.max_length = None
2021-11-13 07:06:16.477 model.embedder.token_embedders.tokens.sub_module = None
2021-11-13 07:06:16.477 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-13 07:06:16.477 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-13 07:06:16.477 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-13 07:06:16.477 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-13 07:06:16.477 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-13 07:06:16.477 model.embedder.token_embedders.tokens.load_weights = True
2021-11-13 07:06:16.477 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-13 07:06:16.477 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-13 07:06:16.477 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-13 07:06:16.526 model.encoder.type = bert_pooler
2021-11-13 07:06:16.526 model.encoder.type = bert_pooler
2021-11-13 07:06:16.527 model.encoder.pretrained_model = bert-base-cased
2021-11-13 07:06:16.527 type = bert-base-cased
2021-11-13 07:06:16.527 model.encoder.override_weights_file = None
2021-11-13 07:06:16.527 model.encoder.override_weights_strip_prefix = None
2021-11-13 07:06:16.527 model.encoder.load_weights = True
2021-11-13 07:06:16.527 model.encoder.requires_grad = True
2021-11-13 07:06:16.527 model.encoder.dropout = 0.0
2021-11-13 07:06:16.527 model.encoder.transformer_kwargs = None
2021-11-13 07:06:16.724 removing temporary unarchived model dir at /tmp/tmpafs1y9xe
2021-11-13 10:59:48.710 Plugin allennlp_models available
2021-11-13 10:59:48.717 Plugin allennlp_server available
2021-11-13 10:59:48.718 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-13 10:59:48.718 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp9326yjvg
2021-11-13 10:59:50.814 dataset_reader.type = example_reader
2021-11-13 10:59:50.814 dataset_reader.max_instances = None
2021-11-13 10:59:50.814 dataset_reader.manual_distributed_sharding = False
2021-11-13 10:59:50.814 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 10:59:50.814 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 10:59:50.815 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 10:59:50.815 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 10:59:50.815 type = bert-base-cased
2021-11-13 10:59:50.815 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 10:59:50.815 dataset_reader.tokenizer.max_length = 128
2021-11-13 10:59:50.815 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 10:59:50.816 dataset_reader.text_token_indexers.type = ref
2021-11-13 10:59:50.817 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 10:59:50.817 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 10:59:50.818 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 10:59:50.818 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 10:59:50.818 type = bert-base-cased
2021-11-13 10:59:50.818 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 10:59:50.818 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 10:59:50.818 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 10:59:50.819 dataset_reader.to_index = 6
2021-11-13 10:59:50.819 dataset_reader.type = example_reader
2021-11-13 10:59:50.819 dataset_reader.max_instances = None
2021-11-13 10:59:50.819 dataset_reader.manual_distributed_sharding = False
2021-11-13 10:59:50.819 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 10:59:50.819 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 10:59:50.819 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 10:59:50.819 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 10:59:50.820 type = bert-base-cased
2021-11-13 10:59:50.820 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 10:59:50.820 dataset_reader.tokenizer.max_length = 128
2021-11-13 10:59:50.820 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 10:59:50.821 dataset_reader.text_token_indexers.type = ref
2021-11-13 10:59:50.821 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 10:59:50.821 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 10:59:50.821 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 10:59:50.821 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 10:59:50.822 type = bert-base-cased
2021-11-13 10:59:50.822 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 10:59:50.822 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 10:59:50.822 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 10:59:50.822 dataset_reader.to_index = 6
2021-11-13 10:59:50.823 type = from_instances
2021-11-13 10:59:50.823 Loading token dictionary from /tmp/tmp9326yjvg/vocabulary.
2021-11-13 10:59:50.823 model.type = sentence_level_classifier
2021-11-13 10:59:50.824 model.embedder.type = ref
2021-11-13 10:59:50.824 model.embedder.type = basic
2021-11-13 10:59:50.824 model.embedder.token_embedders.type = ref
2021-11-13 10:59:50.825 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 10:59:50.825 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 10:59:50.825 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-13 10:59:50.825 type = bert-base-cased
2021-11-13 10:59:50.825 model.embedder.token_embedders.tokens.max_length = None
2021-11-13 10:59:50.826 model.embedder.token_embedders.tokens.sub_module = None
2021-11-13 10:59:50.826 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-13 10:59:50.826 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-13 10:59:50.826 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-13 10:59:50.826 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-13 10:59:50.826 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-13 10:59:50.826 model.embedder.token_embedders.tokens.load_weights = True
2021-11-13 10:59:50.826 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-13 10:59:50.826 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-13 10:59:50.826 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-13 10:59:50.867 model.encoder.type = bert_pooler
2021-11-13 10:59:50.867 model.encoder.type = bert_pooler
2021-11-13 10:59:50.868 model.encoder.pretrained_model = bert-base-cased
2021-11-13 10:59:50.868 type = bert-base-cased
2021-11-13 10:59:50.868 model.encoder.override_weights_file = None
2021-11-13 10:59:50.868 model.encoder.override_weights_strip_prefix = None
2021-11-13 10:59:50.868 model.encoder.load_weights = True
2021-11-13 10:59:50.868 model.encoder.requires_grad = True
2021-11-13 10:59:50.868 model.encoder.dropout = 0.0
2021-11-13 10:59:50.868 model.encoder.transformer_kwargs = None
2021-11-13 10:59:51.067 removing temporary unarchived model dir at /tmp/tmp9326yjvg
2021-11-13 11:01:05.528 Plugin allennlp_models available
2021-11-13 11:01:05.537 Plugin allennlp_server available
2021-11-13 11:01:05.537 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-13 11:01:05.538 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpbyrk220t
2021-11-13 11:01:07.634 dataset_reader.type = example_reader
2021-11-13 11:01:07.634 dataset_reader.max_instances = None
2021-11-13 11:01:07.635 dataset_reader.manual_distributed_sharding = False
2021-11-13 11:01:07.635 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 11:01:07.635 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 11:01:07.635 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 11:01:07.635 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 11:01:07.635 type = bert-base-cased
2021-11-13 11:01:07.635 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 11:01:07.635 dataset_reader.tokenizer.max_length = 128
2021-11-13 11:01:07.635 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 11:01:07.636 dataset_reader.text_token_indexers.type = ref
2021-11-13 11:01:07.637 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 11:01:07.637 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 11:01:07.638 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 11:01:07.638 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 11:01:07.638 type = bert-base-cased
2021-11-13 11:01:07.638 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 11:01:07.638 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 11:01:07.638 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 11:01:07.639 dataset_reader.to_index = 6
2021-11-13 11:01:07.639 dataset_reader.type = example_reader
2021-11-13 11:01:07.639 dataset_reader.max_instances = None
2021-11-13 11:01:07.639 dataset_reader.manual_distributed_sharding = False
2021-11-13 11:01:07.639 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 11:01:07.639 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 11:01:07.639 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 11:01:07.640 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 11:01:07.640 type = bert-base-cased
2021-11-13 11:01:07.640 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 11:01:07.640 dataset_reader.tokenizer.max_length = 128
2021-11-13 11:01:07.640 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 11:01:07.641 dataset_reader.text_token_indexers.type = ref
2021-11-13 11:01:07.643 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 11:01:07.644 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 11:01:07.644 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 11:01:07.644 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 11:01:07.645 type = bert-base-cased
2021-11-13 11:01:07.645 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 11:01:07.645 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 11:01:07.646 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 11:01:07.647 dataset_reader.to_index = 6
2021-11-13 11:01:07.648 type = from_instances
2021-11-13 11:01:07.648 Loading token dictionary from /tmp/tmpbyrk220t/vocabulary.
2021-11-13 11:01:07.649 model.type = sentence_level_classifier
2021-11-13 11:01:07.650 model.embedder.type = ref
2021-11-13 11:01:07.651 model.embedder.type = basic
2021-11-13 11:01:07.652 model.embedder.token_embedders.type = ref
2021-11-13 11:01:07.654 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 11:01:07.654 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 11:01:07.655 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-13 11:01:07.655 type = bert-base-cased
2021-11-13 11:01:07.655 model.embedder.token_embedders.tokens.max_length = None
2021-11-13 11:01:07.656 model.embedder.token_embedders.tokens.sub_module = None
2021-11-13 11:01:07.656 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-13 11:01:07.656 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-13 11:01:07.656 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-13 11:01:07.656 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-13 11:01:07.657 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-13 11:01:07.657 model.embedder.token_embedders.tokens.load_weights = True
2021-11-13 11:01:07.657 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-13 11:01:07.657 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-13 11:01:07.657 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-13 11:01:07.707 model.encoder.type = bert_pooler
2021-11-13 11:01:07.707 model.encoder.type = bert_pooler
2021-11-13 11:01:07.707 model.encoder.pretrained_model = bert-base-cased
2021-11-13 11:01:07.707 type = bert-base-cased
2021-11-13 11:01:07.708 model.encoder.override_weights_file = None
2021-11-13 11:01:07.708 model.encoder.override_weights_strip_prefix = None
2021-11-13 11:01:07.708 model.encoder.load_weights = True
2021-11-13 11:01:07.708 model.encoder.requires_grad = True
2021-11-13 11:01:07.708 model.encoder.dropout = 0.0
2021-11-13 11:01:07.708 model.encoder.transformer_kwargs = None
2021-11-13 11:01:07.903 removing temporary unarchived model dir at /tmp/tmpbyrk220t
2021-11-13 11:23:32.105 Plugin allennlp_models available
2021-11-13 11:23:32.132 Plugin allennlp_server available
2021-11-13 11:23:32.133 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-13 11:23:32.133 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpfglrfo2_
2021-11-13 11:23:34.243 dataset_reader.type = example_reader
2021-11-13 11:23:34.243 dataset_reader.max_instances = None
2021-11-13 11:23:34.243 dataset_reader.manual_distributed_sharding = False
2021-11-13 11:23:34.243 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 11:23:34.243 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 11:23:34.244 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 11:23:34.244 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 11:23:34.244 type = bert-base-cased
2021-11-13 11:23:34.244 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 11:23:34.244 dataset_reader.tokenizer.max_length = 128
2021-11-13 11:23:34.244 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 11:23:34.246 dataset_reader.text_token_indexers.type = ref
2021-11-13 11:23:34.247 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 11:23:34.247 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 11:23:34.247 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 11:23:34.247 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 11:23:34.247 type = bert-base-cased
2021-11-13 11:23:34.247 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 11:23:34.247 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 11:23:34.247 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 11:23:34.248 dataset_reader.to_index = 6
2021-11-13 11:23:34.248 dataset_reader.type = example_reader
2021-11-13 11:23:34.248 dataset_reader.max_instances = None
2021-11-13 11:23:34.249 dataset_reader.manual_distributed_sharding = False
2021-11-13 11:23:34.249 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 11:23:34.249 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 11:23:34.249 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 11:23:34.249 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 11:23:34.249 type = bert-base-cased
2021-11-13 11:23:34.249 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 11:23:34.249 dataset_reader.tokenizer.max_length = 128
2021-11-13 11:23:34.249 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 11:23:34.250 dataset_reader.text_token_indexers.type = ref
2021-11-13 11:23:34.251 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 11:23:34.251 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 11:23:34.251 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 11:23:34.251 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 11:23:34.251 type = bert-base-cased
2021-11-13 11:23:34.251 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 11:23:34.251 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 11:23:34.251 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 11:23:34.252 dataset_reader.to_index = 6
2021-11-13 11:23:34.252 type = from_instances
2021-11-13 11:23:34.252 Loading token dictionary from /tmp/tmpfglrfo2_/vocabulary.
2021-11-13 11:23:34.253 model.type = sentence_level_classifier
2021-11-13 11:23:34.253 model.embedder.type = ref
2021-11-13 11:23:34.254 model.embedder.type = basic
2021-11-13 11:23:34.254 model.embedder.token_embedders.type = ref
2021-11-13 11:23:34.255 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 11:23:34.255 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 11:23:34.255 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-13 11:23:34.255 type = bert-base-cased
2021-11-13 11:23:34.255 model.embedder.token_embedders.tokens.max_length = None
2021-11-13 11:23:34.255 model.embedder.token_embedders.tokens.sub_module = None
2021-11-13 11:23:34.255 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-13 11:23:34.256 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-13 11:23:34.256 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-13 11:23:34.256 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-13 11:23:34.256 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-13 11:23:34.256 model.embedder.token_embedders.tokens.load_weights = True
2021-11-13 11:23:34.256 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-13 11:23:34.256 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-13 11:23:34.256 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-13 11:23:34.305 model.encoder.type = bert_pooler
2021-11-13 11:23:34.306 model.encoder.type = bert_pooler
2021-11-13 11:23:34.306 model.encoder.pretrained_model = bert-base-cased
2021-11-13 11:23:34.306 type = bert-base-cased
2021-11-13 11:23:34.306 model.encoder.override_weights_file = None
2021-11-13 11:23:34.306 model.encoder.override_weights_strip_prefix = None
2021-11-13 11:23:34.306 model.encoder.load_weights = True
2021-11-13 11:23:34.306 model.encoder.requires_grad = True
2021-11-13 11:23:34.306 model.encoder.dropout = 0.0
2021-11-13 11:23:34.306 model.encoder.transformer_kwargs = None
2021-11-13 11:23:34.502 removing temporary unarchived model dir at /tmp/tmpfglrfo2_
2021-11-13 11:44:47.259 Plugin allennlp_models available
2021-11-13 11:44:47.267 Plugin allennlp_server available
2021-11-13 11:44:47.267 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-13 11:44:47.268 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpwlk8mlb6
2021-11-13 11:44:49.430 dataset_reader.type = example_reader
2021-11-13 11:44:49.430 dataset_reader.max_instances = None
2021-11-13 11:44:49.430 dataset_reader.manual_distributed_sharding = False
2021-11-13 11:44:49.430 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 11:44:49.431 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 11:44:49.431 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 11:44:49.431 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 11:44:49.431 type = bert-base-cased
2021-11-13 11:44:49.431 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 11:44:49.431 dataset_reader.tokenizer.max_length = 128
2021-11-13 11:44:49.431 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 11:44:49.432 dataset_reader.text_token_indexers.type = ref
2021-11-13 11:44:49.433 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 11:44:49.433 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 11:44:49.433 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 11:44:49.433 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 11:44:49.434 type = bert-base-cased
2021-11-13 11:44:49.434 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 11:44:49.434 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 11:44:49.434 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 11:44:49.435 dataset_reader.to_index = 6
2021-11-13 11:44:49.435 dataset_reader.type = example_reader
2021-11-13 11:44:49.435 dataset_reader.max_instances = None
2021-11-13 11:44:49.435 dataset_reader.manual_distributed_sharding = False
2021-11-13 11:44:49.435 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 11:44:49.435 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 11:44:49.435 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 11:44:49.435 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 11:44:49.436 type = bert-base-cased
2021-11-13 11:44:49.436 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 11:44:49.436 dataset_reader.tokenizer.max_length = 128
2021-11-13 11:44:49.436 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 11:44:49.436 dataset_reader.text_token_indexers.type = ref
2021-11-13 11:44:49.437 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 11:44:49.437 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 11:44:49.437 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 11:44:49.437 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 11:44:49.437 type = bert-base-cased
2021-11-13 11:44:49.438 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 11:44:49.438 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 11:44:49.438 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 11:44:49.439 dataset_reader.to_index = 6
2021-11-13 11:44:49.439 type = from_instances
2021-11-13 11:44:49.439 Loading token dictionary from /tmp/tmpwlk8mlb6/vocabulary.
2021-11-13 11:44:49.439 model.type = sentence_level_classifier
2021-11-13 11:44:49.440 model.embedder.type = ref
2021-11-13 11:44:49.440 model.embedder.type = basic
2021-11-13 11:44:49.440 model.embedder.token_embedders.type = ref
2021-11-13 11:44:49.442 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 11:44:49.442 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 11:44:49.443 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-13 11:44:49.443 type = bert-base-cased
2021-11-13 11:44:49.443 model.embedder.token_embedders.tokens.max_length = None
2021-11-13 11:44:49.443 model.embedder.token_embedders.tokens.sub_module = None
2021-11-13 11:44:49.443 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-13 11:44:49.443 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-13 11:44:49.443 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-13 11:44:49.443 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-13 11:44:49.443 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-13 11:44:49.443 model.embedder.token_embedders.tokens.load_weights = True
2021-11-13 11:44:49.443 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-13 11:44:49.443 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-13 11:44:49.443 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-13 11:44:49.492 model.encoder.type = bert_pooler
2021-11-13 11:44:49.492 model.encoder.type = bert_pooler
2021-11-13 11:44:49.492 model.encoder.pretrained_model = bert-base-cased
2021-11-13 11:44:49.492 type = bert-base-cased
2021-11-13 11:44:49.492 model.encoder.override_weights_file = None
2021-11-13 11:44:49.492 model.encoder.override_weights_strip_prefix = None
2021-11-13 11:44:49.492 model.encoder.load_weights = True
2021-11-13 11:44:49.492 model.encoder.requires_grad = True
2021-11-13 11:44:49.492 model.encoder.dropout = 0.0
2021-11-13 11:44:49.492 model.encoder.transformer_kwargs = None
2021-11-13 11:44:49.687 removing temporary unarchived model dir at /tmp/tmpwlk8mlb6
2021-11-13 11:45:25.446 Plugin allennlp_models available
2021-11-13 11:45:25.453 Plugin allennlp_server available
2021-11-13 11:45:25.453 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-13 11:45:25.454 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp07vu0az6
2021-11-13 11:45:27.552 dataset_reader.type = example_reader
2021-11-13 11:45:27.553 dataset_reader.max_instances = None
2021-11-13 11:45:27.553 dataset_reader.manual_distributed_sharding = False
2021-11-13 11:45:27.553 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 11:45:27.553 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 11:45:27.553 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 11:45:27.553 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 11:45:27.553 type = bert-base-cased
2021-11-13 11:45:27.553 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 11:45:27.553 dataset_reader.tokenizer.max_length = 128
2021-11-13 11:45:27.553 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 11:45:27.555 dataset_reader.text_token_indexers.type = ref
2021-11-13 11:45:27.556 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 11:45:27.556 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 11:45:27.556 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 11:45:27.556 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 11:45:27.556 type = bert-base-cased
2021-11-13 11:45:27.556 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 11:45:27.556 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 11:45:27.556 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 11:45:27.557 dataset_reader.to_index = 6
2021-11-13 11:45:27.557 dataset_reader.type = example_reader
2021-11-13 11:45:27.557 dataset_reader.max_instances = None
2021-11-13 11:45:27.557 dataset_reader.manual_distributed_sharding = False
2021-11-13 11:45:27.558 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 11:45:27.558 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 11:45:27.558 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 11:45:27.558 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 11:45:27.558 type = bert-base-cased
2021-11-13 11:45:27.558 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 11:45:27.558 dataset_reader.tokenizer.max_length = 128
2021-11-13 11:45:27.558 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 11:45:27.559 dataset_reader.text_token_indexers.type = ref
2021-11-13 11:45:27.560 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 11:45:27.560 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 11:45:27.560 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 11:45:27.560 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 11:45:27.560 type = bert-base-cased
2021-11-13 11:45:27.560 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 11:45:27.560 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 11:45:27.560 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 11:45:27.561 dataset_reader.to_index = 6
2021-11-13 11:45:27.561 type = from_instances
2021-11-13 11:45:27.561 Loading token dictionary from /tmp/tmp07vu0az6/vocabulary.
2021-11-13 11:45:27.562 model.type = sentence_level_classifier
2021-11-13 11:45:27.562 model.embedder.type = ref
2021-11-13 11:45:27.563 model.embedder.type = basic
2021-11-13 11:45:27.563 model.embedder.token_embedders.type = ref
2021-11-13 11:45:27.564 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 11:45:27.564 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 11:45:27.564 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-13 11:45:27.564 type = bert-base-cased
2021-11-13 11:45:27.564 model.embedder.token_embedders.tokens.max_length = None
2021-11-13 11:45:27.564 model.embedder.token_embedders.tokens.sub_module = None
2021-11-13 11:45:27.564 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-13 11:45:27.564 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-13 11:45:27.564 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-13 11:45:27.564 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-13 11:45:27.564 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-13 11:45:27.564 model.embedder.token_embedders.tokens.load_weights = True
2021-11-13 11:45:27.564 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-13 11:45:27.564 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-13 11:45:27.564 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-13 11:45:27.610 model.encoder.type = bert_pooler
2021-11-13 11:45:27.610 model.encoder.type = bert_pooler
2021-11-13 11:45:27.610 model.encoder.pretrained_model = bert-base-cased
2021-11-13 11:45:27.611 type = bert-base-cased
2021-11-13 11:45:27.611 model.encoder.override_weights_file = None
2021-11-13 11:45:27.611 model.encoder.override_weights_strip_prefix = None
2021-11-13 11:45:27.611 model.encoder.load_weights = True
2021-11-13 11:45:27.611 model.encoder.requires_grad = True
2021-11-13 11:45:27.611 model.encoder.dropout = 0.0
2021-11-13 11:45:27.611 model.encoder.transformer_kwargs = None
2021-11-13 11:45:27.798 removing temporary unarchived model dir at /tmp/tmp07vu0az6
2021-11-13 11:45:31.050 Plugin allennlp_models available
2021-11-13 11:45:31.059 Plugin allennlp_server available
2021-11-13 11:45:31.060 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-13 11:45:31.060 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp3h5foqqp
2021-11-13 11:45:33.158 dataset_reader.type = example_reader
2021-11-13 11:45:33.158 dataset_reader.max_instances = None
2021-11-13 11:45:33.158 dataset_reader.manual_distributed_sharding = False
2021-11-13 11:45:33.158 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 11:45:33.158 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 11:45:33.158 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 11:45:33.158 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 11:45:33.159 type = bert-base-cased
2021-11-13 11:45:33.159 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 11:45:33.159 dataset_reader.tokenizer.max_length = 128
2021-11-13 11:45:33.159 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 11:45:33.160 dataset_reader.text_token_indexers.type = ref
2021-11-13 11:45:33.161 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 11:45:33.161 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 11:45:33.161 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 11:45:33.161 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 11:45:33.161 type = bert-base-cased
2021-11-13 11:45:33.161 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 11:45:33.161 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 11:45:33.161 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 11:45:33.162 dataset_reader.to_index = 6
2021-11-13 11:45:33.162 dataset_reader.type = example_reader
2021-11-13 11:45:33.163 dataset_reader.max_instances = None
2021-11-13 11:45:33.163 dataset_reader.manual_distributed_sharding = False
2021-11-13 11:45:33.163 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 11:45:33.163 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 11:45:33.163 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 11:45:33.163 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 11:45:33.163 type = bert-base-cased
2021-11-13 11:45:33.164 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 11:45:33.164 dataset_reader.tokenizer.max_length = 128
2021-11-13 11:45:33.164 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 11:45:33.164 dataset_reader.text_token_indexers.type = ref
2021-11-13 11:45:33.165 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 11:45:33.166 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 11:45:33.166 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 11:45:33.166 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 11:45:33.166 type = bert-base-cased
2021-11-13 11:45:33.166 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 11:45:33.166 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 11:45:33.166 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 11:45:33.167 dataset_reader.to_index = 6
2021-11-13 11:45:33.167 type = from_instances
2021-11-13 11:45:33.167 Loading token dictionary from /tmp/tmp3h5foqqp/vocabulary.
2021-11-13 11:45:33.168 model.type = sentence_level_classifier
2021-11-13 11:45:33.168 model.embedder.type = ref
2021-11-13 11:45:33.169 model.embedder.type = basic
2021-11-13 11:45:33.169 model.embedder.token_embedders.type = ref
2021-11-13 11:45:33.170 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 11:45:33.170 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 11:45:33.170 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-13 11:45:33.170 type = bert-base-cased
2021-11-13 11:45:33.170 model.embedder.token_embedders.tokens.max_length = None
2021-11-13 11:45:33.170 model.embedder.token_embedders.tokens.sub_module = None
2021-11-13 11:45:33.170 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-13 11:45:33.171 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-13 11:45:33.171 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-13 11:45:33.171 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-13 11:45:33.171 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-13 11:45:33.171 model.embedder.token_embedders.tokens.load_weights = True
2021-11-13 11:45:33.171 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-13 11:45:33.171 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-13 11:45:33.171 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-13 11:45:33.220 model.encoder.type = bert_pooler
2021-11-13 11:45:33.220 model.encoder.type = bert_pooler
2021-11-13 11:45:33.220 model.encoder.pretrained_model = bert-base-cased
2021-11-13 11:45:33.220 type = bert-base-cased
2021-11-13 11:45:33.221 model.encoder.override_weights_file = None
2021-11-13 11:45:33.221 model.encoder.override_weights_strip_prefix = None
2021-11-13 11:45:33.221 model.encoder.load_weights = True
2021-11-13 11:45:33.221 model.encoder.requires_grad = True
2021-11-13 11:45:33.221 model.encoder.dropout = 0.0
2021-11-13 11:45:33.221 model.encoder.transformer_kwargs = None
2021-11-13 11:45:33.416 removing temporary unarchived model dir at /tmp/tmp3h5foqqp
2021-11-13 12:07:40.942 Plugin allennlp_models available
2021-11-13 12:07:40.949 Plugin allennlp_server available
2021-11-13 12:07:40.949 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-13 12:07:40.950 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpri6ex6fp
2021-11-13 12:07:43.043 dataset_reader.type = example_reader
2021-11-13 12:07:43.044 dataset_reader.max_instances = None
2021-11-13 12:07:43.044 dataset_reader.manual_distributed_sharding = False
2021-11-13 12:07:43.044 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 12:07:43.044 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 12:07:43.044 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 12:07:43.044 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 12:07:43.044 type = bert-base-cased
2021-11-13 12:07:43.044 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 12:07:43.044 dataset_reader.tokenizer.max_length = 128
2021-11-13 12:07:43.044 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 12:07:43.045 dataset_reader.text_token_indexers.type = ref
2021-11-13 12:07:43.046 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 12:07:43.047 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 12:07:43.047 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 12:07:43.047 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 12:07:43.047 type = bert-base-cased
2021-11-13 12:07:43.047 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 12:07:43.047 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 12:07:43.047 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 12:07:43.048 dataset_reader.to_index = 6
2021-11-13 12:07:43.048 dataset_reader.type = example_reader
2021-11-13 12:07:43.048 dataset_reader.max_instances = None
2021-11-13 12:07:43.048 dataset_reader.manual_distributed_sharding = False
2021-11-13 12:07:43.048 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 12:07:43.048 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 12:07:43.049 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 12:07:43.049 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 12:07:43.049 type = bert-base-cased
2021-11-13 12:07:43.049 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 12:07:43.049 dataset_reader.tokenizer.max_length = 128
2021-11-13 12:07:43.049 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 12:07:43.050 dataset_reader.text_token_indexers.type = ref
2021-11-13 12:07:43.050 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 12:07:43.051 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 12:07:43.051 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 12:07:43.051 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 12:07:43.051 type = bert-base-cased
2021-11-13 12:07:43.051 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 12:07:43.051 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 12:07:43.051 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 12:07:43.052 dataset_reader.to_index = 6
2021-11-13 12:07:43.052 type = from_instances
2021-11-13 12:07:43.052 Loading token dictionary from /tmp/tmpri6ex6fp/vocabulary.
2021-11-13 12:07:43.053 model.type = sentence_level_classifier
2021-11-13 12:07:43.053 model.embedder.type = ref
2021-11-13 12:07:43.054 model.embedder.type = basic
2021-11-13 12:07:43.054 model.embedder.token_embedders.type = ref
2021-11-13 12:07:43.055 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 12:07:43.055 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 12:07:43.055 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-13 12:07:43.055 type = bert-base-cased
2021-11-13 12:07:43.055 model.embedder.token_embedders.tokens.max_length = None
2021-11-13 12:07:43.055 model.embedder.token_embedders.tokens.sub_module = None
2021-11-13 12:07:43.055 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-13 12:07:43.055 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-13 12:07:43.055 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-13 12:07:43.055 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-13 12:07:43.056 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-13 12:07:43.056 model.embedder.token_embedders.tokens.load_weights = True
2021-11-13 12:07:43.056 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-13 12:07:43.056 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-13 12:07:43.056 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-13 12:07:43.101 model.encoder.type = bert_pooler
2021-11-13 12:07:43.101 model.encoder.type = bert_pooler
2021-11-13 12:07:43.101 model.encoder.pretrained_model = bert-base-cased
2021-11-13 12:07:43.101 type = bert-base-cased
2021-11-13 12:07:43.101 model.encoder.override_weights_file = None
2021-11-13 12:07:43.101 model.encoder.override_weights_strip_prefix = None
2021-11-13 12:07:43.101 model.encoder.load_weights = True
2021-11-13 12:07:43.101 model.encoder.requires_grad = True
2021-11-13 12:07:43.101 model.encoder.dropout = 0.0
2021-11-13 12:07:43.101 model.encoder.transformer_kwargs = None
2021-11-13 12:07:43.293 removing temporary unarchived model dir at /tmp/tmpri6ex6fp
2021-11-13 12:21:50.197 Plugin allennlp_models available
2021-11-13 12:21:50.207 Plugin allennlp_server available
2021-11-13 12:21:50.208 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-13 12:21:50.208 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmphnjf4o92
2021-11-13 12:21:52.320 dataset_reader.type = example_reader
2021-11-13 12:21:52.320 dataset_reader.max_instances = None
2021-11-13 12:21:52.320 dataset_reader.manual_distributed_sharding = False
2021-11-13 12:21:52.320 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 12:21:52.321 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 12:21:52.321 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 12:21:52.321 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 12:21:52.321 type = bert-base-cased
2021-11-13 12:21:52.321 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 12:21:52.321 dataset_reader.tokenizer.max_length = 128
2021-11-13 12:21:52.321 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 12:21:52.322 dataset_reader.text_token_indexers.type = ref
2021-11-13 12:21:52.324 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 12:21:52.324 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 12:21:52.324 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 12:21:52.324 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 12:21:52.324 type = bert-base-cased
2021-11-13 12:21:52.324 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 12:21:52.324 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 12:21:52.324 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 12:21:52.325 dataset_reader.to_index = 6
2021-11-13 12:21:52.325 dataset_reader.type = example_reader
2021-11-13 12:21:52.325 dataset_reader.max_instances = None
2021-11-13 12:21:52.325 dataset_reader.manual_distributed_sharding = False
2021-11-13 12:21:52.325 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 12:21:52.325 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 12:21:52.325 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 12:21:52.325 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 12:21:52.326 type = bert-base-cased
2021-11-13 12:21:52.326 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 12:21:52.326 dataset_reader.tokenizer.max_length = 128
2021-11-13 12:21:52.326 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 12:21:52.327 dataset_reader.text_token_indexers.type = ref
2021-11-13 12:21:52.327 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 12:21:52.327 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 12:21:52.327 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 12:21:52.328 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 12:21:52.328 type = bert-base-cased
2021-11-13 12:21:52.328 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 12:21:52.328 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 12:21:52.328 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 12:21:52.329 dataset_reader.to_index = 6
2021-11-13 12:21:52.329 type = from_instances
2021-11-13 12:21:52.329 Loading token dictionary from /tmp/tmphnjf4o92/vocabulary.
2021-11-13 12:21:52.329 model.type = sentence_level_classifier
2021-11-13 12:21:52.330 model.embedder.type = ref
2021-11-13 12:21:52.330 model.embedder.type = basic
2021-11-13 12:21:52.331 model.embedder.token_embedders.type = ref
2021-11-13 12:21:52.331 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 12:21:52.332 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 12:21:52.332 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-13 12:21:52.332 type = bert-base-cased
2021-11-13 12:21:52.332 model.embedder.token_embedders.tokens.max_length = None
2021-11-13 12:21:52.332 model.embedder.token_embedders.tokens.sub_module = None
2021-11-13 12:21:52.332 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-13 12:21:52.332 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-13 12:21:52.332 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-13 12:21:52.332 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-13 12:21:52.332 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-13 12:21:52.332 model.embedder.token_embedders.tokens.load_weights = True
2021-11-13 12:21:52.332 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-13 12:21:52.332 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-13 12:21:52.332 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-13 12:21:52.382 model.encoder.type = bert_pooler
2021-11-13 12:21:52.382 model.encoder.type = bert_pooler
2021-11-13 12:21:52.382 model.encoder.pretrained_model = bert-base-cased
2021-11-13 12:21:52.382 type = bert-base-cased
2021-11-13 12:21:52.382 model.encoder.override_weights_file = None
2021-11-13 12:21:52.382 model.encoder.override_weights_strip_prefix = None
2021-11-13 12:21:52.382 model.encoder.load_weights = True
2021-11-13 12:21:52.382 model.encoder.requires_grad = True
2021-11-13 12:21:52.382 model.encoder.dropout = 0.0
2021-11-13 12:21:52.382 model.encoder.transformer_kwargs = None
2021-11-13 12:21:52.578 removing temporary unarchived model dir at /tmp/tmphnjf4o92
2021-11-13 12:50:51.294 Plugin allennlp_models available
2021-11-13 12:50:51.301 Plugin allennlp_server available
2021-11-13 12:50:51.302 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-13 12:50:51.302 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpu26xbys3
2021-11-13 12:50:53.396 dataset_reader.type = example_reader
2021-11-13 12:50:53.397 dataset_reader.max_instances = None
2021-11-13 12:50:53.397 dataset_reader.manual_distributed_sharding = False
2021-11-13 12:50:53.397 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 12:50:53.397 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 12:50:53.397 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 12:50:53.397 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 12:50:53.397 type = bert-base-cased
2021-11-13 12:50:53.397 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 12:50:53.397 dataset_reader.tokenizer.max_length = 128
2021-11-13 12:50:53.397 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 12:50:53.399 dataset_reader.text_token_indexers.type = ref
2021-11-13 12:50:53.400 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 12:50:53.400 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 12:50:53.400 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 12:50:53.400 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 12:50:53.400 type = bert-base-cased
2021-11-13 12:50:53.400 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 12:50:53.400 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 12:50:53.400 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 12:50:53.401 dataset_reader.to_index = 6
2021-11-13 12:50:53.401 dataset_reader.type = example_reader
2021-11-13 12:50:53.402 dataset_reader.max_instances = None
2021-11-13 12:50:53.402 dataset_reader.manual_distributed_sharding = False
2021-11-13 12:50:53.402 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 12:50:53.402 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 12:50:53.402 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 12:50:53.402 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 12:50:53.402 type = bert-base-cased
2021-11-13 12:50:53.402 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 12:50:53.402 dataset_reader.tokenizer.max_length = 128
2021-11-13 12:50:53.402 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 12:50:53.403 dataset_reader.text_token_indexers.type = ref
2021-11-13 12:50:53.404 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 12:50:53.404 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 12:50:53.404 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 12:50:53.404 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 12:50:53.404 type = bert-base-cased
2021-11-13 12:50:53.404 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 12:50:53.404 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 12:50:53.404 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 12:50:53.405 dataset_reader.to_index = 6
2021-11-13 12:50:53.405 type = from_instances
2021-11-13 12:50:53.405 Loading token dictionary from /tmp/tmpu26xbys3/vocabulary.
2021-11-13 12:50:53.406 model.type = sentence_level_classifier
2021-11-13 12:50:53.406 model.embedder.type = ref
2021-11-13 12:50:53.407 model.embedder.type = basic
2021-11-13 12:50:53.407 model.embedder.token_embedders.type = ref
2021-11-13 12:50:53.408 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 12:50:53.408 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 12:50:53.408 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-13 12:50:53.408 type = bert-base-cased
2021-11-13 12:50:53.408 model.embedder.token_embedders.tokens.max_length = None
2021-11-13 12:50:53.408 model.embedder.token_embedders.tokens.sub_module = None
2021-11-13 12:50:53.408 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-13 12:50:53.408 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-13 12:50:53.408 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-13 12:50:53.408 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-13 12:50:53.408 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-13 12:50:53.408 model.embedder.token_embedders.tokens.load_weights = True
2021-11-13 12:50:53.408 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-13 12:50:53.408 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-13 12:50:53.409 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-13 12:50:53.459 model.encoder.type = bert_pooler
2021-11-13 12:50:53.459 model.encoder.type = bert_pooler
2021-11-13 12:50:53.459 model.encoder.pretrained_model = bert-base-cased
2021-11-13 12:50:53.460 type = bert-base-cased
2021-11-13 12:50:53.460 model.encoder.override_weights_file = None
2021-11-13 12:50:53.460 model.encoder.override_weights_strip_prefix = None
2021-11-13 12:50:53.460 model.encoder.load_weights = True
2021-11-13 12:50:53.460 model.encoder.requires_grad = True
2021-11-13 12:50:53.460 model.encoder.dropout = 0.0
2021-11-13 12:50:53.460 model.encoder.transformer_kwargs = None
2021-11-13 12:50:53.656 removing temporary unarchived model dir at /tmp/tmpu26xbys3
2021-11-13 14:02:20.624 Plugin allennlp_models available
2021-11-13 14:02:20.633 Plugin allennlp_server available
2021-11-13 14:02:20.634 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-13 14:02:20.634 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpedvo3ecp
2021-11-13 14:02:22.744 dataset_reader.type = example_reader
2021-11-13 14:02:22.744 dataset_reader.max_instances = None
2021-11-13 14:02:22.744 dataset_reader.manual_distributed_sharding = False
2021-11-13 14:02:22.744 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 14:02:22.745 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 14:02:22.745 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 14:02:22.745 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 14:02:22.745 type = bert-base-cased
2021-11-13 14:02:22.745 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 14:02:22.745 dataset_reader.tokenizer.max_length = 128
2021-11-13 14:02:22.745 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 14:02:22.746 dataset_reader.text_token_indexers.type = ref
2021-11-13 14:02:22.748 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 14:02:22.748 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 14:02:22.748 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 14:02:22.748 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 14:02:22.748 type = bert-base-cased
2021-11-13 14:02:22.748 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 14:02:22.748 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 14:02:22.748 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 14:02:22.750 dataset_reader.to_index = 6
2021-11-13 14:02:22.750 dataset_reader.type = example_reader
2021-11-13 14:02:22.751 dataset_reader.max_instances = None
2021-11-13 14:02:22.751 dataset_reader.manual_distributed_sharding = False
2021-11-13 14:02:22.751 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 14:02:22.752 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 14:02:22.752 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 14:02:22.752 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 14:02:22.753 type = bert-base-cased
2021-11-13 14:02:22.753 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 14:02:22.753 dataset_reader.tokenizer.max_length = 128
2021-11-13 14:02:22.754 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 14:02:22.755 dataset_reader.text_token_indexers.type = ref
2021-11-13 14:02:22.757 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 14:02:22.758 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 14:02:22.758 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 14:02:22.758 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 14:02:22.759 type = bert-base-cased
2021-11-13 14:02:22.759 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 14:02:22.759 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 14:02:22.759 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 14:02:22.761 dataset_reader.to_index = 6
2021-11-13 14:02:22.761 type = from_instances
2021-11-13 14:02:22.761 Loading token dictionary from /tmp/tmpedvo3ecp/vocabulary.
2021-11-13 14:02:22.762 model.type = sentence_level_classifier
2021-11-13 14:02:22.763 model.embedder.type = ref
2021-11-13 14:02:22.764 model.embedder.type = basic
2021-11-13 14:02:22.765 model.embedder.token_embedders.type = ref
2021-11-13 14:02:22.767 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 14:02:22.767 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 14:02:22.767 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-13 14:02:22.768 type = bert-base-cased
2021-11-13 14:02:22.768 model.embedder.token_embedders.tokens.max_length = None
2021-11-13 14:02:22.768 model.embedder.token_embedders.tokens.sub_module = None
2021-11-13 14:02:22.768 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-13 14:02:22.769 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-13 14:02:22.769 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-13 14:02:22.769 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-13 14:02:22.769 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-13 14:02:22.769 model.embedder.token_embedders.tokens.load_weights = True
2021-11-13 14:02:22.769 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-13 14:02:22.769 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-13 14:02:22.770 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-13 14:02:22.822 model.encoder.type = bert_pooler
2021-11-13 14:02:22.822 model.encoder.type = bert_pooler
2021-11-13 14:02:22.822 model.encoder.pretrained_model = bert-base-cased
2021-11-13 14:02:22.823 type = bert-base-cased
2021-11-13 14:02:22.823 model.encoder.override_weights_file = None
2021-11-13 14:02:22.823 model.encoder.override_weights_strip_prefix = None
2021-11-13 14:02:22.823 model.encoder.load_weights = True
2021-11-13 14:02:22.823 model.encoder.requires_grad = True
2021-11-13 14:02:22.823 model.encoder.dropout = 0.0
2021-11-13 14:02:22.823 model.encoder.transformer_kwargs = None
2021-11-13 14:02:23.015 removing temporary unarchived model dir at /tmp/tmpedvo3ecp
2021-11-13 14:02:33.057 Plugin allennlp_models available
2021-11-13 14:02:33.066 Plugin allennlp_server available
2021-11-13 14:02:33.067 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-13 14:02:33.067 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpqtfzcl9u
2021-11-13 14:02:35.170 dataset_reader.type = example_reader
2021-11-13 14:02:35.170 dataset_reader.max_instances = None
2021-11-13 14:02:35.170 dataset_reader.manual_distributed_sharding = False
2021-11-13 14:02:35.170 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 14:02:35.171 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 14:02:35.171 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 14:02:35.171 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 14:02:35.171 type = bert-base-cased
2021-11-13 14:02:35.171 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 14:02:35.171 dataset_reader.tokenizer.max_length = 128
2021-11-13 14:02:35.171 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 14:02:35.172 dataset_reader.text_token_indexers.type = ref
2021-11-13 14:02:35.173 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 14:02:35.173 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 14:02:35.173 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 14:02:35.174 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 14:02:35.174 type = bert-base-cased
2021-11-13 14:02:35.174 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 14:02:35.174 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 14:02:35.174 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 14:02:35.175 dataset_reader.to_index = 6
2021-11-13 14:02:35.175 dataset_reader.type = example_reader
2021-11-13 14:02:35.175 dataset_reader.max_instances = None
2021-11-13 14:02:35.175 dataset_reader.manual_distributed_sharding = False
2021-11-13 14:02:35.175 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 14:02:35.175 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 14:02:35.175 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 14:02:35.175 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 14:02:35.175 type = bert-base-cased
2021-11-13 14:02:35.175 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 14:02:35.175 dataset_reader.tokenizer.max_length = 128
2021-11-13 14:02:35.176 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 14:02:35.176 dataset_reader.text_token_indexers.type = ref
2021-11-13 14:02:35.177 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 14:02:35.178 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 14:02:35.178 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 14:02:35.178 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 14:02:35.178 type = bert-base-cased
2021-11-13 14:02:35.178 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 14:02:35.178 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 14:02:35.178 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 14:02:35.179 dataset_reader.to_index = 6
2021-11-13 14:02:35.179 type = from_instances
2021-11-13 14:02:35.179 Loading token dictionary from /tmp/tmpqtfzcl9u/vocabulary.
2021-11-13 14:02:35.180 model.type = sentence_level_classifier
2021-11-13 14:02:35.180 model.embedder.type = ref
2021-11-13 14:02:35.181 model.embedder.type = basic
2021-11-13 14:02:35.181 model.embedder.token_embedders.type = ref
2021-11-13 14:02:35.182 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 14:02:35.182 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 14:02:35.182 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-13 14:02:35.182 type = bert-base-cased
2021-11-13 14:02:35.182 model.embedder.token_embedders.tokens.max_length = None
2021-11-13 14:02:35.182 model.embedder.token_embedders.tokens.sub_module = None
2021-11-13 14:02:35.182 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-13 14:02:35.182 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-13 14:02:35.182 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-13 14:02:35.182 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-13 14:02:35.182 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-13 14:02:35.182 model.embedder.token_embedders.tokens.load_weights = True
2021-11-13 14:02:35.182 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-13 14:02:35.182 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-13 14:02:35.182 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-13 14:02:35.230 model.encoder.type = bert_pooler
2021-11-13 14:02:35.230 model.encoder.type = bert_pooler
2021-11-13 14:02:35.230 model.encoder.pretrained_model = bert-base-cased
2021-11-13 14:02:35.230 type = bert-base-cased
2021-11-13 14:02:35.230 model.encoder.override_weights_file = None
2021-11-13 14:02:35.230 model.encoder.override_weights_strip_prefix = None
2021-11-13 14:02:35.230 model.encoder.load_weights = True
2021-11-13 14:02:35.231 model.encoder.requires_grad = True
2021-11-13 14:02:35.231 model.encoder.dropout = 0.0
2021-11-13 14:02:35.231 model.encoder.transformer_kwargs = None
2021-11-13 14:02:35.425 removing temporary unarchived model dir at /tmp/tmpqtfzcl9u
2021-11-13 14:22:54.275 Plugin allennlp_models available
2021-11-13 14:22:54.285 Plugin allennlp_server available
2021-11-13 14:22:54.285 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-13 14:22:54.286 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpgkc130j5
2021-11-13 14:22:56.391 dataset_reader.type = example_reader
2021-11-13 14:22:56.392 dataset_reader.max_instances = None
2021-11-13 14:22:56.392 dataset_reader.manual_distributed_sharding = False
2021-11-13 14:22:56.392 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 14:22:56.392 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 14:22:56.392 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 14:22:56.392 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 14:22:56.392 type = bert-base-cased
2021-11-13 14:22:56.392 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 14:22:56.392 dataset_reader.tokenizer.max_length = 128
2021-11-13 14:22:56.392 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 14:22:56.393 dataset_reader.text_token_indexers.type = ref
2021-11-13 14:22:56.394 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 14:22:56.395 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 14:22:56.395 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 14:22:56.395 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 14:22:56.395 type = bert-base-cased
2021-11-13 14:22:56.395 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 14:22:56.395 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 14:22:56.395 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 14:22:56.396 dataset_reader.to_index = 6
2021-11-13 14:22:56.396 dataset_reader.type = example_reader
2021-11-13 14:22:56.396 dataset_reader.max_instances = None
2021-11-13 14:22:56.397 dataset_reader.manual_distributed_sharding = False
2021-11-13 14:22:56.397 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 14:22:56.397 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 14:22:56.398 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 14:22:56.398 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 14:22:56.398 type = bert-base-cased
2021-11-13 14:22:56.399 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 14:22:56.399 dataset_reader.tokenizer.max_length = 128
2021-11-13 14:22:56.399 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 14:22:56.401 dataset_reader.text_token_indexers.type = ref
2021-11-13 14:22:56.403 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 14:22:56.403 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 14:22:56.403 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 14:22:56.404 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 14:22:56.404 type = bert-base-cased
2021-11-13 14:22:56.404 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 14:22:56.405 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 14:22:56.405 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 14:22:56.406 dataset_reader.to_index = 6
2021-11-13 14:22:56.407 type = from_instances
2021-11-13 14:22:56.407 Loading token dictionary from /tmp/tmpgkc130j5/vocabulary.
2021-11-13 14:22:56.408 model.type = sentence_level_classifier
2021-11-13 14:22:56.409 model.embedder.type = ref
2021-11-13 14:22:56.410 model.embedder.type = basic
2021-11-13 14:22:56.411 model.embedder.token_embedders.type = ref
2021-11-13 14:22:56.413 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 14:22:56.413 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 14:22:56.413 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-13 14:22:56.414 type = bert-base-cased
2021-11-13 14:22:56.414 model.embedder.token_embedders.tokens.max_length = None
2021-11-13 14:22:56.414 model.embedder.token_embedders.tokens.sub_module = None
2021-11-13 14:22:56.414 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-13 14:22:56.415 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-13 14:22:56.415 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-13 14:22:56.415 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-13 14:22:56.415 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-13 14:22:56.415 model.embedder.token_embedders.tokens.load_weights = True
2021-11-13 14:22:56.415 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-13 14:22:56.416 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-13 14:22:56.416 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-13 14:22:56.461 model.encoder.type = bert_pooler
2021-11-13 14:22:56.461 model.encoder.type = bert_pooler
2021-11-13 14:22:56.462 model.encoder.pretrained_model = bert-base-cased
2021-11-13 14:22:56.462 type = bert-base-cased
2021-11-13 14:22:56.462 model.encoder.override_weights_file = None
2021-11-13 14:22:56.462 model.encoder.override_weights_strip_prefix = None
2021-11-13 14:22:56.462 model.encoder.load_weights = True
2021-11-13 14:22:56.462 model.encoder.requires_grad = True
2021-11-13 14:22:56.462 model.encoder.dropout = 0.0
2021-11-13 14:22:56.462 model.encoder.transformer_kwargs = None
2021-11-13 14:22:56.656 removing temporary unarchived model dir at /tmp/tmpgkc130j5
2021-11-13 15:57:27.454 Plugin allennlp_models available
2021-11-13 15:57:27.461 Plugin allennlp_server available
2021-11-13 15:57:27.461 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-13 15:57:27.462 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpox846cvn
2021-11-13 15:57:29.564 dataset_reader.type = example_reader
2021-11-13 15:57:29.564 dataset_reader.max_instances = None
2021-11-13 15:57:29.564 dataset_reader.manual_distributed_sharding = False
2021-11-13 15:57:29.564 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 15:57:29.564 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 15:57:29.564 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 15:57:29.564 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 15:57:29.564 type = bert-base-cased
2021-11-13 15:57:29.564 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 15:57:29.564 dataset_reader.tokenizer.max_length = 128
2021-11-13 15:57:29.564 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 15:57:29.566 dataset_reader.text_token_indexers.type = ref
2021-11-13 15:57:29.567 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 15:57:29.567 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 15:57:29.567 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 15:57:29.567 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 15:57:29.567 type = bert-base-cased
2021-11-13 15:57:29.567 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 15:57:29.567 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 15:57:29.567 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 15:57:29.568 dataset_reader.to_index = 6
2021-11-13 15:57:29.568 dataset_reader.type = example_reader
2021-11-13 15:57:29.568 dataset_reader.max_instances = None
2021-11-13 15:57:29.568 dataset_reader.manual_distributed_sharding = False
2021-11-13 15:57:29.568 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 15:57:29.569 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 15:57:29.569 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 15:57:29.569 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 15:57:29.569 type = bert-base-cased
2021-11-13 15:57:29.569 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 15:57:29.569 dataset_reader.tokenizer.max_length = 128
2021-11-13 15:57:29.569 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 15:57:29.570 dataset_reader.text_token_indexers.type = ref
2021-11-13 15:57:29.570 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 15:57:29.571 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 15:57:29.571 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 15:57:29.571 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 15:57:29.571 type = bert-base-cased
2021-11-13 15:57:29.571 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 15:57:29.571 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 15:57:29.571 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 15:57:29.572 dataset_reader.to_index = 6
2021-11-13 15:57:29.572 type = from_instances
2021-11-13 15:57:29.572 Loading token dictionary from /tmp/tmpox846cvn/vocabulary.
2021-11-13 15:57:29.573 model.type = sentence_level_classifier
2021-11-13 15:57:29.574 model.embedder.type = ref
2021-11-13 15:57:29.575 model.embedder.type = basic
2021-11-13 15:57:29.576 model.embedder.token_embedders.type = ref
2021-11-13 15:57:29.578 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 15:57:29.578 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 15:57:29.579 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-13 15:57:29.579 type = bert-base-cased
2021-11-13 15:57:29.580 model.embedder.token_embedders.tokens.max_length = None
2021-11-13 15:57:29.580 model.embedder.token_embedders.tokens.sub_module = None
2021-11-13 15:57:29.580 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-13 15:57:29.580 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-13 15:57:29.580 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-13 15:57:29.581 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-13 15:57:29.581 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-13 15:57:29.581 model.embedder.token_embedders.tokens.load_weights = True
2021-11-13 15:57:29.581 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-13 15:57:29.581 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-13 15:57:29.581 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-13 15:57:29.634 model.encoder.type = bert_pooler
2021-11-13 15:57:29.634 model.encoder.type = bert_pooler
2021-11-13 15:57:29.634 model.encoder.pretrained_model = bert-base-cased
2021-11-13 15:57:29.634 type = bert-base-cased
2021-11-13 15:57:29.634 model.encoder.override_weights_file = None
2021-11-13 15:57:29.634 model.encoder.override_weights_strip_prefix = None
2021-11-13 15:57:29.635 model.encoder.load_weights = True
2021-11-13 15:57:29.635 model.encoder.requires_grad = True
2021-11-13 15:57:29.635 model.encoder.dropout = 0.0
2021-11-13 15:57:29.635 model.encoder.transformer_kwargs = None
2021-11-13 15:57:29.827 removing temporary unarchived model dir at /tmp/tmpox846cvn
2021-11-13 15:57:34.815 Plugin allennlp_models available
2021-11-13 15:57:34.823 Plugin allennlp_server available
2021-11-13 15:57:34.823 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-13 15:57:34.824 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpm85d5rzq
2021-11-13 15:57:36.924 dataset_reader.type = example_reader
2021-11-13 15:57:36.924 dataset_reader.max_instances = None
2021-11-13 15:57:36.924 dataset_reader.manual_distributed_sharding = False
2021-11-13 15:57:36.924 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 15:57:36.924 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 15:57:36.924 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 15:57:36.924 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 15:57:36.924 type = bert-base-cased
2021-11-13 15:57:36.924 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 15:57:36.924 dataset_reader.tokenizer.max_length = 128
2021-11-13 15:57:36.924 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 15:57:36.926 dataset_reader.text_token_indexers.type = ref
2021-11-13 15:57:36.927 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 15:57:36.927 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 15:57:36.927 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 15:57:36.927 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 15:57:36.927 type = bert-base-cased
2021-11-13 15:57:36.927 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 15:57:36.927 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 15:57:36.927 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 15:57:36.928 dataset_reader.to_index = 6
2021-11-13 15:57:36.928 dataset_reader.type = example_reader
2021-11-13 15:57:36.928 dataset_reader.max_instances = None
2021-11-13 15:57:36.929 dataset_reader.manual_distributed_sharding = False
2021-11-13 15:57:36.929 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 15:57:36.929 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 15:57:36.929 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 15:57:36.929 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 15:57:36.929 type = bert-base-cased
2021-11-13 15:57:36.929 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 15:57:36.929 dataset_reader.tokenizer.max_length = 128
2021-11-13 15:57:36.929 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 15:57:36.930 dataset_reader.text_token_indexers.type = ref
2021-11-13 15:57:36.931 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 15:57:36.931 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 15:57:36.931 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 15:57:36.931 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 15:57:36.931 type = bert-base-cased
2021-11-13 15:57:36.931 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 15:57:36.931 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 15:57:36.931 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 15:57:36.932 dataset_reader.to_index = 6
2021-11-13 15:57:36.932 type = from_instances
2021-11-13 15:57:36.932 Loading token dictionary from /tmp/tmpm85d5rzq/vocabulary.
2021-11-13 15:57:36.933 model.type = sentence_level_classifier
2021-11-13 15:57:36.933 model.embedder.type = ref
2021-11-13 15:57:36.934 model.embedder.type = basic
2021-11-13 15:57:36.935 model.embedder.token_embedders.type = ref
2021-11-13 15:57:36.935 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 15:57:36.935 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 15:57:36.936 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-13 15:57:36.936 type = bert-base-cased
2021-11-13 15:57:36.936 model.embedder.token_embedders.tokens.max_length = None
2021-11-13 15:57:36.936 model.embedder.token_embedders.tokens.sub_module = None
2021-11-13 15:57:36.936 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-13 15:57:36.936 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-13 15:57:36.936 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-13 15:57:36.936 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-13 15:57:36.936 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-13 15:57:36.936 model.embedder.token_embedders.tokens.load_weights = True
2021-11-13 15:57:36.936 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-13 15:57:36.936 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-13 15:57:36.936 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-13 15:57:36.985 model.encoder.type = bert_pooler
2021-11-13 15:57:36.986 model.encoder.type = bert_pooler
2021-11-13 15:57:36.986 model.encoder.pretrained_model = bert-base-cased
2021-11-13 15:57:36.986 type = bert-base-cased
2021-11-13 15:57:36.986 model.encoder.override_weights_file = None
2021-11-13 15:57:36.986 model.encoder.override_weights_strip_prefix = None
2021-11-13 15:57:36.986 model.encoder.load_weights = True
2021-11-13 15:57:36.986 model.encoder.requires_grad = True
2021-11-13 15:57:36.986 model.encoder.dropout = 0.0
2021-11-13 15:57:36.986 model.encoder.transformer_kwargs = None
2021-11-13 15:57:37.181 removing temporary unarchived model dir at /tmp/tmpm85d5rzq
2021-11-13 15:57:41.095 Plugin allennlp_models available
2021-11-13 15:57:41.101 Plugin allennlp_server available
2021-11-13 15:57:41.102 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-13 15:57:41.102 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpc2ycrfxu
2021-11-13 15:57:43.220 dataset_reader.type = example_reader
2021-11-13 15:57:43.220 dataset_reader.max_instances = None
2021-11-13 15:57:43.220 dataset_reader.manual_distributed_sharding = False
2021-11-13 15:57:43.220 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 15:57:43.220 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 15:57:43.221 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 15:57:43.221 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 15:57:43.221 type = bert-base-cased
2021-11-13 15:57:43.221 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 15:57:43.221 dataset_reader.tokenizer.max_length = 128
2021-11-13 15:57:43.221 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 15:57:43.222 dataset_reader.text_token_indexers.type = ref
2021-11-13 15:57:43.223 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 15:57:43.224 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 15:57:43.224 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 15:57:43.224 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 15:57:43.224 type = bert-base-cased
2021-11-13 15:57:43.224 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 15:57:43.224 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 15:57:43.224 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 15:57:43.225 dataset_reader.to_index = 6
2021-11-13 15:57:43.225 dataset_reader.type = example_reader
2021-11-13 15:57:43.225 dataset_reader.max_instances = None
2021-11-13 15:57:43.225 dataset_reader.manual_distributed_sharding = False
2021-11-13 15:57:43.225 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 15:57:43.225 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 15:57:43.225 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 15:57:43.225 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 15:57:43.226 type = bert-base-cased
2021-11-13 15:57:43.226 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 15:57:43.226 dataset_reader.tokenizer.max_length = 128
2021-11-13 15:57:43.226 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 15:57:43.227 dataset_reader.text_token_indexers.type = ref
2021-11-13 15:57:43.227 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 15:57:43.227 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 15:57:43.227 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 15:57:43.228 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 15:57:43.228 type = bert-base-cased
2021-11-13 15:57:43.228 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 15:57:43.228 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 15:57:43.228 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 15:57:43.229 dataset_reader.to_index = 6
2021-11-13 15:57:43.229 type = from_instances
2021-11-13 15:57:43.229 Loading token dictionary from /tmp/tmpc2ycrfxu/vocabulary.
2021-11-13 15:57:43.229 model.type = sentence_level_classifier
2021-11-13 15:57:43.230 model.embedder.type = ref
2021-11-13 15:57:43.230 model.embedder.type = basic
2021-11-13 15:57:43.231 model.embedder.token_embedders.type = ref
2021-11-13 15:57:43.232 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 15:57:43.232 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 15:57:43.232 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-13 15:57:43.232 type = bert-base-cased
2021-11-13 15:57:43.232 model.embedder.token_embedders.tokens.max_length = None
2021-11-13 15:57:43.233 model.embedder.token_embedders.tokens.sub_module = None
2021-11-13 15:57:43.233 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-13 15:57:43.233 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-13 15:57:43.233 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-13 15:57:43.233 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-13 15:57:43.233 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-13 15:57:43.233 model.embedder.token_embedders.tokens.load_weights = True
2021-11-13 15:57:43.233 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-13 15:57:43.233 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-13 15:57:43.233 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-13 15:57:43.274 model.encoder.type = bert_pooler
2021-11-13 15:57:43.274 model.encoder.type = bert_pooler
2021-11-13 15:57:43.275 model.encoder.pretrained_model = bert-base-cased
2021-11-13 15:57:43.275 type = bert-base-cased
2021-11-13 15:57:43.275 model.encoder.override_weights_file = None
2021-11-13 15:57:43.275 model.encoder.override_weights_strip_prefix = None
2021-11-13 15:57:43.275 model.encoder.load_weights = True
2021-11-13 15:57:43.275 model.encoder.requires_grad = True
2021-11-13 15:57:43.275 model.encoder.dropout = 0.0
2021-11-13 15:57:43.275 model.encoder.transformer_kwargs = None
2021-11-13 15:57:43.464 removing temporary unarchived model dir at /tmp/tmpc2ycrfxu
2021-11-13 15:57:56.123 Plugin allennlp_models available
2021-11-13 15:57:56.133 Plugin allennlp_server available
2021-11-13 15:57:56.133 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-13 15:57:56.134 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpmr5zitk0
2021-11-13 15:57:58.247 dataset_reader.type = example_reader
2021-11-13 15:57:58.247 dataset_reader.max_instances = None
2021-11-13 15:57:58.247 dataset_reader.manual_distributed_sharding = False
2021-11-13 15:57:58.247 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 15:57:58.247 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 15:57:58.248 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 15:57:58.248 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 15:57:58.248 type = bert-base-cased
2021-11-13 15:57:58.248 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 15:57:58.248 dataset_reader.tokenizer.max_length = 128
2021-11-13 15:57:58.248 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 15:57:58.249 dataset_reader.text_token_indexers.type = ref
2021-11-13 15:57:58.250 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 15:57:58.250 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 15:57:58.250 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 15:57:58.250 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 15:57:58.251 type = bert-base-cased
2021-11-13 15:57:58.251 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 15:57:58.251 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 15:57:58.251 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 15:57:58.252 dataset_reader.to_index = 6
2021-11-13 15:57:58.252 dataset_reader.type = example_reader
2021-11-13 15:57:58.252 dataset_reader.max_instances = None
2021-11-13 15:57:58.252 dataset_reader.manual_distributed_sharding = False
2021-11-13 15:57:58.252 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 15:57:58.252 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 15:57:58.252 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 15:57:58.252 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 15:57:58.252 type = bert-base-cased
2021-11-13 15:57:58.252 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 15:57:58.253 dataset_reader.tokenizer.max_length = 128
2021-11-13 15:57:58.253 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 15:57:58.254 dataset_reader.text_token_indexers.type = ref
2021-11-13 15:57:58.254 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 15:57:58.254 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 15:57:58.255 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 15:57:58.255 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 15:57:58.255 type = bert-base-cased
2021-11-13 15:57:58.255 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 15:57:58.255 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 15:57:58.255 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 15:57:58.256 dataset_reader.to_index = 6
2021-11-13 15:57:58.256 type = from_instances
2021-11-13 15:57:58.256 Loading token dictionary from /tmp/tmpmr5zitk0/vocabulary.
2021-11-13 15:57:58.257 model.type = sentence_level_classifier
2021-11-13 15:57:58.258 model.embedder.type = ref
2021-11-13 15:57:58.259 model.embedder.type = basic
2021-11-13 15:57:58.260 model.embedder.token_embedders.type = ref
2021-11-13 15:57:58.262 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 15:57:58.262 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 15:57:58.263 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-13 15:57:58.263 type = bert-base-cased
2021-11-13 15:57:58.263 model.embedder.token_embedders.tokens.max_length = None
2021-11-13 15:57:58.264 model.embedder.token_embedders.tokens.sub_module = None
2021-11-13 15:57:58.264 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-13 15:57:58.264 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-13 15:57:58.264 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-13 15:57:58.264 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-13 15:57:58.265 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-13 15:57:58.265 model.embedder.token_embedders.tokens.load_weights = True
2021-11-13 15:57:58.265 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-13 15:57:58.265 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-13 15:57:58.265 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-13 15:57:58.309 model.encoder.type = bert_pooler
2021-11-13 15:57:58.309 model.encoder.type = bert_pooler
2021-11-13 15:57:58.310 model.encoder.pretrained_model = bert-base-cased
2021-11-13 15:57:58.310 type = bert-base-cased
2021-11-13 15:57:58.310 model.encoder.override_weights_file = None
2021-11-13 15:57:58.310 model.encoder.override_weights_strip_prefix = None
2021-11-13 15:57:58.310 model.encoder.load_weights = True
2021-11-13 15:57:58.310 model.encoder.requires_grad = True
2021-11-13 15:57:58.310 model.encoder.dropout = 0.0
2021-11-13 15:57:58.310 model.encoder.transformer_kwargs = None
2021-11-13 15:57:58.507 removing temporary unarchived model dir at /tmp/tmpmr5zitk0
2021-11-13 15:59:01.021 Plugin allennlp_models available
2021-11-13 15:59:01.031 Plugin allennlp_server available
2021-11-13 15:59:01.032 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-13 15:59:01.032 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmper49nzmk
2021-11-13 15:59:03.148 dataset_reader.type = example_reader
2021-11-13 15:59:03.148 dataset_reader.max_instances = None
2021-11-13 15:59:03.148 dataset_reader.manual_distributed_sharding = False
2021-11-13 15:59:03.148 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 15:59:03.148 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 15:59:03.149 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 15:59:03.149 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 15:59:03.149 type = bert-base-cased
2021-11-13 15:59:03.149 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 15:59:03.149 dataset_reader.tokenizer.max_length = 128
2021-11-13 15:59:03.149 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 15:59:03.150 dataset_reader.text_token_indexers.type = ref
2021-11-13 15:59:03.151 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 15:59:03.151 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 15:59:03.151 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 15:59:03.151 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 15:59:03.151 type = bert-base-cased
2021-11-13 15:59:03.152 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 15:59:03.152 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 15:59:03.152 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 15:59:03.153 dataset_reader.to_index = 6
2021-11-13 15:59:03.153 dataset_reader.type = example_reader
2021-11-13 15:59:03.153 dataset_reader.max_instances = None
2021-11-13 15:59:03.153 dataset_reader.manual_distributed_sharding = False
2021-11-13 15:59:03.153 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 15:59:03.153 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 15:59:03.153 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 15:59:03.153 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 15:59:03.153 type = bert-base-cased
2021-11-13 15:59:03.154 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 15:59:03.154 dataset_reader.tokenizer.max_length = 128
2021-11-13 15:59:03.154 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 15:59:03.155 dataset_reader.text_token_indexers.type = ref
2021-11-13 15:59:03.155 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 15:59:03.155 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 15:59:03.156 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 15:59:03.156 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 15:59:03.156 type = bert-base-cased
2021-11-13 15:59:03.156 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 15:59:03.156 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 15:59:03.156 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 15:59:03.157 dataset_reader.to_index = 6
2021-11-13 15:59:03.158 type = from_instances
2021-11-13 15:59:03.158 Loading token dictionary from /tmp/tmper49nzmk/vocabulary.
2021-11-13 15:59:03.159 model.type = sentence_level_classifier
2021-11-13 15:59:03.160 model.embedder.type = ref
2021-11-13 15:59:03.161 model.embedder.type = basic
2021-11-13 15:59:03.162 model.embedder.token_embedders.type = ref
2021-11-13 15:59:03.164 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 15:59:03.165 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 15:59:03.165 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-13 15:59:03.166 type = bert-base-cased
2021-11-13 15:59:03.166 model.embedder.token_embedders.tokens.max_length = None
2021-11-13 15:59:03.166 model.embedder.token_embedders.tokens.sub_module = None
2021-11-13 15:59:03.166 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-13 15:59:03.167 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-13 15:59:03.167 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-13 15:59:03.167 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-13 15:59:03.167 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-13 15:59:03.167 model.embedder.token_embedders.tokens.load_weights = True
2021-11-13 15:59:03.168 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-13 15:59:03.168 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-13 15:59:03.168 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-13 15:59:03.218 model.encoder.type = bert_pooler
2021-11-13 15:59:03.218 model.encoder.type = bert_pooler
2021-11-13 15:59:03.218 model.encoder.pretrained_model = bert-base-cased
2021-11-13 15:59:03.218 type = bert-base-cased
2021-11-13 15:59:03.218 model.encoder.override_weights_file = None
2021-11-13 15:59:03.218 model.encoder.override_weights_strip_prefix = None
2021-11-13 15:59:03.218 model.encoder.load_weights = True
2021-11-13 15:59:03.218 model.encoder.requires_grad = True
2021-11-13 15:59:03.219 model.encoder.dropout = 0.0
2021-11-13 15:59:03.219 model.encoder.transformer_kwargs = None
2021-11-13 15:59:03.416 removing temporary unarchived model dir at /tmp/tmper49nzmk
2021-11-13 15:59:08.404 Plugin allennlp_models available
2021-11-13 15:59:08.411 Plugin allennlp_server available
2021-11-13 15:59:08.412 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-13 15:59:08.412 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpoi1uagiz
2021-11-13 15:59:10.523 dataset_reader.type = example_reader
2021-11-13 15:59:10.524 dataset_reader.max_instances = None
2021-11-13 15:59:10.524 dataset_reader.manual_distributed_sharding = False
2021-11-13 15:59:10.524 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 15:59:10.524 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 15:59:10.524 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 15:59:10.524 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 15:59:10.524 type = bert-base-cased
2021-11-13 15:59:10.524 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 15:59:10.524 dataset_reader.tokenizer.max_length = 128
2021-11-13 15:59:10.524 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 15:59:10.525 dataset_reader.text_token_indexers.type = ref
2021-11-13 15:59:10.526 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 15:59:10.527 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 15:59:10.527 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 15:59:10.527 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 15:59:10.527 type = bert-base-cased
2021-11-13 15:59:10.527 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 15:59:10.527 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 15:59:10.527 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 15:59:10.528 dataset_reader.to_index = 6
2021-11-13 15:59:10.528 dataset_reader.type = example_reader
2021-11-13 15:59:10.528 dataset_reader.max_instances = None
2021-11-13 15:59:10.528 dataset_reader.manual_distributed_sharding = False
2021-11-13 15:59:10.528 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 15:59:10.529 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 15:59:10.529 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 15:59:10.529 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 15:59:10.529 type = bert-base-cased
2021-11-13 15:59:10.529 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 15:59:10.529 dataset_reader.tokenizer.max_length = 128
2021-11-13 15:59:10.529 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 15:59:10.530 dataset_reader.text_token_indexers.type = ref
2021-11-13 15:59:10.531 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 15:59:10.531 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 15:59:10.531 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 15:59:10.531 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 15:59:10.531 type = bert-base-cased
2021-11-13 15:59:10.531 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 15:59:10.531 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 15:59:10.532 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 15:59:10.532 dataset_reader.to_index = 6
2021-11-13 15:59:10.533 type = from_instances
2021-11-13 15:59:10.533 Loading token dictionary from /tmp/tmpoi1uagiz/vocabulary.
2021-11-13 15:59:10.533 model.type = sentence_level_classifier
2021-11-13 15:59:10.533 model.embedder.type = ref
2021-11-13 15:59:10.534 model.embedder.type = basic
2021-11-13 15:59:10.534 model.embedder.token_embedders.type = ref
2021-11-13 15:59:10.535 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 15:59:10.535 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 15:59:10.536 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-13 15:59:10.536 type = bert-base-cased
2021-11-13 15:59:10.536 model.embedder.token_embedders.tokens.max_length = None
2021-11-13 15:59:10.536 model.embedder.token_embedders.tokens.sub_module = None
2021-11-13 15:59:10.536 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-13 15:59:10.536 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-13 15:59:10.536 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-13 15:59:10.536 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-13 15:59:10.536 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-13 15:59:10.536 model.embedder.token_embedders.tokens.load_weights = True
2021-11-13 15:59:10.536 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-13 15:59:10.536 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-13 15:59:10.536 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-13 15:59:10.587 model.encoder.type = bert_pooler
2021-11-13 15:59:10.587 model.encoder.type = bert_pooler
2021-11-13 15:59:10.587 model.encoder.pretrained_model = bert-base-cased
2021-11-13 15:59:10.587 type = bert-base-cased
2021-11-13 15:59:10.587 model.encoder.override_weights_file = None
2021-11-13 15:59:10.587 model.encoder.override_weights_strip_prefix = None
2021-11-13 15:59:10.587 model.encoder.load_weights = True
2021-11-13 15:59:10.588 model.encoder.requires_grad = True
2021-11-13 15:59:10.588 model.encoder.dropout = 0.0
2021-11-13 15:59:10.588 model.encoder.transformer_kwargs = None
2021-11-13 15:59:10.782 removing temporary unarchived model dir at /tmp/tmpoi1uagiz
2021-11-13 15:59:13.982 Plugin allennlp_models available
2021-11-13 15:59:13.989 Plugin allennlp_server available
2021-11-13 15:59:13.990 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-13 15:59:13.990 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp373xe6fx
2021-11-13 15:59:16.092 dataset_reader.type = example_reader
2021-11-13 15:59:16.092 dataset_reader.max_instances = None
2021-11-13 15:59:16.092 dataset_reader.manual_distributed_sharding = False
2021-11-13 15:59:16.092 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 15:59:16.092 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 15:59:16.092 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 15:59:16.092 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 15:59:16.092 type = bert-base-cased
2021-11-13 15:59:16.092 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 15:59:16.092 dataset_reader.tokenizer.max_length = 128
2021-11-13 15:59:16.092 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 15:59:16.094 dataset_reader.text_token_indexers.type = ref
2021-11-13 15:59:16.095 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 15:59:16.095 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 15:59:16.095 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 15:59:16.095 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 15:59:16.095 type = bert-base-cased
2021-11-13 15:59:16.095 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 15:59:16.095 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 15:59:16.095 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 15:59:16.096 dataset_reader.to_index = 6
2021-11-13 15:59:16.097 dataset_reader.type = example_reader
2021-11-13 15:59:16.097 dataset_reader.max_instances = None
2021-11-13 15:59:16.097 dataset_reader.manual_distributed_sharding = False
2021-11-13 15:59:16.097 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 15:59:16.097 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 15:59:16.097 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 15:59:16.097 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 15:59:16.097 type = bert-base-cased
2021-11-13 15:59:16.097 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 15:59:16.097 dataset_reader.tokenizer.max_length = 128
2021-11-13 15:59:16.097 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 15:59:16.098 dataset_reader.text_token_indexers.type = ref
2021-11-13 15:59:16.099 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 15:59:16.099 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 15:59:16.099 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 15:59:16.099 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 15:59:16.099 type = bert-base-cased
2021-11-13 15:59:16.099 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 15:59:16.099 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 15:59:16.099 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 15:59:16.100 dataset_reader.to_index = 6
2021-11-13 15:59:16.100 type = from_instances
2021-11-13 15:59:16.100 Loading token dictionary from /tmp/tmp373xe6fx/vocabulary.
2021-11-13 15:59:16.101 model.type = sentence_level_classifier
2021-11-13 15:59:16.102 model.embedder.type = ref
2021-11-13 15:59:16.104 model.embedder.type = basic
2021-11-13 15:59:16.104 model.embedder.token_embedders.type = ref
2021-11-13 15:59:16.107 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 15:59:16.107 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 15:59:16.108 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-13 15:59:16.108 type = bert-base-cased
2021-11-13 15:59:16.108 model.embedder.token_embedders.tokens.max_length = None
2021-11-13 15:59:16.109 model.embedder.token_embedders.tokens.sub_module = None
2021-11-13 15:59:16.109 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-13 15:59:16.109 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-13 15:59:16.109 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-13 15:59:16.109 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-13 15:59:16.110 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-13 15:59:16.110 model.embedder.token_embedders.tokens.load_weights = True
2021-11-13 15:59:16.110 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-13 15:59:16.110 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-13 15:59:16.110 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-13 15:59:16.160 model.encoder.type = bert_pooler
2021-11-13 15:59:16.160 model.encoder.type = bert_pooler
2021-11-13 15:59:16.161 model.encoder.pretrained_model = bert-base-cased
2021-11-13 15:59:16.161 type = bert-base-cased
2021-11-13 15:59:16.161 model.encoder.override_weights_file = None
2021-11-13 15:59:16.161 model.encoder.override_weights_strip_prefix = None
2021-11-13 15:59:16.161 model.encoder.load_weights = True
2021-11-13 15:59:16.161 model.encoder.requires_grad = True
2021-11-13 15:59:16.161 model.encoder.dropout = 0.0
2021-11-13 15:59:16.161 model.encoder.transformer_kwargs = None
2021-11-13 15:59:16.356 removing temporary unarchived model dir at /tmp/tmp373xe6fx
2021-11-13 15:59:17.430 Plugin allennlp_models available
2021-11-13 15:59:17.437 Plugin allennlp_server available
2021-11-13 15:59:17.437 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-13 15:59:17.438 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpppmbgncz
2021-11-13 15:59:19.540 dataset_reader.type = example_reader
2021-11-13 15:59:19.540 dataset_reader.max_instances = None
2021-11-13 15:59:19.540 dataset_reader.manual_distributed_sharding = False
2021-11-13 15:59:19.540 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 15:59:19.540 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 15:59:19.541 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 15:59:19.541 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 15:59:19.541 type = bert-base-cased
2021-11-13 15:59:19.541 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 15:59:19.541 dataset_reader.tokenizer.max_length = 128
2021-11-13 15:59:19.541 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 15:59:19.542 dataset_reader.text_token_indexers.type = ref
2021-11-13 15:59:19.543 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 15:59:19.544 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 15:59:19.544 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 15:59:19.544 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 15:59:19.544 type = bert-base-cased
2021-11-13 15:59:19.544 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 15:59:19.544 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 15:59:19.544 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 15:59:19.545 dataset_reader.to_index = 6
2021-11-13 15:59:19.545 dataset_reader.type = example_reader
2021-11-13 15:59:19.545 dataset_reader.max_instances = None
2021-11-13 15:59:19.545 dataset_reader.manual_distributed_sharding = False
2021-11-13 15:59:19.545 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 15:59:19.545 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 15:59:19.545 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 15:59:19.546 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 15:59:19.546 type = bert-base-cased
2021-11-13 15:59:19.546 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 15:59:19.546 dataset_reader.tokenizer.max_length = 128
2021-11-13 15:59:19.546 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 15:59:19.547 dataset_reader.text_token_indexers.type = ref
2021-11-13 15:59:19.547 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 15:59:19.547 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 15:59:19.547 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 15:59:19.548 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 15:59:19.548 type = bert-base-cased
2021-11-13 15:59:19.548 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 15:59:19.548 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 15:59:19.548 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 15:59:19.549 dataset_reader.to_index = 6
2021-11-13 15:59:19.549 type = from_instances
2021-11-13 15:59:19.549 Loading token dictionary from /tmp/tmpppmbgncz/vocabulary.
2021-11-13 15:59:19.549 model.type = sentence_level_classifier
2021-11-13 15:59:19.550 model.embedder.type = ref
2021-11-13 15:59:19.550 model.embedder.type = basic
2021-11-13 15:59:19.551 model.embedder.token_embedders.type = ref
2021-11-13 15:59:19.551 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 15:59:19.552 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 15:59:19.552 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-13 15:59:19.552 type = bert-base-cased
2021-11-13 15:59:19.552 model.embedder.token_embedders.tokens.max_length = None
2021-11-13 15:59:19.552 model.embedder.token_embedders.tokens.sub_module = None
2021-11-13 15:59:19.552 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-13 15:59:19.552 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-13 15:59:19.552 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-13 15:59:19.552 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-13 15:59:19.552 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-13 15:59:19.552 model.embedder.token_embedders.tokens.load_weights = True
2021-11-13 15:59:19.552 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-13 15:59:19.552 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-13 15:59:19.552 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-13 15:59:19.601 model.encoder.type = bert_pooler
2021-11-13 15:59:19.602 model.encoder.type = bert_pooler
2021-11-13 15:59:19.602 model.encoder.pretrained_model = bert-base-cased
2021-11-13 15:59:19.602 type = bert-base-cased
2021-11-13 15:59:19.602 model.encoder.override_weights_file = None
2021-11-13 15:59:19.602 model.encoder.override_weights_strip_prefix = None
2021-11-13 15:59:19.602 model.encoder.load_weights = True
2021-11-13 15:59:19.602 model.encoder.requires_grad = True
2021-11-13 15:59:19.603 model.encoder.dropout = 0.0
2021-11-13 15:59:19.603 model.encoder.transformer_kwargs = None
2021-11-13 15:59:19.799 removing temporary unarchived model dir at /tmp/tmpppmbgncz
2021-11-13 16:01:35.644 Plugin allennlp_models available
2021-11-13 16:01:35.654 Plugin allennlp_server available
2021-11-13 16:01:35.655 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-13 16:01:35.655 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp3vu2jyqv
2021-11-13 16:01:37.768 dataset_reader.type = example_reader
2021-11-13 16:01:37.768 dataset_reader.max_instances = None
2021-11-13 16:01:37.768 dataset_reader.manual_distributed_sharding = False
2021-11-13 16:01:37.768 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 16:01:37.768 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 16:01:37.768 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 16:01:37.768 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 16:01:37.768 type = bert-base-cased
2021-11-13 16:01:37.768 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 16:01:37.768 dataset_reader.tokenizer.max_length = 128
2021-11-13 16:01:37.768 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 16:01:37.770 dataset_reader.text_token_indexers.type = ref
2021-11-13 16:01:37.771 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 16:01:37.771 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 16:01:37.771 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 16:01:37.771 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 16:01:37.771 type = bert-base-cased
2021-11-13 16:01:37.771 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 16:01:37.771 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 16:01:37.771 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 16:01:37.772 dataset_reader.to_index = 6
2021-11-13 16:01:37.772 dataset_reader.type = example_reader
2021-11-13 16:01:37.773 dataset_reader.max_instances = None
2021-11-13 16:01:37.773 dataset_reader.manual_distributed_sharding = False
2021-11-13 16:01:37.773 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 16:01:37.773 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 16:01:37.773 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 16:01:37.773 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 16:01:37.773 type = bert-base-cased
2021-11-13 16:01:37.773 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 16:01:37.773 dataset_reader.tokenizer.max_length = 128
2021-11-13 16:01:37.773 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 16:01:37.774 dataset_reader.text_token_indexers.type = ref
2021-11-13 16:01:37.775 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 16:01:37.775 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 16:01:37.775 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 16:01:37.775 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 16:01:37.775 type = bert-base-cased
2021-11-13 16:01:37.775 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 16:01:37.775 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 16:01:37.775 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 16:01:37.776 dataset_reader.to_index = 6
2021-11-13 16:01:37.776 type = from_instances
2021-11-13 16:01:37.777 Loading token dictionary from /tmp/tmp3vu2jyqv/vocabulary.
2021-11-13 16:01:37.777 model.type = sentence_level_classifier
2021-11-13 16:01:37.778 model.embedder.type = ref
2021-11-13 16:01:37.780 model.embedder.type = basic
2021-11-13 16:01:37.781 model.embedder.token_embedders.type = ref
2021-11-13 16:01:37.783 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 16:01:37.783 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 16:01:37.784 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-13 16:01:37.784 type = bert-base-cased
2021-11-13 16:01:37.784 model.embedder.token_embedders.tokens.max_length = None
2021-11-13 16:01:37.784 model.embedder.token_embedders.tokens.sub_module = None
2021-11-13 16:01:37.784 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-13 16:01:37.785 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-13 16:01:37.785 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-13 16:01:37.785 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-13 16:01:37.785 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-13 16:01:37.785 model.embedder.token_embedders.tokens.load_weights = True
2021-11-13 16:01:37.786 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-13 16:01:37.786 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-13 16:01:37.786 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-13 16:01:37.837 model.encoder.type = bert_pooler
2021-11-13 16:01:37.838 model.encoder.type = bert_pooler
2021-11-13 16:01:37.838 model.encoder.pretrained_model = bert-base-cased
2021-11-13 16:01:37.838 type = bert-base-cased
2021-11-13 16:01:37.838 model.encoder.override_weights_file = None
2021-11-13 16:01:37.838 model.encoder.override_weights_strip_prefix = None
2021-11-13 16:01:37.838 model.encoder.load_weights = True
2021-11-13 16:01:37.838 model.encoder.requires_grad = True
2021-11-13 16:01:37.838 model.encoder.dropout = 0.0
2021-11-13 16:01:37.838 model.encoder.transformer_kwargs = None
2021-11-13 16:01:38.032 removing temporary unarchived model dir at /tmp/tmp3vu2jyqv
2021-11-13 16:01:51.916 Plugin allennlp_models available
2021-11-13 16:01:51.924 Plugin allennlp_server available
2021-11-13 16:01:51.925 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-13 16:01:51.925 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpvewayna9
2021-11-13 16:01:54.037 dataset_reader.type = example_reader
2021-11-13 16:01:54.037 dataset_reader.max_instances = None
2021-11-13 16:01:54.037 dataset_reader.manual_distributed_sharding = False
2021-11-13 16:01:54.037 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 16:01:54.038 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 16:01:54.038 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 16:01:54.038 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 16:01:54.038 type = bert-base-cased
2021-11-13 16:01:54.038 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 16:01:54.038 dataset_reader.tokenizer.max_length = 128
2021-11-13 16:01:54.038 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 16:01:54.039 dataset_reader.text_token_indexers.type = ref
2021-11-13 16:01:54.040 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 16:01:54.040 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 16:01:54.041 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 16:01:54.041 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 16:01:54.041 type = bert-base-cased
2021-11-13 16:01:54.041 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 16:01:54.041 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 16:01:54.041 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 16:01:54.042 dataset_reader.to_index = 6
2021-11-13 16:01:54.042 dataset_reader.type = example_reader
2021-11-13 16:01:54.042 dataset_reader.max_instances = None
2021-11-13 16:01:54.042 dataset_reader.manual_distributed_sharding = False
2021-11-13 16:01:54.042 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 16:01:54.042 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 16:01:54.042 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 16:01:54.042 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 16:01:54.042 type = bert-base-cased
2021-11-13 16:01:54.042 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 16:01:54.043 dataset_reader.tokenizer.max_length = 128
2021-11-13 16:01:54.043 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 16:01:54.043 dataset_reader.text_token_indexers.type = ref
2021-11-13 16:01:54.044 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 16:01:54.044 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 16:01:54.044 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 16:01:54.044 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 16:01:54.044 type = bert-base-cased
2021-11-13 16:01:54.044 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 16:01:54.045 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 16:01:54.045 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 16:01:54.045 dataset_reader.to_index = 6
2021-11-13 16:01:54.045 type = from_instances
2021-11-13 16:01:54.045 Loading token dictionary from /tmp/tmpvewayna9/vocabulary.
2021-11-13 16:01:54.046 model.type = sentence_level_classifier
2021-11-13 16:01:54.046 model.embedder.type = ref
2021-11-13 16:01:54.047 model.embedder.type = basic
2021-11-13 16:01:54.047 model.embedder.token_embedders.type = ref
2021-11-13 16:01:54.048 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 16:01:54.048 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 16:01:54.048 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-13 16:01:54.048 type = bert-base-cased
2021-11-13 16:01:54.048 model.embedder.token_embedders.tokens.max_length = None
2021-11-13 16:01:54.048 model.embedder.token_embedders.tokens.sub_module = None
2021-11-13 16:01:54.048 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-13 16:01:54.048 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-13 16:01:54.048 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-13 16:01:54.048 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-13 16:01:54.049 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-13 16:01:54.049 model.embedder.token_embedders.tokens.load_weights = True
2021-11-13 16:01:54.049 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-13 16:01:54.049 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-13 16:01:54.049 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-13 16:01:54.098 model.encoder.type = bert_pooler
2021-11-13 16:01:54.098 model.encoder.type = bert_pooler
2021-11-13 16:01:54.098 model.encoder.pretrained_model = bert-base-cased
2021-11-13 16:01:54.098 type = bert-base-cased
2021-11-13 16:01:54.098 model.encoder.override_weights_file = None
2021-11-13 16:01:54.099 model.encoder.override_weights_strip_prefix = None
2021-11-13 16:01:54.099 model.encoder.load_weights = True
2021-11-13 16:01:54.099 model.encoder.requires_grad = True
2021-11-13 16:01:54.099 model.encoder.dropout = 0.0
2021-11-13 16:01:54.099 model.encoder.transformer_kwargs = None
2021-11-13 16:01:54.295 removing temporary unarchived model dir at /tmp/tmpvewayna9
2021-11-13 16:02:07.838 Plugin allennlp_models available
2021-11-13 16:02:07.848 Plugin allennlp_server available
2021-11-13 16:02:07.849 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-13 16:02:07.849 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp9v89br29
2021-11-13 16:02:09.962 dataset_reader.type = example_reader
2021-11-13 16:02:09.962 dataset_reader.max_instances = None
2021-11-13 16:02:09.962 dataset_reader.manual_distributed_sharding = False
2021-11-13 16:02:09.962 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 16:02:09.963 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 16:02:09.963 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 16:02:09.963 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 16:02:09.963 type = bert-base-cased
2021-11-13 16:02:09.963 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 16:02:09.964 dataset_reader.tokenizer.max_length = 128
2021-11-13 16:02:09.964 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 16:02:09.965 dataset_reader.text_token_indexers.type = ref
2021-11-13 16:02:09.966 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 16:02:09.966 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 16:02:09.966 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 16:02:09.966 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 16:02:09.966 type = bert-base-cased
2021-11-13 16:02:09.966 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 16:02:09.967 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 16:02:09.967 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 16:02:09.967 dataset_reader.to_index = 6
2021-11-13 16:02:09.967 dataset_reader.type = example_reader
2021-11-13 16:02:09.968 dataset_reader.max_instances = None
2021-11-13 16:02:09.968 dataset_reader.manual_distributed_sharding = False
2021-11-13 16:02:09.968 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 16:02:09.968 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 16:02:09.968 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 16:02:09.968 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 16:02:09.969 type = bert-base-cased
2021-11-13 16:02:09.969 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 16:02:09.969 dataset_reader.tokenizer.max_length = 128
2021-11-13 16:02:09.969 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 16:02:09.970 dataset_reader.text_token_indexers.type = ref
2021-11-13 16:02:09.970 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 16:02:09.971 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 16:02:09.971 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 16:02:09.971 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 16:02:09.971 type = bert-base-cased
2021-11-13 16:02:09.971 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 16:02:09.971 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 16:02:09.971 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 16:02:09.972 dataset_reader.to_index = 6
2021-11-13 16:02:09.972 type = from_instances
2021-11-13 16:02:09.972 Loading token dictionary from /tmp/tmp9v89br29/vocabulary.
2021-11-13 16:02:09.973 model.type = sentence_level_classifier
2021-11-13 16:02:09.973 model.embedder.type = ref
2021-11-13 16:02:09.974 model.embedder.type = basic
2021-11-13 16:02:09.974 model.embedder.token_embedders.type = ref
2021-11-13 16:02:09.975 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 16:02:09.975 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 16:02:09.975 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-13 16:02:09.975 type = bert-base-cased
2021-11-13 16:02:09.975 model.embedder.token_embedders.tokens.max_length = None
2021-11-13 16:02:09.975 model.embedder.token_embedders.tokens.sub_module = None
2021-11-13 16:02:09.975 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-13 16:02:09.975 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-13 16:02:09.975 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-13 16:02:09.975 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-13 16:02:09.976 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-13 16:02:09.976 model.embedder.token_embedders.tokens.load_weights = True
2021-11-13 16:02:09.976 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-13 16:02:09.976 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-13 16:02:09.976 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-13 16:02:10.026 model.encoder.type = bert_pooler
2021-11-13 16:02:10.026 model.encoder.type = bert_pooler
2021-11-13 16:02:10.026 model.encoder.pretrained_model = bert-base-cased
2021-11-13 16:02:10.026 type = bert-base-cased
2021-11-13 16:02:10.027 model.encoder.override_weights_file = None
2021-11-13 16:02:10.027 model.encoder.override_weights_strip_prefix = None
2021-11-13 16:02:10.027 model.encoder.load_weights = True
2021-11-13 16:02:10.027 model.encoder.requires_grad = True
2021-11-13 16:02:10.027 model.encoder.dropout = 0.0
2021-11-13 16:02:10.027 model.encoder.transformer_kwargs = None
2021-11-13 16:02:10.224 removing temporary unarchived model dir at /tmp/tmp9v89br29
2021-11-13 16:02:11.551 Plugin allennlp_models available
2021-11-13 16:02:11.559 Plugin allennlp_server available
2021-11-13 16:02:11.560 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-13 16:02:11.560 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp4hg00txz
2021-11-13 16:02:13.676 dataset_reader.type = example_reader
2021-11-13 16:02:13.677 dataset_reader.max_instances = None
2021-11-13 16:02:13.677 dataset_reader.manual_distributed_sharding = False
2021-11-13 16:02:13.677 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 16:02:13.677 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 16:02:13.677 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 16:02:13.677 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 16:02:13.677 type = bert-base-cased
2021-11-13 16:02:13.677 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 16:02:13.677 dataset_reader.tokenizer.max_length = 128
2021-11-13 16:02:13.677 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 16:02:13.678 dataset_reader.text_token_indexers.type = ref
2021-11-13 16:02:13.679 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 16:02:13.679 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 16:02:13.680 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 16:02:13.680 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 16:02:13.680 type = bert-base-cased
2021-11-13 16:02:13.680 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 16:02:13.680 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 16:02:13.680 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 16:02:13.681 dataset_reader.to_index = 6
2021-11-13 16:02:13.681 dataset_reader.type = example_reader
2021-11-13 16:02:13.681 dataset_reader.max_instances = None
2021-11-13 16:02:13.681 dataset_reader.manual_distributed_sharding = False
2021-11-13 16:02:13.681 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 16:02:13.682 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 16:02:13.682 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 16:02:13.682 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 16:02:13.682 type = bert-base-cased
2021-11-13 16:02:13.682 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 16:02:13.682 dataset_reader.tokenizer.max_length = 128
2021-11-13 16:02:13.682 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 16:02:13.683 dataset_reader.text_token_indexers.type = ref
2021-11-13 16:02:13.683 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 16:02:13.683 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 16:02:13.683 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 16:02:13.684 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 16:02:13.684 type = bert-base-cased
2021-11-13 16:02:13.684 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 16:02:13.684 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 16:02:13.684 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 16:02:13.684 dataset_reader.to_index = 6
2021-11-13 16:02:13.685 type = from_instances
2021-11-13 16:02:13.685 Loading token dictionary from /tmp/tmp4hg00txz/vocabulary.
2021-11-13 16:02:13.685 model.type = sentence_level_classifier
2021-11-13 16:02:13.685 model.embedder.type = ref
2021-11-13 16:02:13.686 model.embedder.type = basic
2021-11-13 16:02:13.686 model.embedder.token_embedders.type = ref
2021-11-13 16:02:13.687 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 16:02:13.687 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 16:02:13.688 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-13 16:02:13.688 type = bert-base-cased
2021-11-13 16:02:13.688 model.embedder.token_embedders.tokens.max_length = None
2021-11-13 16:02:13.688 model.embedder.token_embedders.tokens.sub_module = None
2021-11-13 16:02:13.688 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-13 16:02:13.688 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-13 16:02:13.688 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-13 16:02:13.688 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-13 16:02:13.688 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-13 16:02:13.688 model.embedder.token_embedders.tokens.load_weights = True
2021-11-13 16:02:13.688 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-13 16:02:13.688 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-13 16:02:13.688 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-13 16:02:13.737 model.encoder.type = bert_pooler
2021-11-13 16:02:13.737 model.encoder.type = bert_pooler
2021-11-13 16:02:13.738 model.encoder.pretrained_model = bert-base-cased
2021-11-13 16:02:13.738 type = bert-base-cased
2021-11-13 16:02:13.738 model.encoder.override_weights_file = None
2021-11-13 16:02:13.738 model.encoder.override_weights_strip_prefix = None
2021-11-13 16:02:13.738 model.encoder.load_weights = True
2021-11-13 16:02:13.738 model.encoder.requires_grad = True
2021-11-13 16:02:13.739 model.encoder.dropout = 0.0
2021-11-13 16:02:13.739 model.encoder.transformer_kwargs = None
2021-11-13 16:02:13.939 removing temporary unarchived model dir at /tmp/tmp4hg00txz
2021-11-13 16:02:21.684 Plugin allennlp_models available
2021-11-13 16:02:21.691 Plugin allennlp_server available
2021-11-13 16:02:21.692 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-13 16:02:21.692 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpeib8zr7c
2021-11-13 16:02:22.025 Plugin allennlp_models available
2021-11-13 16:02:22.030 Plugin allennlp_server available
2021-11-13 16:02:22.030 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-13 16:02:22.031 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp659egmgk
2021-11-13 16:02:24.436 dataset_reader.type = example_reader
2021-11-13 16:02:24.437 dataset_reader.max_instances = None
2021-11-13 16:02:24.437 dataset_reader.manual_distributed_sharding = False
2021-11-13 16:02:24.437 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 16:02:24.437 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 16:02:24.437 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 16:02:24.438 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 16:02:24.438 type = bert-base-cased
2021-11-13 16:02:24.438 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 16:02:24.438 dataset_reader.tokenizer.max_length = 128
2021-11-13 16:02:24.438 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 16:02:24.439 dataset_reader.text_token_indexers.type = ref
2021-11-13 16:02:24.440 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 16:02:24.440 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 16:02:24.440 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 16:02:24.441 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 16:02:24.441 type = bert-base-cased
2021-11-13 16:02:24.441 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 16:02:24.441 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 16:02:24.441 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 16:02:24.442 dataset_reader.to_index = 6
2021-11-13 16:02:24.442 dataset_reader.type = example_reader
2021-11-13 16:02:24.442 dataset_reader.max_instances = None
2021-11-13 16:02:24.442 dataset_reader.manual_distributed_sharding = False
2021-11-13 16:02:24.442 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 16:02:24.442 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 16:02:24.443 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 16:02:24.443 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 16:02:24.443 type = bert-base-cased
2021-11-13 16:02:24.443 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 16:02:24.443 dataset_reader.tokenizer.max_length = 128
2021-11-13 16:02:24.443 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 16:02:24.444 dataset_reader.text_token_indexers.type = ref
2021-11-13 16:02:24.445 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 16:02:24.445 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 16:02:24.445 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 16:02:24.445 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 16:02:24.445 type = bert-base-cased
2021-11-13 16:02:24.445 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 16:02:24.446 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 16:02:24.446 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 16:02:24.446 dataset_reader.to_index = 6
2021-11-13 16:02:24.446 type = from_instances
2021-11-13 16:02:24.446 Loading token dictionary from /tmp/tmpeib8zr7c/vocabulary.
2021-11-13 16:02:24.447 model.type = sentence_level_classifier
2021-11-13 16:02:24.447 model.embedder.type = ref
2021-11-13 16:02:24.448 model.embedder.type = basic
2021-11-13 16:02:24.448 model.embedder.token_embedders.type = ref
2021-11-13 16:02:24.449 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 16:02:24.449 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 16:02:24.449 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-13 16:02:24.449 type = bert-base-cased
2021-11-13 16:02:24.450 model.embedder.token_embedders.tokens.max_length = None
2021-11-13 16:02:24.450 model.embedder.token_embedders.tokens.sub_module = None
2021-11-13 16:02:24.450 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-13 16:02:24.450 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-13 16:02:24.450 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-13 16:02:24.450 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-13 16:02:24.450 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-13 16:02:24.450 model.embedder.token_embedders.tokens.load_weights = True
2021-11-13 16:02:24.450 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-13 16:02:24.450 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-13 16:02:24.450 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-13 16:02:24.471 dataset_reader.type = example_reader
2021-11-13 16:02:24.471 dataset_reader.max_instances = None
2021-11-13 16:02:24.471 dataset_reader.manual_distributed_sharding = False
2021-11-13 16:02:24.471 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 16:02:24.472 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 16:02:24.472 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 16:02:24.473 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 16:02:24.473 type = bert-base-cased
2021-11-13 16:02:24.473 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 16:02:24.474 dataset_reader.tokenizer.max_length = 128
2021-11-13 16:02:24.474 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 16:02:24.476 dataset_reader.text_token_indexers.type = ref
2021-11-13 16:02:24.478 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 16:02:24.479 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 16:02:24.479 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 16:02:24.479 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 16:02:24.480 type = bert-base-cased
2021-11-13 16:02:24.480 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 16:02:24.480 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 16:02:24.480 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 16:02:24.482 dataset_reader.to_index = 6
2021-11-13 16:02:24.483 dataset_reader.type = example_reader
2021-11-13 16:02:24.483 dataset_reader.max_instances = None
2021-11-13 16:02:24.483 dataset_reader.manual_distributed_sharding = False
2021-11-13 16:02:24.484 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 16:02:24.484 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 16:02:24.484 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 16:02:24.485 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 16:02:24.485 type = bert-base-cased
2021-11-13 16:02:24.485 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 16:02:24.486 dataset_reader.tokenizer.max_length = 128
2021-11-13 16:02:24.486 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 16:02:24.488 dataset_reader.text_token_indexers.type = ref
2021-11-13 16:02:24.490 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 16:02:24.490 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 16:02:24.491 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 16:02:24.491 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 16:02:24.491 type = bert-base-cased
2021-11-13 16:02:24.491 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 16:02:24.492 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 16:02:24.492 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 16:02:24.493 dataset_reader.to_index = 6
2021-11-13 16:02:24.494 type = from_instances
2021-11-13 16:02:24.494 Loading token dictionary from /tmp/tmp659egmgk/vocabulary.
2021-11-13 16:02:24.495 model.type = sentence_level_classifier
2021-11-13 16:02:24.495 model.embedder.type = ref
2021-11-13 16:02:24.497 model.embedder.type = basic
2021-11-13 16:02:24.497 model.embedder.token_embedders.type = ref
2021-11-13 16:02:24.498 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 16:02:24.499 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 16:02:24.499 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-13 16:02:24.500 type = bert-base-cased
2021-11-13 16:02:24.500 model.embedder.token_embedders.tokens.max_length = None
2021-11-13 16:02:24.500 model.embedder.token_embedders.tokens.sub_module = None
2021-11-13 16:02:24.500 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-13 16:02:24.500 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-13 16:02:24.500 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-13 16:02:24.501 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-13 16:02:24.501 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-13 16:02:24.501 model.embedder.token_embedders.tokens.load_weights = True
2021-11-13 16:02:24.501 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-13 16:02:24.502 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-13 16:02:24.502 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-13 16:02:24.546 model.encoder.type = bert_pooler
2021-11-13 16:02:24.546 model.encoder.type = bert_pooler
2021-11-13 16:02:24.547 model.encoder.pretrained_model = bert-base-cased
2021-11-13 16:02:24.547 type = bert-base-cased
2021-11-13 16:02:24.547 model.encoder.override_weights_file = None
2021-11-13 16:02:24.547 model.encoder.override_weights_strip_prefix = None
2021-11-13 16:02:24.547 model.encoder.load_weights = True
2021-11-13 16:02:24.547 model.encoder.requires_grad = True
2021-11-13 16:02:24.547 model.encoder.dropout = 0.0
2021-11-13 16:02:24.547 model.encoder.transformer_kwargs = None
2021-11-13 16:02:24.593 model.encoder.type = bert_pooler
2021-11-13 16:02:24.593 model.encoder.type = bert_pooler
2021-11-13 16:02:24.594 model.encoder.pretrained_model = bert-base-cased
2021-11-13 16:02:24.594 type = bert-base-cased
2021-11-13 16:02:24.594 model.encoder.override_weights_file = None
2021-11-13 16:02:24.594 model.encoder.override_weights_strip_prefix = None
2021-11-13 16:02:24.594 model.encoder.load_weights = True
2021-11-13 16:02:24.594 model.encoder.requires_grad = True
2021-11-13 16:02:24.595 model.encoder.dropout = 0.0
2021-11-13 16:02:24.595 model.encoder.transformer_kwargs = None
2021-11-13 16:02:24.776 removing temporary unarchived model dir at /tmp/tmpeib8zr7c
2021-11-13 16:02:24.892 removing temporary unarchived model dir at /tmp/tmp659egmgk
2021-11-13 16:04:14.722 Plugin allennlp_models available
2021-11-13 16:04:14.729 Plugin allennlp_server available
2021-11-13 16:04:14.729 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-13 16:04:14.730 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp4z46iphp
2021-11-13 16:04:16.837 dataset_reader.type = example_reader
2021-11-13 16:04:16.837 dataset_reader.max_instances = None
2021-11-13 16:04:16.837 dataset_reader.manual_distributed_sharding = False
2021-11-13 16:04:16.837 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 16:04:16.837 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 16:04:16.838 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 16:04:16.838 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 16:04:16.838 type = bert-base-cased
2021-11-13 16:04:16.838 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 16:04:16.838 dataset_reader.tokenizer.max_length = 128
2021-11-13 16:04:16.838 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 16:04:16.839 dataset_reader.text_token_indexers.type = ref
2021-11-13 16:04:16.840 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 16:04:16.840 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 16:04:16.841 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 16:04:16.841 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 16:04:16.841 type = bert-base-cased
2021-11-13 16:04:16.841 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 16:04:16.842 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 16:04:16.842 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 16:04:16.842 dataset_reader.to_index = 6
2021-11-13 16:04:16.843 dataset_reader.type = example_reader
2021-11-13 16:04:16.843 dataset_reader.max_instances = None
2021-11-13 16:04:16.843 dataset_reader.manual_distributed_sharding = False
2021-11-13 16:04:16.843 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 16:04:16.843 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 16:04:16.843 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 16:04:16.843 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 16:04:16.844 type = bert-base-cased
2021-11-13 16:04:16.844 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 16:04:16.844 dataset_reader.tokenizer.max_length = 128
2021-11-13 16:04:16.844 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 16:04:16.845 dataset_reader.text_token_indexers.type = ref
2021-11-13 16:04:16.845 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 16:04:16.846 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 16:04:16.846 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 16:04:16.846 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 16:04:16.846 type = bert-base-cased
2021-11-13 16:04:16.846 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 16:04:16.846 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 16:04:16.846 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 16:04:16.847 dataset_reader.to_index = 6
2021-11-13 16:04:16.847 type = from_instances
2021-11-13 16:04:16.847 Loading token dictionary from /tmp/tmp4z46iphp/vocabulary.
2021-11-13 16:04:16.848 model.type = sentence_level_classifier
2021-11-13 16:04:16.848 model.embedder.type = ref
2021-11-13 16:04:16.849 model.embedder.type = basic
2021-11-13 16:04:16.849 model.embedder.token_embedders.type = ref
2021-11-13 16:04:16.850 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 16:04:16.850 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 16:04:16.850 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-13 16:04:16.850 type = bert-base-cased
2021-11-13 16:04:16.851 model.embedder.token_embedders.tokens.max_length = None
2021-11-13 16:04:16.851 model.embedder.token_embedders.tokens.sub_module = None
2021-11-13 16:04:16.851 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-13 16:04:16.851 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-13 16:04:16.851 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-13 16:04:16.851 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-13 16:04:16.851 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-13 16:04:16.851 model.embedder.token_embedders.tokens.load_weights = True
2021-11-13 16:04:16.851 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-13 16:04:16.851 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-13 16:04:16.851 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-13 16:04:16.891 model.encoder.type = bert_pooler
2021-11-13 16:04:16.891 model.encoder.type = bert_pooler
2021-11-13 16:04:16.891 model.encoder.pretrained_model = bert-base-cased
2021-11-13 16:04:16.891 type = bert-base-cased
2021-11-13 16:04:16.892 model.encoder.override_weights_file = None
2021-11-13 16:04:16.892 model.encoder.override_weights_strip_prefix = None
2021-11-13 16:04:16.892 model.encoder.load_weights = True
2021-11-13 16:04:16.892 model.encoder.requires_grad = True
2021-11-13 16:04:16.892 model.encoder.dropout = 0.0
2021-11-13 16:04:16.892 model.encoder.transformer_kwargs = None
2021-11-13 16:04:17.094 removing temporary unarchived model dir at /tmp/tmp4z46iphp
2021-11-13 16:04:29.096 Plugin allennlp_models available
2021-11-13 16:04:29.111 Plugin allennlp_server available
2021-11-13 16:04:29.112 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-13 16:04:29.112 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp9yk4avzj
2021-11-13 16:04:31.244 dataset_reader.type = example_reader
2021-11-13 16:04:31.244 dataset_reader.max_instances = None
2021-11-13 16:04:31.244 dataset_reader.manual_distributed_sharding = False
2021-11-13 16:04:31.244 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 16:04:31.244 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 16:04:31.244 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 16:04:31.244 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 16:04:31.245 type = bert-base-cased
2021-11-13 16:04:31.245 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 16:04:31.245 dataset_reader.tokenizer.max_length = 128
2021-11-13 16:04:31.245 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 16:04:31.246 dataset_reader.text_token_indexers.type = ref
2021-11-13 16:04:31.247 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 16:04:31.247 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 16:04:31.247 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 16:04:31.247 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 16:04:31.247 type = bert-base-cased
2021-11-13 16:04:31.247 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 16:04:31.247 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 16:04:31.247 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 16:04:31.248 dataset_reader.to_index = 6
2021-11-13 16:04:31.249 dataset_reader.type = example_reader
2021-11-13 16:04:31.249 dataset_reader.max_instances = None
2021-11-13 16:04:31.249 dataset_reader.manual_distributed_sharding = False
2021-11-13 16:04:31.249 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 16:04:31.249 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 16:04:31.249 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 16:04:31.249 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 16:04:31.249 type = bert-base-cased
2021-11-13 16:04:31.249 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 16:04:31.249 dataset_reader.tokenizer.max_length = 128
2021-11-13 16:04:31.249 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 16:04:31.250 dataset_reader.text_token_indexers.type = ref
2021-11-13 16:04:31.251 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 16:04:31.251 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 16:04:31.251 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 16:04:31.251 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 16:04:31.251 type = bert-base-cased
2021-11-13 16:04:31.251 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 16:04:31.251 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 16:04:31.251 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 16:04:31.252 dataset_reader.to_index = 6
2021-11-13 16:04:31.252 type = from_instances
2021-11-13 16:04:31.253 Loading token dictionary from /tmp/tmp9yk4avzj/vocabulary.
2021-11-13 16:04:31.253 model.type = sentence_level_classifier
2021-11-13 16:04:31.253 model.embedder.type = ref
2021-11-13 16:04:31.254 model.embedder.type = basic
2021-11-13 16:04:31.255 model.embedder.token_embedders.type = ref
2021-11-13 16:04:31.255 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 16:04:31.256 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 16:04:31.256 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-13 16:04:31.256 type = bert-base-cased
2021-11-13 16:04:31.256 model.embedder.token_embedders.tokens.max_length = None
2021-11-13 16:04:31.256 model.embedder.token_embedders.tokens.sub_module = None
2021-11-13 16:04:31.256 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-13 16:04:31.256 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-13 16:04:31.256 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-13 16:04:31.256 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-13 16:04:31.256 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-13 16:04:31.256 model.embedder.token_embedders.tokens.load_weights = True
2021-11-13 16:04:31.256 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-13 16:04:31.256 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-13 16:04:31.256 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-13 16:04:31.309 model.encoder.type = bert_pooler
2021-11-13 16:04:31.309 model.encoder.type = bert_pooler
2021-11-13 16:04:31.309 model.encoder.pretrained_model = bert-base-cased
2021-11-13 16:04:31.309 type = bert-base-cased
2021-11-13 16:04:31.310 model.encoder.override_weights_file = None
2021-11-13 16:04:31.310 model.encoder.override_weights_strip_prefix = None
2021-11-13 16:04:31.310 model.encoder.load_weights = True
2021-11-13 16:04:31.310 model.encoder.requires_grad = True
2021-11-13 16:04:31.310 model.encoder.dropout = 0.0
2021-11-13 16:04:31.310 model.encoder.transformer_kwargs = None
2021-11-13 16:04:31.515 removing temporary unarchived model dir at /tmp/tmp9yk4avzj
2021-11-13 16:22:24.667 Plugin allennlp_models available
2021-11-13 16:22:24.678 Plugin allennlp_server available
2021-11-13 16:22:24.679 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-13 16:22:24.679 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp1a5w8t_x
2021-11-13 16:22:26.820 dataset_reader.type = example_reader
2021-11-13 16:22:26.820 dataset_reader.max_instances = None
2021-11-13 16:22:26.820 dataset_reader.manual_distributed_sharding = False
2021-11-13 16:22:26.820 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 16:22:26.820 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 16:22:26.820 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 16:22:26.821 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 16:22:26.821 type = bert-base-cased
2021-11-13 16:22:26.821 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 16:22:26.821 dataset_reader.tokenizer.max_length = 128
2021-11-13 16:22:26.821 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 16:22:26.822 dataset_reader.text_token_indexers.type = ref
2021-11-13 16:22:26.823 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 16:22:26.823 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 16:22:26.823 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 16:22:26.823 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 16:22:26.823 type = bert-base-cased
2021-11-13 16:22:26.823 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 16:22:26.823 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 16:22:26.824 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 16:22:26.824 dataset_reader.to_index = 6
2021-11-13 16:22:26.825 dataset_reader.type = example_reader
2021-11-13 16:22:26.825 dataset_reader.max_instances = None
2021-11-13 16:22:26.825 dataset_reader.manual_distributed_sharding = False
2021-11-13 16:22:26.825 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 16:22:26.825 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 16:22:26.825 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 16:22:26.825 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 16:22:26.825 type = bert-base-cased
2021-11-13 16:22:26.825 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 16:22:26.825 dataset_reader.tokenizer.max_length = 128
2021-11-13 16:22:26.825 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 16:22:26.826 dataset_reader.text_token_indexers.type = ref
2021-11-13 16:22:26.827 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 16:22:26.827 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 16:22:26.827 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 16:22:26.827 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 16:22:26.827 type = bert-base-cased
2021-11-13 16:22:26.827 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 16:22:26.827 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 16:22:26.827 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 16:22:26.828 dataset_reader.to_index = 6
2021-11-13 16:22:26.828 type = from_instances
2021-11-13 16:22:26.829 Loading token dictionary from /tmp/tmp1a5w8t_x/vocabulary.
2021-11-13 16:22:26.829 model.type = sentence_level_classifier
2021-11-13 16:22:26.829 model.embedder.type = ref
2021-11-13 16:22:26.830 model.embedder.type = basic
2021-11-13 16:22:26.830 model.embedder.token_embedders.type = ref
2021-11-13 16:22:26.832 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 16:22:26.832 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 16:22:26.832 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-13 16:22:26.832 type = bert-base-cased
2021-11-13 16:22:26.832 model.embedder.token_embedders.tokens.max_length = None
2021-11-13 16:22:26.832 model.embedder.token_embedders.tokens.sub_module = None
2021-11-13 16:22:26.832 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-13 16:22:26.832 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-13 16:22:26.832 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-13 16:22:26.832 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-13 16:22:26.832 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-13 16:22:26.832 model.embedder.token_embedders.tokens.load_weights = True
2021-11-13 16:22:26.832 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-13 16:22:26.832 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-13 16:22:26.832 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-13 16:22:26.873 model.encoder.type = bert_pooler
2021-11-13 16:22:26.873 model.encoder.type = bert_pooler
2021-11-13 16:22:26.874 model.encoder.pretrained_model = bert-base-cased
2021-11-13 16:22:26.874 type = bert-base-cased
2021-11-13 16:22:26.874 model.encoder.override_weights_file = None
2021-11-13 16:22:26.874 model.encoder.override_weights_strip_prefix = None
2021-11-13 16:22:26.874 model.encoder.load_weights = True
2021-11-13 16:22:26.874 model.encoder.requires_grad = True
2021-11-13 16:22:26.874 model.encoder.dropout = 0.0
2021-11-13 16:22:26.875 model.encoder.transformer_kwargs = None
2021-11-13 16:22:27.075 removing temporary unarchived model dir at /tmp/tmp1a5w8t_x
2021-11-13 16:22:30.717 Plugin allennlp_models available
2021-11-13 16:22:30.723 Plugin allennlp_server available
2021-11-13 16:22:30.724 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-13 16:22:30.724 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmputzkyqn7
2021-11-13 16:22:32.822 dataset_reader.type = example_reader
2021-11-13 16:22:32.822 dataset_reader.max_instances = None
2021-11-13 16:22:32.822 dataset_reader.manual_distributed_sharding = False
2021-11-13 16:22:32.822 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 16:22:32.822 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 16:22:32.822 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 16:22:32.822 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 16:22:32.822 type = bert-base-cased
2021-11-13 16:22:32.822 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 16:22:32.822 dataset_reader.tokenizer.max_length = 128
2021-11-13 16:22:32.822 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 16:22:32.824 dataset_reader.text_token_indexers.type = ref
2021-11-13 16:22:32.825 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 16:22:32.825 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 16:22:32.825 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 16:22:32.825 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 16:22:32.825 type = bert-base-cased
2021-11-13 16:22:32.826 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 16:22:32.826 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 16:22:32.826 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 16:22:32.826 dataset_reader.to_index = 6
2021-11-13 16:22:32.826 dataset_reader.type = example_reader
2021-11-13 16:22:32.827 dataset_reader.max_instances = None
2021-11-13 16:22:32.827 dataset_reader.manual_distributed_sharding = False
2021-11-13 16:22:32.827 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 16:22:32.827 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 16:22:32.827 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 16:22:32.827 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 16:22:32.828 type = bert-base-cased
2021-11-13 16:22:32.828 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 16:22:32.828 dataset_reader.tokenizer.max_length = 128
2021-11-13 16:22:32.828 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 16:22:32.829 dataset_reader.text_token_indexers.type = ref
2021-11-13 16:22:32.829 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 16:22:32.830 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 16:22:32.830 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 16:22:32.830 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 16:22:32.830 type = bert-base-cased
2021-11-13 16:22:32.830 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 16:22:32.830 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 16:22:32.830 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 16:22:32.831 dataset_reader.to_index = 6
2021-11-13 16:22:32.831 type = from_instances
2021-11-13 16:22:32.831 Loading token dictionary from /tmp/tmputzkyqn7/vocabulary.
2021-11-13 16:22:32.832 model.type = sentence_level_classifier
2021-11-13 16:22:32.832 model.embedder.type = ref
2021-11-13 16:22:32.832 model.embedder.type = basic
2021-11-13 16:22:32.833 model.embedder.token_embedders.type = ref
2021-11-13 16:22:32.833 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 16:22:32.834 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 16:22:32.834 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-13 16:22:32.834 type = bert-base-cased
2021-11-13 16:22:32.834 model.embedder.token_embedders.tokens.max_length = None
2021-11-13 16:22:32.834 model.embedder.token_embedders.tokens.sub_module = None
2021-11-13 16:22:32.835 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-13 16:22:32.835 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-13 16:22:32.835 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-13 16:22:32.835 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-13 16:22:32.835 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-13 16:22:32.835 model.embedder.token_embedders.tokens.load_weights = True
2021-11-13 16:22:32.835 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-13 16:22:32.835 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-13 16:22:32.835 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-13 16:22:32.878 model.encoder.type = bert_pooler
2021-11-13 16:22:32.879 model.encoder.type = bert_pooler
2021-11-13 16:22:32.879 model.encoder.pretrained_model = bert-base-cased
2021-11-13 16:22:32.879 type = bert-base-cased
2021-11-13 16:22:32.879 model.encoder.override_weights_file = None
2021-11-13 16:22:32.879 model.encoder.override_weights_strip_prefix = None
2021-11-13 16:22:32.879 model.encoder.load_weights = True
2021-11-13 16:22:32.880 model.encoder.requires_grad = True
2021-11-13 16:22:32.880 model.encoder.dropout = 0.0
2021-11-13 16:22:32.880 model.encoder.transformer_kwargs = None
2021-11-13 16:22:33.075 removing temporary unarchived model dir at /tmp/tmputzkyqn7
2021-11-13 16:28:16.052 Plugin allennlp_models available
2021-11-13 16:28:16.059 Plugin allennlp_server available
2021-11-13 16:28:16.059 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-13 16:28:16.060 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpbwr2g5ne
2021-11-13 16:28:18.159 dataset_reader.type = example_reader
2021-11-13 16:28:18.160 dataset_reader.max_instances = None
2021-11-13 16:28:18.160 dataset_reader.manual_distributed_sharding = False
2021-11-13 16:28:18.161 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 16:28:18.161 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 16:28:18.161 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 16:28:18.161 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 16:28:18.161 type = bert-base-cased
2021-11-13 16:28:18.161 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 16:28:18.161 dataset_reader.tokenizer.max_length = 128
2021-11-13 16:28:18.161 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 16:28:18.162 dataset_reader.text_token_indexers.type = ref
2021-11-13 16:28:18.163 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 16:28:18.163 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 16:28:18.164 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 16:28:18.164 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 16:28:18.164 type = bert-base-cased
2021-11-13 16:28:18.164 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 16:28:18.164 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 16:28:18.164 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 16:28:18.165 dataset_reader.to_index = 6
2021-11-13 16:28:18.165 dataset_reader.type = example_reader
2021-11-13 16:28:18.165 dataset_reader.max_instances = None
2021-11-13 16:28:18.166 dataset_reader.manual_distributed_sharding = False
2021-11-13 16:28:18.166 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 16:28:18.166 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 16:28:18.166 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 16:28:18.166 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 16:28:18.166 type = bert-base-cased
2021-11-13 16:28:18.166 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 16:28:18.166 dataset_reader.tokenizer.max_length = 128
2021-11-13 16:28:18.166 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 16:28:18.167 dataset_reader.text_token_indexers.type = ref
2021-11-13 16:28:18.168 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 16:28:18.168 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 16:28:18.168 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 16:28:18.168 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 16:28:18.169 type = bert-base-cased
2021-11-13 16:28:18.169 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 16:28:18.169 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 16:28:18.169 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 16:28:18.170 dataset_reader.to_index = 6
2021-11-13 16:28:18.170 type = from_instances
2021-11-13 16:28:18.170 Loading token dictionary from /tmp/tmpbwr2g5ne/vocabulary.
2021-11-13 16:28:18.171 model.type = sentence_level_classifier
2021-11-13 16:28:18.171 model.embedder.type = ref
2021-11-13 16:28:18.171 model.embedder.type = basic
2021-11-13 16:28:18.172 model.embedder.token_embedders.type = ref
2021-11-13 16:28:18.172 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 16:28:18.173 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 16:28:18.173 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-13 16:28:18.173 type = bert-base-cased
2021-11-13 16:28:18.173 model.embedder.token_embedders.tokens.max_length = None
2021-11-13 16:28:18.173 model.embedder.token_embedders.tokens.sub_module = None
2021-11-13 16:28:18.173 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-13 16:28:18.173 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-13 16:28:18.173 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-13 16:28:18.173 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-13 16:28:18.173 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-13 16:28:18.173 model.embedder.token_embedders.tokens.load_weights = True
2021-11-13 16:28:18.173 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-13 16:28:18.173 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-13 16:28:18.173 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-13 16:28:18.225 model.encoder.type = bert_pooler
2021-11-13 16:28:18.225 model.encoder.type = bert_pooler
2021-11-13 16:28:18.225 model.encoder.pretrained_model = bert-base-cased
2021-11-13 16:28:18.226 type = bert-base-cased
2021-11-13 16:28:18.226 model.encoder.override_weights_file = None
2021-11-13 16:28:18.226 model.encoder.override_weights_strip_prefix = None
2021-11-13 16:28:18.226 model.encoder.load_weights = True
2021-11-13 16:28:18.226 model.encoder.requires_grad = True
2021-11-13 16:28:18.226 model.encoder.dropout = 0.0
2021-11-13 16:28:18.226 model.encoder.transformer_kwargs = None
2021-11-13 16:28:18.422 removing temporary unarchived model dir at /tmp/tmpbwr2g5ne
2021-11-13 16:30:06.743 Plugin allennlp_models available
2021-11-13 16:30:06.754 Plugin allennlp_server available
2021-11-13 16:30:06.755 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-13 16:30:06.755 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpwq307pfj
2021-11-13 16:30:08.865 dataset_reader.type = example_reader
2021-11-13 16:30:08.866 dataset_reader.max_instances = None
2021-11-13 16:30:08.866 dataset_reader.manual_distributed_sharding = False
2021-11-13 16:30:08.866 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 16:30:08.866 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 16:30:08.866 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 16:30:08.866 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 16:30:08.866 type = bert-base-cased
2021-11-13 16:30:08.866 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 16:30:08.866 dataset_reader.tokenizer.max_length = 128
2021-11-13 16:30:08.866 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 16:30:08.868 dataset_reader.text_token_indexers.type = ref
2021-11-13 16:30:08.869 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 16:30:08.869 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 16:30:08.869 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 16:30:08.869 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 16:30:08.869 type = bert-base-cased
2021-11-13 16:30:08.869 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 16:30:08.869 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 16:30:08.869 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 16:30:08.870 dataset_reader.to_index = 6
2021-11-13 16:30:08.870 dataset_reader.type = example_reader
2021-11-13 16:30:08.870 dataset_reader.max_instances = None
2021-11-13 16:30:08.870 dataset_reader.manual_distributed_sharding = False
2021-11-13 16:30:08.870 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 16:30:08.871 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 16:30:08.871 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 16:30:08.871 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 16:30:08.871 type = bert-base-cased
2021-11-13 16:30:08.871 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 16:30:08.871 dataset_reader.tokenizer.max_length = 128
2021-11-13 16:30:08.871 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 16:30:08.872 dataset_reader.text_token_indexers.type = ref
2021-11-13 16:30:08.872 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 16:30:08.873 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 16:30:08.873 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 16:30:08.873 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 16:30:08.873 type = bert-base-cased
2021-11-13 16:30:08.873 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 16:30:08.873 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 16:30:08.873 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 16:30:08.874 dataset_reader.to_index = 6
2021-11-13 16:30:08.874 type = from_instances
2021-11-13 16:30:08.874 Loading token dictionary from /tmp/tmpwq307pfj/vocabulary.
2021-11-13 16:30:08.875 model.type = sentence_level_classifier
2021-11-13 16:30:08.875 model.embedder.type = ref
2021-11-13 16:30:08.876 model.embedder.type = basic
2021-11-13 16:30:08.876 model.embedder.token_embedders.type = ref
2021-11-13 16:30:08.877 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 16:30:08.877 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 16:30:08.877 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-13 16:30:08.877 type = bert-base-cased
2021-11-13 16:30:08.877 model.embedder.token_embedders.tokens.max_length = None
2021-11-13 16:30:08.877 model.embedder.token_embedders.tokens.sub_module = None
2021-11-13 16:30:08.877 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-13 16:30:08.877 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-13 16:30:08.878 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-13 16:30:08.878 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-13 16:30:08.878 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-13 16:30:08.878 model.embedder.token_embedders.tokens.load_weights = True
2021-11-13 16:30:08.878 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-13 16:30:08.878 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-13 16:30:08.878 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-13 16:30:08.919 model.encoder.type = bert_pooler
2021-11-13 16:30:08.919 model.encoder.type = bert_pooler
2021-11-13 16:30:08.920 model.encoder.pretrained_model = bert-base-cased
2021-11-13 16:30:08.920 type = bert-base-cased
2021-11-13 16:30:08.920 model.encoder.override_weights_file = None
2021-11-13 16:30:08.920 model.encoder.override_weights_strip_prefix = None
2021-11-13 16:30:08.920 model.encoder.load_weights = True
2021-11-13 16:30:08.920 model.encoder.requires_grad = True
2021-11-13 16:30:08.920 model.encoder.dropout = 0.0
2021-11-13 16:30:08.920 model.encoder.transformer_kwargs = None
2021-11-13 16:30:09.143 removing temporary unarchived model dir at /tmp/tmpwq307pfj
2021-11-13 17:02:04.192 Plugin allennlp_models available
2021-11-13 17:02:04.212 Plugin allennlp_server available
2021-11-13 17:02:04.213 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-13 17:02:04.214 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpc755r3o0
2021-11-13 17:02:06.327 dataset_reader.type = example_reader
2021-11-13 17:02:06.328 dataset_reader.max_instances = None
2021-11-13 17:02:06.328 dataset_reader.manual_distributed_sharding = False
2021-11-13 17:02:06.328 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 17:02:06.328 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 17:02:06.328 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 17:02:06.328 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 17:02:06.328 type = bert-base-cased
2021-11-13 17:02:06.328 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 17:02:06.328 dataset_reader.tokenizer.max_length = 128
2021-11-13 17:02:06.328 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 17:02:06.329 dataset_reader.text_token_indexers.type = ref
2021-11-13 17:02:06.331 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 17:02:06.331 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 17:02:06.331 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 17:02:06.331 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 17:02:06.331 type = bert-base-cased
2021-11-13 17:02:06.331 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 17:02:06.331 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 17:02:06.331 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 17:02:06.332 dataset_reader.to_index = 6
2021-11-13 17:02:06.332 dataset_reader.type = example_reader
2021-11-13 17:02:06.332 dataset_reader.max_instances = None
2021-11-13 17:02:06.332 dataset_reader.manual_distributed_sharding = False
2021-11-13 17:02:06.333 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 17:02:06.333 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 17:02:06.333 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 17:02:06.333 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 17:02:06.333 type = bert-base-cased
2021-11-13 17:02:06.333 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 17:02:06.333 dataset_reader.tokenizer.max_length = 128
2021-11-13 17:02:06.333 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 17:02:06.334 dataset_reader.text_token_indexers.type = ref
2021-11-13 17:02:06.334 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 17:02:06.335 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 17:02:06.335 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 17:02:06.335 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 17:02:06.335 type = bert-base-cased
2021-11-13 17:02:06.335 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 17:02:06.335 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 17:02:06.335 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 17:02:06.336 dataset_reader.to_index = 6
2021-11-13 17:02:06.336 type = from_instances
2021-11-13 17:02:06.337 Loading token dictionary from /tmp/tmpc755r3o0/vocabulary.
2021-11-13 17:02:06.337 model.type = sentence_level_classifier
2021-11-13 17:02:06.338 model.embedder.type = ref
2021-11-13 17:02:06.340 model.embedder.type = basic
2021-11-13 17:02:06.340 model.embedder.token_embedders.type = ref
2021-11-13 17:02:06.342 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 17:02:06.342 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 17:02:06.343 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-13 17:02:06.343 type = bert-base-cased
2021-11-13 17:02:06.344 model.embedder.token_embedders.tokens.max_length = None
2021-11-13 17:02:06.344 model.embedder.token_embedders.tokens.sub_module = None
2021-11-13 17:02:06.344 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-13 17:02:06.344 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-13 17:02:06.345 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-13 17:02:06.345 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-13 17:02:06.345 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-13 17:02:06.345 model.embedder.token_embedders.tokens.load_weights = True
2021-11-13 17:02:06.345 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-13 17:02:06.346 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-13 17:02:06.346 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-13 17:02:06.396 model.encoder.type = bert_pooler
2021-11-13 17:02:06.396 model.encoder.type = bert_pooler
2021-11-13 17:02:06.396 model.encoder.pretrained_model = bert-base-cased
2021-11-13 17:02:06.396 type = bert-base-cased
2021-11-13 17:02:06.396 model.encoder.override_weights_file = None
2021-11-13 17:02:06.396 model.encoder.override_weights_strip_prefix = None
2021-11-13 17:02:06.396 model.encoder.load_weights = True
2021-11-13 17:02:06.396 model.encoder.requires_grad = True
2021-11-13 17:02:06.396 model.encoder.dropout = 0.0
2021-11-13 17:02:06.396 model.encoder.transformer_kwargs = None
2021-11-13 17:02:06.610 removing temporary unarchived model dir at /tmp/tmpc755r3o0
2021-11-13 17:36:08.302 Plugin allennlp_models available
2021-11-13 17:36:08.311 Plugin allennlp_server available
2021-11-13 17:36:08.312 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-13 17:36:08.312 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpolu3lam6
2021-11-13 17:36:10.424 dataset_reader.type = example_reader
2021-11-13 17:36:10.424 dataset_reader.max_instances = None
2021-11-13 17:36:10.424 dataset_reader.manual_distributed_sharding = False
2021-11-13 17:36:10.424 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 17:36:10.424 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 17:36:10.424 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 17:36:10.424 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 17:36:10.424 type = bert-base-cased
2021-11-13 17:36:10.425 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 17:36:10.425 dataset_reader.tokenizer.max_length = 128
2021-11-13 17:36:10.425 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 17:36:10.426 dataset_reader.text_token_indexers.type = ref
2021-11-13 17:36:10.427 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 17:36:10.427 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 17:36:10.428 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 17:36:10.428 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 17:36:10.428 type = bert-base-cased
2021-11-13 17:36:10.428 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 17:36:10.428 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 17:36:10.428 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 17:36:10.429 dataset_reader.to_index = 6
2021-11-13 17:36:10.430 dataset_reader.type = example_reader
2021-11-13 17:36:10.430 dataset_reader.max_instances = None
2021-11-13 17:36:10.431 dataset_reader.manual_distributed_sharding = False
2021-11-13 17:36:10.431 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 17:36:10.431 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 17:36:10.432 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 17:36:10.432 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 17:36:10.432 type = bert-base-cased
2021-11-13 17:36:10.433 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 17:36:10.433 dataset_reader.tokenizer.max_length = 128
2021-11-13 17:36:10.433 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 17:36:10.435 dataset_reader.text_token_indexers.type = ref
2021-11-13 17:36:10.437 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 17:36:10.437 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 17:36:10.438 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 17:36:10.438 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 17:36:10.438 type = bert-base-cased
2021-11-13 17:36:10.439 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 17:36:10.439 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 17:36:10.439 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 17:36:10.440 dataset_reader.to_index = 6
2021-11-13 17:36:10.441 type = from_instances
2021-11-13 17:36:10.441 Loading token dictionary from /tmp/tmpolu3lam6/vocabulary.
2021-11-13 17:36:10.442 model.type = sentence_level_classifier
2021-11-13 17:36:10.443 model.embedder.type = ref
2021-11-13 17:36:10.444 model.embedder.type = basic
2021-11-13 17:36:10.445 model.embedder.token_embedders.type = ref
2021-11-13 17:36:10.447 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 17:36:10.447 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 17:36:10.448 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-13 17:36:10.448 type = bert-base-cased
2021-11-13 17:36:10.449 model.embedder.token_embedders.tokens.max_length = None
2021-11-13 17:36:10.449 model.embedder.token_embedders.tokens.sub_module = None
2021-11-13 17:36:10.449 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-13 17:36:10.449 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-13 17:36:10.449 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-13 17:36:10.450 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-13 17:36:10.450 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-13 17:36:10.450 model.embedder.token_embedders.tokens.load_weights = True
2021-11-13 17:36:10.450 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-13 17:36:10.450 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-13 17:36:10.450 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-13 17:36:10.503 model.encoder.type = bert_pooler
2021-11-13 17:36:10.504 model.encoder.type = bert_pooler
2021-11-13 17:36:10.504 model.encoder.pretrained_model = bert-base-cased
2021-11-13 17:36:10.504 type = bert-base-cased
2021-11-13 17:36:10.504 model.encoder.override_weights_file = None
2021-11-13 17:36:10.504 model.encoder.override_weights_strip_prefix = None
2021-11-13 17:36:10.504 model.encoder.load_weights = True
2021-11-13 17:36:10.504 model.encoder.requires_grad = True
2021-11-13 17:36:10.505 model.encoder.dropout = 0.0
2021-11-13 17:36:10.505 model.encoder.transformer_kwargs = None
2021-11-13 17:36:10.698 removing temporary unarchived model dir at /tmp/tmpolu3lam6
2021-11-13 18:45:40.821 Plugin allennlp_models available
2021-11-13 18:45:40.831 Plugin allennlp_server available
2021-11-13 18:45:40.832 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-13 18:45:40.832 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpxg5ypk2z
2021-11-13 18:45:42.949 dataset_reader.type = example_reader
2021-11-13 18:45:42.949 dataset_reader.max_instances = None
2021-11-13 18:45:42.949 dataset_reader.manual_distributed_sharding = False
2021-11-13 18:45:42.949 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 18:45:42.949 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 18:45:42.949 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 18:45:42.949 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 18:45:42.950 type = bert-base-cased
2021-11-13 18:45:42.950 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 18:45:42.950 dataset_reader.tokenizer.max_length = 128
2021-11-13 18:45:42.950 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 18:45:42.951 dataset_reader.text_token_indexers.type = ref
2021-11-13 18:45:42.952 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 18:45:42.952 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 18:45:42.952 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 18:45:42.952 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 18:45:42.952 type = bert-base-cased
2021-11-13 18:45:42.952 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 18:45:42.952 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 18:45:42.952 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 18:45:42.953 dataset_reader.to_index = 6
2021-11-13 18:45:42.954 dataset_reader.type = example_reader
2021-11-13 18:45:42.954 dataset_reader.max_instances = None
2021-11-13 18:45:42.954 dataset_reader.manual_distributed_sharding = False
2021-11-13 18:45:42.954 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 18:45:42.954 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 18:45:42.954 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 18:45:42.954 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 18:45:42.954 type = bert-base-cased
2021-11-13 18:45:42.954 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 18:45:42.954 dataset_reader.tokenizer.max_length = 128
2021-11-13 18:45:42.954 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 18:45:42.955 dataset_reader.text_token_indexers.type = ref
2021-11-13 18:45:42.956 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 18:45:42.956 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 18:45:42.956 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 18:45:42.956 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 18:45:42.956 type = bert-base-cased
2021-11-13 18:45:42.956 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 18:45:42.956 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 18:45:42.956 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 18:45:42.957 dataset_reader.to_index = 6
2021-11-13 18:45:42.957 type = from_instances
2021-11-13 18:45:42.957 Loading token dictionary from /tmp/tmpxg5ypk2z/vocabulary.
2021-11-13 18:45:42.958 model.type = sentence_level_classifier
2021-11-13 18:45:42.958 model.embedder.type = ref
2021-11-13 18:45:42.959 model.embedder.type = basic
2021-11-13 18:45:42.959 model.embedder.token_embedders.type = ref
2021-11-13 18:45:42.959 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 18:45:42.960 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 18:45:42.960 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-13 18:45:42.960 type = bert-base-cased
2021-11-13 18:45:42.960 model.embedder.token_embedders.tokens.max_length = None
2021-11-13 18:45:42.960 model.embedder.token_embedders.tokens.sub_module = None
2021-11-13 18:45:42.960 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-13 18:45:42.960 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-13 18:45:42.960 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-13 18:45:42.960 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-13 18:45:42.960 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-13 18:45:42.960 model.embedder.token_embedders.tokens.load_weights = True
2021-11-13 18:45:42.960 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-13 18:45:42.960 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-13 18:45:42.960 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-13 18:45:43.010 model.encoder.type = bert_pooler
2021-11-13 18:45:43.010 model.encoder.type = bert_pooler
2021-11-13 18:45:43.010 model.encoder.pretrained_model = bert-base-cased
2021-11-13 18:45:43.010 type = bert-base-cased
2021-11-13 18:45:43.010 model.encoder.override_weights_file = None
2021-11-13 18:45:43.010 model.encoder.override_weights_strip_prefix = None
2021-11-13 18:45:43.010 model.encoder.load_weights = True
2021-11-13 18:45:43.010 model.encoder.requires_grad = True
2021-11-13 18:45:43.010 model.encoder.dropout = 0.0
2021-11-13 18:45:43.010 model.encoder.transformer_kwargs = None
2021-11-13 18:45:43.209 removing temporary unarchived model dir at /tmp/tmpxg5ypk2z
2021-11-13 19:47:12.745 Plugin allennlp_models available
2021-11-13 19:47:12.754 Plugin allennlp_server available
2021-11-13 19:47:12.755 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-13 19:47:12.755 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmps7w751lk
2021-11-13 19:47:14.881 dataset_reader.type = example_reader
2021-11-13 19:47:14.881 dataset_reader.max_instances = None
2021-11-13 19:47:14.881 dataset_reader.manual_distributed_sharding = False
2021-11-13 19:47:14.881 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 19:47:14.881 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 19:47:14.881 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 19:47:14.881 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 19:47:14.881 type = bert-base-cased
2021-11-13 19:47:14.881 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 19:47:14.881 dataset_reader.tokenizer.max_length = 128
2021-11-13 19:47:14.882 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 19:47:14.883 dataset_reader.text_token_indexers.type = ref
2021-11-13 19:47:14.884 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 19:47:14.884 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 19:47:14.884 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 19:47:14.884 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 19:47:14.884 type = bert-base-cased
2021-11-13 19:47:14.884 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 19:47:14.884 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 19:47:14.884 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 19:47:14.885 dataset_reader.to_index = 6
2021-11-13 19:47:14.885 dataset_reader.type = example_reader
2021-11-13 19:47:14.886 dataset_reader.max_instances = None
2021-11-13 19:47:14.886 dataset_reader.manual_distributed_sharding = False
2021-11-13 19:47:14.886 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 19:47:14.886 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 19:47:14.886 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 19:47:14.886 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 19:47:14.886 type = bert-base-cased
2021-11-13 19:47:14.886 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 19:47:14.886 dataset_reader.tokenizer.max_length = 128
2021-11-13 19:47:14.886 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 19:47:14.887 dataset_reader.text_token_indexers.type = ref
2021-11-13 19:47:14.888 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 19:47:14.888 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 19:47:14.888 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 19:47:14.888 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 19:47:14.888 type = bert-base-cased
2021-11-13 19:47:14.888 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 19:47:14.888 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 19:47:14.888 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 19:47:14.889 dataset_reader.to_index = 6
2021-11-13 19:47:14.889 type = from_instances
2021-11-13 19:47:14.889 Loading token dictionary from /tmp/tmps7w751lk/vocabulary.
2021-11-13 19:47:14.890 model.type = sentence_level_classifier
2021-11-13 19:47:14.890 model.embedder.type = ref
2021-11-13 19:47:14.891 model.embedder.type = basic
2021-11-13 19:47:14.891 model.embedder.token_embedders.type = ref
2021-11-13 19:47:14.892 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 19:47:14.892 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 19:47:14.892 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-13 19:47:14.892 type = bert-base-cased
2021-11-13 19:47:14.892 model.embedder.token_embedders.tokens.max_length = None
2021-11-13 19:47:14.892 model.embedder.token_embedders.tokens.sub_module = None
2021-11-13 19:47:14.892 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-13 19:47:14.892 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-13 19:47:14.892 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-13 19:47:14.892 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-13 19:47:14.892 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-13 19:47:14.892 model.embedder.token_embedders.tokens.load_weights = True
2021-11-13 19:47:14.893 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-13 19:47:14.893 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-13 19:47:14.893 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-13 19:47:14.935 model.encoder.type = bert_pooler
2021-11-13 19:47:14.935 model.encoder.type = bert_pooler
2021-11-13 19:47:14.935 model.encoder.pretrained_model = bert-base-cased
2021-11-13 19:47:14.935 type = bert-base-cased
2021-11-13 19:47:14.935 model.encoder.override_weights_file = None
2021-11-13 19:47:14.935 model.encoder.override_weights_strip_prefix = None
2021-11-13 19:47:14.935 model.encoder.load_weights = True
2021-11-13 19:47:14.935 model.encoder.requires_grad = True
2021-11-13 19:47:14.935 model.encoder.dropout = 0.0
2021-11-13 19:47:14.935 model.encoder.transformer_kwargs = None
2021-11-13 19:47:15.137 removing temporary unarchived model dir at /tmp/tmps7w751lk
2021-11-13 19:49:28.618 Plugin allennlp_models available
2021-11-13 19:49:28.629 Plugin allennlp_server available
2021-11-13 19:49:28.630 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-13 19:49:28.630 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmptbzzlmvo
2021-11-13 19:49:30.741 dataset_reader.type = example_reader
2021-11-13 19:49:30.741 dataset_reader.max_instances = None
2021-11-13 19:49:30.741 dataset_reader.manual_distributed_sharding = False
2021-11-13 19:49:30.741 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 19:49:30.741 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 19:49:30.741 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 19:49:30.742 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 19:49:30.742 type = bert-base-cased
2021-11-13 19:49:30.742 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 19:49:30.742 dataset_reader.tokenizer.max_length = 128
2021-11-13 19:49:30.742 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 19:49:30.743 dataset_reader.text_token_indexers.type = ref
2021-11-13 19:49:30.744 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 19:49:30.744 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 19:49:30.745 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 19:49:30.745 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 19:49:30.745 type = bert-base-cased
2021-11-13 19:49:30.745 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 19:49:30.745 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 19:49:30.745 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 19:49:30.746 dataset_reader.to_index = 6
2021-11-13 19:49:30.746 dataset_reader.type = example_reader
2021-11-13 19:49:30.746 dataset_reader.max_instances = None
2021-11-13 19:49:30.746 dataset_reader.manual_distributed_sharding = False
2021-11-13 19:49:30.746 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 19:49:30.746 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 19:49:30.746 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 19:49:30.746 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 19:49:30.747 type = bert-base-cased
2021-11-13 19:49:30.747 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 19:49:30.747 dataset_reader.tokenizer.max_length = 128
2021-11-13 19:49:30.747 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 19:49:30.747 dataset_reader.text_token_indexers.type = ref
2021-11-13 19:49:30.748 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 19:49:30.748 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 19:49:30.748 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 19:49:30.748 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 19:49:30.749 type = bert-base-cased
2021-11-13 19:49:30.749 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 19:49:30.749 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 19:49:30.749 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 19:49:30.749 dataset_reader.to_index = 6
2021-11-13 19:49:30.750 type = from_instances
2021-11-13 19:49:30.750 Loading token dictionary from /tmp/tmptbzzlmvo/vocabulary.
2021-11-13 19:49:30.750 model.type = sentence_level_classifier
2021-11-13 19:49:30.750 model.embedder.type = ref
2021-11-13 19:49:30.751 model.embedder.type = basic
2021-11-13 19:49:30.751 model.embedder.token_embedders.type = ref
2021-11-13 19:49:30.752 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 19:49:30.752 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 19:49:30.753 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-13 19:49:30.753 type = bert-base-cased
2021-11-13 19:49:30.754 model.embedder.token_embedders.tokens.max_length = None
2021-11-13 19:49:30.754 model.embedder.token_embedders.tokens.sub_module = None
2021-11-13 19:49:30.754 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-13 19:49:30.754 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-13 19:49:30.754 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-13 19:49:30.754 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-13 19:49:30.755 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-13 19:49:30.755 model.embedder.token_embedders.tokens.load_weights = True
2021-11-13 19:49:30.755 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-13 19:49:30.755 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-13 19:49:30.755 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-13 19:49:30.808 model.encoder.type = bert_pooler
2021-11-13 19:49:30.808 model.encoder.type = bert_pooler
2021-11-13 19:49:30.808 model.encoder.pretrained_model = bert-base-cased
2021-11-13 19:49:30.808 type = bert-base-cased
2021-11-13 19:49:30.808 model.encoder.override_weights_file = None
2021-11-13 19:49:30.808 model.encoder.override_weights_strip_prefix = None
2021-11-13 19:49:30.808 model.encoder.load_weights = True
2021-11-13 19:49:30.808 model.encoder.requires_grad = True
2021-11-13 19:49:30.808 model.encoder.dropout = 0.0
2021-11-13 19:49:30.808 model.encoder.transformer_kwargs = None
2021-11-13 19:49:31.005 removing temporary unarchived model dir at /tmp/tmptbzzlmvo
2021-11-13 19:56:20.151 Plugin allennlp_models available
2021-11-13 19:56:20.159 Plugin allennlp_server available
2021-11-13 19:56:20.160 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-13 19:56:20.161 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpqa6whpev
2021-11-13 19:56:22.284 dataset_reader.type = example_reader
2021-11-13 19:56:22.284 dataset_reader.max_instances = None
2021-11-13 19:56:22.284 dataset_reader.manual_distributed_sharding = False
2021-11-13 19:56:22.284 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 19:56:22.285 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 19:56:22.285 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 19:56:22.285 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 19:56:22.285 type = bert-base-cased
2021-11-13 19:56:22.285 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 19:56:22.285 dataset_reader.tokenizer.max_length = 128
2021-11-13 19:56:22.285 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 19:56:22.286 dataset_reader.text_token_indexers.type = ref
2021-11-13 19:56:22.287 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 19:56:22.287 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 19:56:22.287 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 19:56:22.288 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 19:56:22.288 type = bert-base-cased
2021-11-13 19:56:22.288 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 19:56:22.288 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 19:56:22.288 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 19:56:22.289 dataset_reader.to_index = 6
2021-11-13 19:56:22.289 dataset_reader.type = example_reader
2021-11-13 19:56:22.289 dataset_reader.max_instances = None
2021-11-13 19:56:22.289 dataset_reader.manual_distributed_sharding = False
2021-11-13 19:56:22.289 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 19:56:22.289 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 19:56:22.289 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 19:56:22.289 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 19:56:22.290 type = bert-base-cased
2021-11-13 19:56:22.290 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 19:56:22.290 dataset_reader.tokenizer.max_length = 128
2021-11-13 19:56:22.290 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 19:56:22.290 dataset_reader.text_token_indexers.type = ref
2021-11-13 19:56:22.292 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 19:56:22.292 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 19:56:22.292 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 19:56:22.292 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 19:56:22.292 type = bert-base-cased
2021-11-13 19:56:22.292 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 19:56:22.292 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 19:56:22.292 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 19:56:22.293 dataset_reader.to_index = 6
2021-11-13 19:56:22.293 type = from_instances
2021-11-13 19:56:22.293 Loading token dictionary from /tmp/tmpqa6whpev/vocabulary.
2021-11-13 19:56:22.294 model.type = sentence_level_classifier
2021-11-13 19:56:22.294 model.embedder.type = ref
2021-11-13 19:56:22.295 model.embedder.type = basic
2021-11-13 19:56:22.295 model.embedder.token_embedders.type = ref
2021-11-13 19:56:22.295 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 19:56:22.296 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 19:56:22.296 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-13 19:56:22.296 type = bert-base-cased
2021-11-13 19:56:22.296 model.embedder.token_embedders.tokens.max_length = None
2021-11-13 19:56:22.296 model.embedder.token_embedders.tokens.sub_module = None
2021-11-13 19:56:22.296 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-13 19:56:22.296 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-13 19:56:22.296 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-13 19:56:22.296 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-13 19:56:22.296 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-13 19:56:22.296 model.embedder.token_embedders.tokens.load_weights = True
2021-11-13 19:56:22.296 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-13 19:56:22.296 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-13 19:56:22.296 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-13 19:56:22.345 model.encoder.type = bert_pooler
2021-11-13 19:56:22.345 model.encoder.type = bert_pooler
2021-11-13 19:56:22.345 model.encoder.pretrained_model = bert-base-cased
2021-11-13 19:56:22.346 type = bert-base-cased
2021-11-13 19:56:22.346 model.encoder.override_weights_file = None
2021-11-13 19:56:22.346 model.encoder.override_weights_strip_prefix = None
2021-11-13 19:56:22.346 model.encoder.load_weights = True
2021-11-13 19:56:22.346 model.encoder.requires_grad = True
2021-11-13 19:56:22.346 model.encoder.dropout = 0.0
2021-11-13 19:56:22.346 model.encoder.transformer_kwargs = None
2021-11-13 19:56:22.545 removing temporary unarchived model dir at /tmp/tmpqa6whpev
2021-11-13 22:44:21.871 Plugin allennlp_models available
2021-11-13 22:44:21.878 Plugin allennlp_server available
2021-11-13 22:44:21.879 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-13 22:44:21.879 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp71nynt03
2021-11-13 22:44:23.989 dataset_reader.type = example_reader
2021-11-13 22:44:23.989 dataset_reader.max_instances = None
2021-11-13 22:44:23.989 dataset_reader.manual_distributed_sharding = False
2021-11-13 22:44:23.989 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 22:44:23.989 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 22:44:23.989 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 22:44:23.989 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 22:44:23.989 type = bert-base-cased
2021-11-13 22:44:23.990 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 22:44:23.990 dataset_reader.tokenizer.max_length = 128
2021-11-13 22:44:23.990 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 22:44:23.991 dataset_reader.text_token_indexers.type = ref
2021-11-13 22:44:23.992 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 22:44:23.992 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 22:44:23.992 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 22:44:23.992 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 22:44:23.992 type = bert-base-cased
2021-11-13 22:44:23.992 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 22:44:23.992 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 22:44:23.992 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 22:44:23.993 dataset_reader.to_index = 6
2021-11-13 22:44:23.994 dataset_reader.type = example_reader
2021-11-13 22:44:23.994 dataset_reader.max_instances = None
2021-11-13 22:44:23.994 dataset_reader.manual_distributed_sharding = False
2021-11-13 22:44:23.994 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 22:44:23.994 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 22:44:23.994 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 22:44:23.994 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 22:44:23.994 type = bert-base-cased
2021-11-13 22:44:23.994 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 22:44:23.994 dataset_reader.tokenizer.max_length = 128
2021-11-13 22:44:23.994 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 22:44:23.995 dataset_reader.text_token_indexers.type = ref
2021-11-13 22:44:23.996 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 22:44:23.997 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 22:44:23.997 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 22:44:23.997 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 22:44:23.997 type = bert-base-cased
2021-11-13 22:44:23.997 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 22:44:23.997 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 22:44:23.997 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 22:44:23.998 dataset_reader.to_index = 6
2021-11-13 22:44:23.998 type = from_instances
2021-11-13 22:44:23.998 Loading token dictionary from /tmp/tmp71nynt03/vocabulary.
2021-11-13 22:44:23.999 model.type = sentence_level_classifier
2021-11-13 22:44:23.999 model.embedder.type = ref
2021-11-13 22:44:24.000 model.embedder.type = basic
2021-11-13 22:44:24.000 model.embedder.token_embedders.type = ref
2021-11-13 22:44:24.000 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 22:44:24.001 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 22:44:24.001 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-13 22:44:24.001 type = bert-base-cased
2021-11-13 22:44:24.001 model.embedder.token_embedders.tokens.max_length = None
2021-11-13 22:44:24.001 model.embedder.token_embedders.tokens.sub_module = None
2021-11-13 22:44:24.001 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-13 22:44:24.001 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-13 22:44:24.001 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-13 22:44:24.001 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-13 22:44:24.001 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-13 22:44:24.001 model.embedder.token_embedders.tokens.load_weights = True
2021-11-13 22:44:24.002 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-13 22:44:24.002 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-13 22:44:24.002 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-13 22:44:24.050 model.encoder.type = bert_pooler
2021-11-13 22:44:24.050 model.encoder.type = bert_pooler
2021-11-13 22:44:24.050 model.encoder.pretrained_model = bert-base-cased
2021-11-13 22:44:24.050 type = bert-base-cased
2021-11-13 22:44:24.050 model.encoder.override_weights_file = None
2021-11-13 22:44:24.050 model.encoder.override_weights_strip_prefix = None
2021-11-13 22:44:24.050 model.encoder.load_weights = True
2021-11-13 22:44:24.050 model.encoder.requires_grad = True
2021-11-13 22:44:24.050 model.encoder.dropout = 0.0
2021-11-13 22:44:24.051 model.encoder.transformer_kwargs = None
2021-11-13 22:44:24.249 removing temporary unarchived model dir at /tmp/tmp71nynt03
2021-11-13 23:57:45.796 Plugin allennlp_models available
2021-11-13 23:57:45.807 Plugin allennlp_server available
2021-11-13 23:57:45.808 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-13 23:57:45.808 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmps0t0m_ac
2021-11-13 23:57:47.924 dataset_reader.type = example_reader
2021-11-13 23:57:47.925 dataset_reader.max_instances = None
2021-11-13 23:57:47.925 dataset_reader.manual_distributed_sharding = False
2021-11-13 23:57:47.925 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 23:57:47.925 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 23:57:47.925 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 23:57:47.925 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 23:57:47.925 type = bert-base-cased
2021-11-13 23:57:47.925 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 23:57:47.925 dataset_reader.tokenizer.max_length = 128
2021-11-13 23:57:47.925 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 23:57:47.927 dataset_reader.text_token_indexers.type = ref
2021-11-13 23:57:47.927 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 23:57:47.928 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 23:57:47.928 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 23:57:47.928 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 23:57:47.928 type = bert-base-cased
2021-11-13 23:57:47.928 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 23:57:47.928 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 23:57:47.928 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 23:57:47.929 dataset_reader.to_index = 6
2021-11-13 23:57:47.930 dataset_reader.type = example_reader
2021-11-13 23:57:47.930 dataset_reader.max_instances = None
2021-11-13 23:57:47.930 dataset_reader.manual_distributed_sharding = False
2021-11-13 23:57:47.931 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 23:57:47.931 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 23:57:47.931 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 23:57:47.932 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 23:57:47.932 type = bert-base-cased
2021-11-13 23:57:47.932 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 23:57:47.933 dataset_reader.tokenizer.max_length = 128
2021-11-13 23:57:47.933 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 23:57:47.935 dataset_reader.text_token_indexers.type = ref
2021-11-13 23:57:47.936 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 23:57:47.937 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 23:57:47.937 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 23:57:47.937 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 23:57:47.938 type = bert-base-cased
2021-11-13 23:57:47.938 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 23:57:47.938 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 23:57:47.938 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 23:57:47.940 dataset_reader.to_index = 6
2021-11-13 23:57:47.940 type = from_instances
2021-11-13 23:57:47.941 Loading token dictionary from /tmp/tmps0t0m_ac/vocabulary.
2021-11-13 23:57:47.941 model.type = sentence_level_classifier
2021-11-13 23:57:47.942 model.embedder.type = ref
2021-11-13 23:57:47.944 model.embedder.type = basic
2021-11-13 23:57:47.945 model.embedder.token_embedders.type = ref
2021-11-13 23:57:47.947 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 23:57:47.947 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 23:57:47.948 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-13 23:57:47.948 type = bert-base-cased
2021-11-13 23:57:47.948 model.embedder.token_embedders.tokens.max_length = None
2021-11-13 23:57:47.949 model.embedder.token_embedders.tokens.sub_module = None
2021-11-13 23:57:47.949 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-13 23:57:47.949 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-13 23:57:47.949 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-13 23:57:47.949 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-13 23:57:47.949 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-13 23:57:47.950 model.embedder.token_embedders.tokens.load_weights = True
2021-11-13 23:57:47.950 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-13 23:57:47.950 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-13 23:57:47.950 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-13 23:57:47.999 model.encoder.type = bert_pooler
2021-11-13 23:57:47.999 model.encoder.type = bert_pooler
2021-11-13 23:57:47.999 model.encoder.pretrained_model = bert-base-cased
2021-11-13 23:57:47.999 type = bert-base-cased
2021-11-13 23:57:48.000 model.encoder.override_weights_file = None
2021-11-13 23:57:48.000 model.encoder.override_weights_strip_prefix = None
2021-11-13 23:57:48.000 model.encoder.load_weights = True
2021-11-13 23:57:48.000 model.encoder.requires_grad = True
2021-11-13 23:57:48.000 model.encoder.dropout = 0.0
2021-11-13 23:57:48.000 model.encoder.transformer_kwargs = None
2021-11-13 23:57:48.197 removing temporary unarchived model dir at /tmp/tmps0t0m_ac
2021-11-13 23:59:47.349 Plugin allennlp_models available
2021-11-13 23:59:47.358 Plugin allennlp_server available
2021-11-13 23:59:47.359 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-13 23:59:47.359 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpdwxtj53q
2021-11-13 23:59:49.478 dataset_reader.type = example_reader
2021-11-13 23:59:49.479 dataset_reader.max_instances = None
2021-11-13 23:59:49.479 dataset_reader.manual_distributed_sharding = False
2021-11-13 23:59:49.479 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 23:59:49.479 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 23:59:49.479 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 23:59:49.479 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 23:59:49.479 type = bert-base-cased
2021-11-13 23:59:49.479 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 23:59:49.479 dataset_reader.tokenizer.max_length = 128
2021-11-13 23:59:49.479 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 23:59:49.480 dataset_reader.text_token_indexers.type = ref
2021-11-13 23:59:49.482 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 23:59:49.482 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 23:59:49.482 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 23:59:49.482 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 23:59:49.482 type = bert-base-cased
2021-11-13 23:59:49.482 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 23:59:49.482 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 23:59:49.482 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 23:59:49.483 dataset_reader.to_index = 6
2021-11-13 23:59:49.483 dataset_reader.type = example_reader
2021-11-13 23:59:49.483 dataset_reader.max_instances = None
2021-11-13 23:59:49.484 dataset_reader.manual_distributed_sharding = False
2021-11-13 23:59:49.484 dataset_reader.manual_multiprocess_sharding = False
2021-11-13 23:59:49.484 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 23:59:49.484 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-13 23:59:49.484 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-13 23:59:49.484 type = bert-base-cased
2021-11-13 23:59:49.484 dataset_reader.tokenizer.add_special_tokens = True
2021-11-13 23:59:49.484 dataset_reader.tokenizer.max_length = 128
2021-11-13 23:59:49.484 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-13 23:59:49.486 dataset_reader.text_token_indexers.type = ref
2021-11-13 23:59:49.488 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 23:59:49.488 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-13 23:59:49.489 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-13 23:59:49.489 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-13 23:59:49.489 type = bert-base-cased
2021-11-13 23:59:49.489 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-13 23:59:49.490 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-13 23:59:49.490 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-13 23:59:49.491 dataset_reader.to_index = 6
2021-11-13 23:59:49.492 type = from_instances
2021-11-13 23:59:49.492 Loading token dictionary from /tmp/tmpdwxtj53q/vocabulary.
2021-11-13 23:59:49.493 model.type = sentence_level_classifier
2021-11-13 23:59:49.494 model.embedder.type = ref
2021-11-13 23:59:49.496 model.embedder.type = basic
2021-11-13 23:59:49.497 model.embedder.token_embedders.type = ref
2021-11-13 23:59:49.499 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 23:59:49.500 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-13 23:59:49.500 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-13 23:59:49.501 type = bert-base-cased
2021-11-13 23:59:49.501 model.embedder.token_embedders.tokens.max_length = None
2021-11-13 23:59:49.501 model.embedder.token_embedders.tokens.sub_module = None
2021-11-13 23:59:49.501 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-13 23:59:49.501 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-13 23:59:49.502 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-13 23:59:49.502 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-13 23:59:49.502 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-13 23:59:49.502 model.embedder.token_embedders.tokens.load_weights = True
2021-11-13 23:59:49.502 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-13 23:59:49.502 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-13 23:59:49.503 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-13 23:59:49.557 model.encoder.type = bert_pooler
2021-11-13 23:59:49.557 model.encoder.type = bert_pooler
2021-11-13 23:59:49.557 model.encoder.pretrained_model = bert-base-cased
2021-11-13 23:59:49.557 type = bert-base-cased
2021-11-13 23:59:49.557 model.encoder.override_weights_file = None
2021-11-13 23:59:49.557 model.encoder.override_weights_strip_prefix = None
2021-11-13 23:59:49.557 model.encoder.load_weights = True
2021-11-13 23:59:49.557 model.encoder.requires_grad = True
2021-11-13 23:59:49.557 model.encoder.dropout = 0.0
2021-11-13 23:59:49.557 model.encoder.transformer_kwargs = None
2021-11-13 23:59:49.754 removing temporary unarchived model dir at /tmp/tmpdwxtj53q
2021-11-14 00:15:37.702 Plugin allennlp_models available
2021-11-14 00:15:37.708 Plugin allennlp_server available
2021-11-14 00:15:37.708 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-14 00:15:37.709 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpta8xdzrv
2021-11-14 00:15:39.808 dataset_reader.type = example_reader
2021-11-14 00:15:39.808 dataset_reader.max_instances = None
2021-11-14 00:15:39.808 dataset_reader.manual_distributed_sharding = False
2021-11-14 00:15:39.808 dataset_reader.manual_multiprocess_sharding = False
2021-11-14 00:15:39.808 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-14 00:15:39.808 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-14 00:15:39.808 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-14 00:15:39.808 type = bert-base-cased
2021-11-14 00:15:39.808 dataset_reader.tokenizer.add_special_tokens = True
2021-11-14 00:15:39.808 dataset_reader.tokenizer.max_length = 128
2021-11-14 00:15:39.808 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-14 00:15:39.810 dataset_reader.text_token_indexers.type = ref
2021-11-14 00:15:39.811 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-14 00:15:39.811 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-14 00:15:39.811 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-14 00:15:39.811 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-14 00:15:39.811 type = bert-base-cased
2021-11-14 00:15:39.811 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-14 00:15:39.811 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-14 00:15:39.811 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-14 00:15:39.812 dataset_reader.to_index = 6
2021-11-14 00:15:39.812 dataset_reader.type = example_reader
2021-11-14 00:15:39.813 dataset_reader.max_instances = None
2021-11-14 00:15:39.813 dataset_reader.manual_distributed_sharding = False
2021-11-14 00:15:39.813 dataset_reader.manual_multiprocess_sharding = False
2021-11-14 00:15:39.813 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-14 00:15:39.813 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-14 00:15:39.813 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-14 00:15:39.813 type = bert-base-cased
2021-11-14 00:15:39.813 dataset_reader.tokenizer.add_special_tokens = True
2021-11-14 00:15:39.813 dataset_reader.tokenizer.max_length = 128
2021-11-14 00:15:39.813 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-14 00:15:39.814 dataset_reader.text_token_indexers.type = ref
2021-11-14 00:15:39.815 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-14 00:15:39.815 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-14 00:15:39.815 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-14 00:15:39.815 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-14 00:15:39.815 type = bert-base-cased
2021-11-14 00:15:39.815 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-14 00:15:39.815 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-14 00:15:39.815 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-14 00:15:39.816 dataset_reader.to_index = 6
2021-11-14 00:15:39.816 type = from_instances
2021-11-14 00:15:39.816 Loading token dictionary from /tmp/tmpta8xdzrv/vocabulary.
2021-11-14 00:15:39.817 model.type = sentence_level_classifier
2021-11-14 00:15:39.817 model.embedder.type = ref
2021-11-14 00:15:39.818 model.embedder.type = basic
2021-11-14 00:15:39.818 model.embedder.token_embedders.type = ref
2021-11-14 00:15:39.818 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-14 00:15:39.819 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-14 00:15:39.819 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-14 00:15:39.819 type = bert-base-cased
2021-11-14 00:15:39.819 model.embedder.token_embedders.tokens.max_length = None
2021-11-14 00:15:39.819 model.embedder.token_embedders.tokens.sub_module = None
2021-11-14 00:15:39.819 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-14 00:15:39.819 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-14 00:15:39.819 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-14 00:15:39.819 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-14 00:15:39.819 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-14 00:15:39.819 model.embedder.token_embedders.tokens.load_weights = True
2021-11-14 00:15:39.819 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-14 00:15:39.819 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-14 00:15:39.819 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-14 00:15:39.861 model.encoder.type = bert_pooler
2021-11-14 00:15:39.861 model.encoder.type = bert_pooler
2021-11-14 00:15:39.861 model.encoder.pretrained_model = bert-base-cased
2021-11-14 00:15:39.862 type = bert-base-cased
2021-11-14 00:15:39.862 model.encoder.override_weights_file = None
2021-11-14 00:15:39.862 model.encoder.override_weights_strip_prefix = None
2021-11-14 00:15:39.862 model.encoder.load_weights = True
2021-11-14 00:15:39.862 model.encoder.requires_grad = True
2021-11-14 00:15:39.862 model.encoder.dropout = 0.0
2021-11-14 00:15:39.862 model.encoder.transformer_kwargs = None
2021-11-14 00:15:40.060 removing temporary unarchived model dir at /tmp/tmpta8xdzrv
2021-11-14 00:47:02.257 Plugin allennlp_models available
2021-11-14 00:47:02.290 Plugin allennlp_server available
2021-11-14 00:47:02.291 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-14 00:47:02.291 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmptqwmtb7r
2021-11-14 00:47:04.414 dataset_reader.type = example_reader
2021-11-14 00:47:04.414 dataset_reader.max_instances = None
2021-11-14 00:47:04.414 dataset_reader.manual_distributed_sharding = False
2021-11-14 00:47:04.414 dataset_reader.manual_multiprocess_sharding = False
2021-11-14 00:47:04.414 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-14 00:47:04.415 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-14 00:47:04.415 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-14 00:47:04.415 type = bert-base-cased
2021-11-14 00:47:04.415 dataset_reader.tokenizer.add_special_tokens = True
2021-11-14 00:47:04.415 dataset_reader.tokenizer.max_length = 128
2021-11-14 00:47:04.415 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-14 00:47:04.416 dataset_reader.text_token_indexers.type = ref
2021-11-14 00:47:04.417 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-14 00:47:04.417 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-14 00:47:04.418 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-14 00:47:04.418 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-14 00:47:04.418 type = bert-base-cased
2021-11-14 00:47:04.418 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-14 00:47:04.418 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-14 00:47:04.418 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-14 00:47:04.419 dataset_reader.to_index = 6
2021-11-14 00:47:04.419 dataset_reader.type = example_reader
2021-11-14 00:47:04.419 dataset_reader.max_instances = None
2021-11-14 00:47:04.419 dataset_reader.manual_distributed_sharding = False
2021-11-14 00:47:04.419 dataset_reader.manual_multiprocess_sharding = False
2021-11-14 00:47:04.419 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-14 00:47:04.419 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-14 00:47:04.419 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-14 00:47:04.419 type = bert-base-cased
2021-11-14 00:47:04.420 dataset_reader.tokenizer.add_special_tokens = True
2021-11-14 00:47:04.420 dataset_reader.tokenizer.max_length = 128
2021-11-14 00:47:04.420 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-14 00:47:04.421 dataset_reader.text_token_indexers.type = ref
2021-11-14 00:47:04.421 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-14 00:47:04.422 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-14 00:47:04.422 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-14 00:47:04.422 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-14 00:47:04.422 type = bert-base-cased
2021-11-14 00:47:04.422 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-14 00:47:04.422 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-14 00:47:04.422 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-14 00:47:04.423 dataset_reader.to_index = 6
2021-11-14 00:47:04.424 type = from_instances
2021-11-14 00:47:04.424 Loading token dictionary from /tmp/tmptqwmtb7r/vocabulary.
2021-11-14 00:47:04.424 model.type = sentence_level_classifier
2021-11-14 00:47:04.424 model.embedder.type = ref
2021-11-14 00:47:04.425 model.embedder.type = basic
2021-11-14 00:47:04.425 model.embedder.token_embedders.type = ref
2021-11-14 00:47:04.426 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-14 00:47:04.426 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-14 00:47:04.426 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-14 00:47:04.426 type = bert-base-cased
2021-11-14 00:47:04.426 model.embedder.token_embedders.tokens.max_length = None
2021-11-14 00:47:04.426 model.embedder.token_embedders.tokens.sub_module = None
2021-11-14 00:47:04.426 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-14 00:47:04.427 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-14 00:47:04.427 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-14 00:47:04.427 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-14 00:47:04.427 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-14 00:47:04.427 model.embedder.token_embedders.tokens.load_weights = True
2021-11-14 00:47:04.427 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-14 00:47:04.427 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-14 00:47:04.427 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-14 00:47:04.477 model.encoder.type = bert_pooler
2021-11-14 00:47:04.477 model.encoder.type = bert_pooler
2021-11-14 00:47:04.477 model.encoder.pretrained_model = bert-base-cased
2021-11-14 00:47:04.477 type = bert-base-cased
2021-11-14 00:47:04.477 model.encoder.override_weights_file = None
2021-11-14 00:47:04.477 model.encoder.override_weights_strip_prefix = None
2021-11-14 00:47:04.477 model.encoder.load_weights = True
2021-11-14 00:47:04.477 model.encoder.requires_grad = True
2021-11-14 00:47:04.478 model.encoder.dropout = 0.0
2021-11-14 00:47:04.478 model.encoder.transformer_kwargs = None
2021-11-14 00:47:04.677 removing temporary unarchived model dir at /tmp/tmptqwmtb7r
2021-11-14 01:08:47.132 Plugin allennlp_models available
2021-11-14 01:08:47.142 Plugin allennlp_server available
2021-11-14 01:08:47.143 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-14 01:08:47.144 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpe4oqpxb_
2021-11-14 01:08:49.257 dataset_reader.type = example_reader
2021-11-14 01:08:49.258 dataset_reader.max_instances = None
2021-11-14 01:08:49.258 dataset_reader.manual_distributed_sharding = False
2021-11-14 01:08:49.259 dataset_reader.manual_multiprocess_sharding = False
2021-11-14 01:08:49.259 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-14 01:08:49.259 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-14 01:08:49.259 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-14 01:08:49.259 type = bert-base-cased
2021-11-14 01:08:49.259 dataset_reader.tokenizer.add_special_tokens = True
2021-11-14 01:08:49.259 dataset_reader.tokenizer.max_length = 128
2021-11-14 01:08:49.259 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-14 01:08:49.260 dataset_reader.text_token_indexers.type = ref
2021-11-14 01:08:49.261 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-14 01:08:49.261 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-14 01:08:49.261 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-14 01:08:49.262 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-14 01:08:49.262 type = bert-base-cased
2021-11-14 01:08:49.262 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-14 01:08:49.262 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-14 01:08:49.262 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-14 01:08:49.263 dataset_reader.to_index = 6
2021-11-14 01:08:49.263 dataset_reader.type = example_reader
2021-11-14 01:08:49.263 dataset_reader.max_instances = None
2021-11-14 01:08:49.263 dataset_reader.manual_distributed_sharding = False
2021-11-14 01:08:49.263 dataset_reader.manual_multiprocess_sharding = False
2021-11-14 01:08:49.263 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-14 01:08:49.263 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-14 01:08:49.263 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-14 01:08:49.263 type = bert-base-cased
2021-11-14 01:08:49.263 dataset_reader.tokenizer.add_special_tokens = True
2021-11-14 01:08:49.263 dataset_reader.tokenizer.max_length = 128
2021-11-14 01:08:49.263 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-14 01:08:49.264 dataset_reader.text_token_indexers.type = ref
2021-11-14 01:08:49.265 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-14 01:08:49.265 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-14 01:08:49.265 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-14 01:08:49.265 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-14 01:08:49.265 type = bert-base-cased
2021-11-14 01:08:49.265 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-14 01:08:49.266 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-14 01:08:49.266 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-14 01:08:49.266 dataset_reader.to_index = 6
2021-11-14 01:08:49.267 type = from_instances
2021-11-14 01:08:49.267 Loading token dictionary from /tmp/tmpe4oqpxb_/vocabulary.
2021-11-14 01:08:49.267 model.type = sentence_level_classifier
2021-11-14 01:08:49.267 model.embedder.type = ref
2021-11-14 01:08:49.268 model.embedder.type = basic
2021-11-14 01:08:49.268 model.embedder.token_embedders.type = ref
2021-11-14 01:08:49.269 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-14 01:08:49.269 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-14 01:08:49.269 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-14 01:08:49.269 type = bert-base-cased
2021-11-14 01:08:49.269 model.embedder.token_embedders.tokens.max_length = None
2021-11-14 01:08:49.269 model.embedder.token_embedders.tokens.sub_module = None
2021-11-14 01:08:49.269 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-14 01:08:49.269 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-14 01:08:49.269 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-14 01:08:49.269 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-14 01:08:49.269 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-14 01:08:49.269 model.embedder.token_embedders.tokens.load_weights = True
2021-11-14 01:08:49.270 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-14 01:08:49.270 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-14 01:08:49.270 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-14 01:08:49.318 model.encoder.type = bert_pooler
2021-11-14 01:08:49.319 model.encoder.type = bert_pooler
2021-11-14 01:08:49.319 model.encoder.pretrained_model = bert-base-cased
2021-11-14 01:08:49.319 type = bert-base-cased
2021-11-14 01:08:49.319 model.encoder.override_weights_file = None
2021-11-14 01:08:49.319 model.encoder.override_weights_strip_prefix = None
2021-11-14 01:08:49.319 model.encoder.load_weights = True
2021-11-14 01:08:49.319 model.encoder.requires_grad = True
2021-11-14 01:08:49.320 model.encoder.dropout = 0.0
2021-11-14 01:08:49.320 model.encoder.transformer_kwargs = None
2021-11-14 01:08:49.515 removing temporary unarchived model dir at /tmp/tmpe4oqpxb_
2021-11-14 01:11:23.493 Plugin allennlp_models available
2021-11-14 01:11:23.500 Plugin allennlp_server available
2021-11-14 01:11:23.501 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-14 01:11:23.501 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp5wkgl88h
2021-11-14 01:11:25.597 dataset_reader.type = example_reader
2021-11-14 01:11:25.597 dataset_reader.max_instances = None
2021-11-14 01:11:25.597 dataset_reader.manual_distributed_sharding = False
2021-11-14 01:11:25.597 dataset_reader.manual_multiprocess_sharding = False
2021-11-14 01:11:25.597 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-14 01:11:25.597 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-14 01:11:25.598 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-14 01:11:25.598 type = bert-base-cased
2021-11-14 01:11:25.598 dataset_reader.tokenizer.add_special_tokens = True
2021-11-14 01:11:25.598 dataset_reader.tokenizer.max_length = 128
2021-11-14 01:11:25.598 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-14 01:11:25.599 dataset_reader.text_token_indexers.type = ref
2021-11-14 01:11:25.600 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-14 01:11:25.600 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-14 01:11:25.601 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-14 01:11:25.601 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-14 01:11:25.601 type = bert-base-cased
2021-11-14 01:11:25.601 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-14 01:11:25.601 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-14 01:11:25.601 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-14 01:11:25.602 dataset_reader.to_index = 6
2021-11-14 01:11:25.602 dataset_reader.type = example_reader
2021-11-14 01:11:25.602 dataset_reader.max_instances = None
2021-11-14 01:11:25.602 dataset_reader.manual_distributed_sharding = False
2021-11-14 01:11:25.602 dataset_reader.manual_multiprocess_sharding = False
2021-11-14 01:11:25.602 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-14 01:11:25.602 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-14 01:11:25.603 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-14 01:11:25.603 type = bert-base-cased
2021-11-14 01:11:25.603 dataset_reader.tokenizer.add_special_tokens = True
2021-11-14 01:11:25.603 dataset_reader.tokenizer.max_length = 128
2021-11-14 01:11:25.603 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-14 01:11:25.604 dataset_reader.text_token_indexers.type = ref
2021-11-14 01:11:25.604 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-14 01:11:25.604 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-14 01:11:25.604 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-14 01:11:25.605 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-14 01:11:25.605 type = bert-base-cased
2021-11-14 01:11:25.605 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-14 01:11:25.605 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-14 01:11:25.605 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-14 01:11:25.606 dataset_reader.to_index = 6
2021-11-14 01:11:25.606 type = from_instances
2021-11-14 01:11:25.606 Loading token dictionary from /tmp/tmp5wkgl88h/vocabulary.
2021-11-14 01:11:25.606 model.type = sentence_level_classifier
2021-11-14 01:11:25.607 model.embedder.type = ref
2021-11-14 01:11:25.607 model.embedder.type = basic
2021-11-14 01:11:25.607 model.embedder.token_embedders.type = ref
2021-11-14 01:11:25.608 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-14 01:11:25.608 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-14 01:11:25.609 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-14 01:11:25.609 type = bert-base-cased
2021-11-14 01:11:25.609 model.embedder.token_embedders.tokens.max_length = None
2021-11-14 01:11:25.609 model.embedder.token_embedders.tokens.sub_module = None
2021-11-14 01:11:25.609 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-14 01:11:25.609 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-14 01:11:25.609 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-14 01:11:25.609 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-14 01:11:25.609 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-14 01:11:25.609 model.embedder.token_embedders.tokens.load_weights = True
2021-11-14 01:11:25.609 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-14 01:11:25.609 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-14 01:11:25.609 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-14 01:11:25.660 model.encoder.type = bert_pooler
2021-11-14 01:11:25.660 model.encoder.type = bert_pooler
2021-11-14 01:11:25.660 model.encoder.pretrained_model = bert-base-cased
2021-11-14 01:11:25.661 type = bert-base-cased
2021-11-14 01:11:25.661 model.encoder.override_weights_file = None
2021-11-14 01:11:25.661 model.encoder.override_weights_strip_prefix = None
2021-11-14 01:11:25.661 model.encoder.load_weights = True
2021-11-14 01:11:25.661 model.encoder.requires_grad = True
2021-11-14 01:11:25.661 model.encoder.dropout = 0.0
2021-11-14 01:11:25.661 model.encoder.transformer_kwargs = None
2021-11-14 01:11:25.858 removing temporary unarchived model dir at /tmp/tmp5wkgl88h
2021-11-14 03:50:35.852 Plugin allennlp_models available
2021-11-14 03:50:35.867 Plugin allennlp_server available
2021-11-14 03:50:35.868 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-14 03:50:35.868 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpl0yap3cf
2021-11-14 03:50:37.983 dataset_reader.type = example_reader
2021-11-14 03:50:37.983 dataset_reader.max_instances = None
2021-11-14 03:50:37.983 dataset_reader.manual_distributed_sharding = False
2021-11-14 03:50:37.983 dataset_reader.manual_multiprocess_sharding = False
2021-11-14 03:50:37.983 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-14 03:50:37.983 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-14 03:50:37.984 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-14 03:50:37.984 type = bert-base-cased
2021-11-14 03:50:37.984 dataset_reader.tokenizer.add_special_tokens = True
2021-11-14 03:50:37.984 dataset_reader.tokenizer.max_length = 128
2021-11-14 03:50:37.984 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-14 03:50:37.985 dataset_reader.text_token_indexers.type = ref
2021-11-14 03:50:37.986 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-14 03:50:37.986 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-14 03:50:37.986 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-14 03:50:37.987 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-14 03:50:37.987 type = bert-base-cased
2021-11-14 03:50:37.987 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-14 03:50:37.987 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-14 03:50:37.987 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-14 03:50:37.987 dataset_reader.to_index = 6
2021-11-14 03:50:37.988 dataset_reader.type = example_reader
2021-11-14 03:50:37.988 dataset_reader.max_instances = None
2021-11-14 03:50:37.988 dataset_reader.manual_distributed_sharding = False
2021-11-14 03:50:37.988 dataset_reader.manual_multiprocess_sharding = False
2021-11-14 03:50:37.988 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-14 03:50:37.988 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-14 03:50:37.988 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-14 03:50:37.988 type = bert-base-cased
2021-11-14 03:50:37.988 dataset_reader.tokenizer.add_special_tokens = True
2021-11-14 03:50:37.988 dataset_reader.tokenizer.max_length = 128
2021-11-14 03:50:37.988 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-14 03:50:37.989 dataset_reader.text_token_indexers.type = ref
2021-11-14 03:50:37.990 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-14 03:50:37.990 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-14 03:50:37.990 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-14 03:50:37.990 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-14 03:50:37.990 type = bert-base-cased
2021-11-14 03:50:37.991 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-14 03:50:37.991 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-14 03:50:37.991 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-14 03:50:37.991 dataset_reader.to_index = 6
2021-11-14 03:50:37.992 type = from_instances
2021-11-14 03:50:37.992 Loading token dictionary from /tmp/tmpl0yap3cf/vocabulary.
2021-11-14 03:50:37.992 model.type = sentence_level_classifier
2021-11-14 03:50:37.992 model.embedder.type = ref
2021-11-14 03:50:37.993 model.embedder.type = basic
2021-11-14 03:50:37.993 model.embedder.token_embedders.type = ref
2021-11-14 03:50:37.994 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-14 03:50:37.994 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-14 03:50:37.994 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-14 03:50:37.994 type = bert-base-cased
2021-11-14 03:50:37.994 model.embedder.token_embedders.tokens.max_length = None
2021-11-14 03:50:37.994 model.embedder.token_embedders.tokens.sub_module = None
2021-11-14 03:50:37.994 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-14 03:50:37.994 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-14 03:50:37.994 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-14 03:50:37.994 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-14 03:50:37.994 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-14 03:50:37.994 model.embedder.token_embedders.tokens.load_weights = True
2021-11-14 03:50:37.994 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-14 03:50:37.995 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-14 03:50:37.995 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-14 03:50:38.045 model.encoder.type = bert_pooler
2021-11-14 03:50:38.045 model.encoder.type = bert_pooler
2021-11-14 03:50:38.045 model.encoder.pretrained_model = bert-base-cased
2021-11-14 03:50:38.046 type = bert-base-cased
2021-11-14 03:50:38.046 model.encoder.override_weights_file = None
2021-11-14 03:50:38.046 model.encoder.override_weights_strip_prefix = None
2021-11-14 03:50:38.046 model.encoder.load_weights = True
2021-11-14 03:50:38.046 model.encoder.requires_grad = True
2021-11-14 03:50:38.046 model.encoder.dropout = 0.0
2021-11-14 03:50:38.046 model.encoder.transformer_kwargs = None
2021-11-14 03:50:38.245 removing temporary unarchived model dir at /tmp/tmpl0yap3cf
2021-11-14 06:05:36.042 Plugin allennlp_models available
2021-11-14 06:05:36.050 Plugin allennlp_server available
2021-11-14 06:05:36.051 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-14 06:05:36.051 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpj97ffjvw
2021-11-14 06:05:38.162 dataset_reader.type = example_reader
2021-11-14 06:05:38.163 dataset_reader.max_instances = None
2021-11-14 06:05:38.163 dataset_reader.manual_distributed_sharding = False
2021-11-14 06:05:38.163 dataset_reader.manual_multiprocess_sharding = False
2021-11-14 06:05:38.163 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-14 06:05:38.163 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-14 06:05:38.163 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-14 06:05:38.163 type = bert-base-cased
2021-11-14 06:05:38.163 dataset_reader.tokenizer.add_special_tokens = True
2021-11-14 06:05:38.163 dataset_reader.tokenizer.max_length = 128
2021-11-14 06:05:38.163 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-14 06:05:38.165 dataset_reader.text_token_indexers.type = ref
2021-11-14 06:05:38.166 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-14 06:05:38.166 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-14 06:05:38.166 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-14 06:05:38.166 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-14 06:05:38.166 type = bert-base-cased
2021-11-14 06:05:38.166 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-14 06:05:38.166 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-14 06:05:38.166 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-14 06:05:38.167 dataset_reader.to_index = 6
2021-11-14 06:05:38.167 dataset_reader.type = example_reader
2021-11-14 06:05:38.168 dataset_reader.max_instances = None
2021-11-14 06:05:38.168 dataset_reader.manual_distributed_sharding = False
2021-11-14 06:05:38.168 dataset_reader.manual_multiprocess_sharding = False
2021-11-14 06:05:38.168 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-14 06:05:38.168 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-14 06:05:38.168 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-14 06:05:38.168 type = bert-base-cased
2021-11-14 06:05:38.168 dataset_reader.tokenizer.add_special_tokens = True
2021-11-14 06:05:38.168 dataset_reader.tokenizer.max_length = 128
2021-11-14 06:05:38.168 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-14 06:05:38.169 dataset_reader.text_token_indexers.type = ref
2021-11-14 06:05:38.170 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-14 06:05:38.170 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-14 06:05:38.170 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-14 06:05:38.170 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-14 06:05:38.170 type = bert-base-cased
2021-11-14 06:05:38.170 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-14 06:05:38.170 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-14 06:05:38.170 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-14 06:05:38.171 dataset_reader.to_index = 6
2021-11-14 06:05:38.171 type = from_instances
2021-11-14 06:05:38.171 Loading token dictionary from /tmp/tmpj97ffjvw/vocabulary.
2021-11-14 06:05:38.172 model.type = sentence_level_classifier
2021-11-14 06:05:38.172 model.embedder.type = ref
2021-11-14 06:05:38.173 model.embedder.type = basic
2021-11-14 06:05:38.173 model.embedder.token_embedders.type = ref
2021-11-14 06:05:38.174 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-14 06:05:38.174 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-14 06:05:38.174 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-14 06:05:38.174 type = bert-base-cased
2021-11-14 06:05:38.174 model.embedder.token_embedders.tokens.max_length = None
2021-11-14 06:05:38.174 model.embedder.token_embedders.tokens.sub_module = None
2021-11-14 06:05:38.174 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-14 06:05:38.174 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-14 06:05:38.174 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-14 06:05:38.174 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-14 06:05:38.174 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-14 06:05:38.175 model.embedder.token_embedders.tokens.load_weights = True
2021-11-14 06:05:38.175 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-14 06:05:38.175 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-14 06:05:38.175 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-14 06:05:38.224 model.encoder.type = bert_pooler
2021-11-14 06:05:38.224 model.encoder.type = bert_pooler
2021-11-14 06:05:38.225 model.encoder.pretrained_model = bert-base-cased
2021-11-14 06:05:38.225 type = bert-base-cased
2021-11-14 06:05:38.225 model.encoder.override_weights_file = None
2021-11-14 06:05:38.225 model.encoder.override_weights_strip_prefix = None
2021-11-14 06:05:38.225 model.encoder.load_weights = True
2021-11-14 06:05:38.225 model.encoder.requires_grad = True
2021-11-14 06:05:38.225 model.encoder.dropout = 0.0
2021-11-14 06:05:38.225 model.encoder.transformer_kwargs = None
2021-11-14 06:05:38.424 removing temporary unarchived model dir at /tmp/tmpj97ffjvw
2021-11-14 06:45:07.898 Plugin allennlp_models available
2021-11-14 06:45:07.906 Plugin allennlp_server available
2021-11-14 06:45:07.907 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-14 06:45:07.908 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpvqhxate_
2021-11-14 06:45:10.039 dataset_reader.type = example_reader
2021-11-14 06:45:10.040 dataset_reader.max_instances = None
2021-11-14 06:45:10.040 dataset_reader.manual_distributed_sharding = False
2021-11-14 06:45:10.040 dataset_reader.manual_multiprocess_sharding = False
2021-11-14 06:45:10.040 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-14 06:45:10.040 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-14 06:45:10.040 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-14 06:45:10.040 type = bert-base-cased
2021-11-14 06:45:10.040 dataset_reader.tokenizer.add_special_tokens = True
2021-11-14 06:45:10.040 dataset_reader.tokenizer.max_length = 128
2021-11-14 06:45:10.040 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-14 06:45:10.042 dataset_reader.text_token_indexers.type = ref
2021-11-14 06:45:10.043 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-14 06:45:10.043 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-14 06:45:10.043 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-14 06:45:10.043 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-14 06:45:10.043 type = bert-base-cased
2021-11-14 06:45:10.043 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-14 06:45:10.043 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-14 06:45:10.043 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-14 06:45:10.044 dataset_reader.to_index = 6
2021-11-14 06:45:10.044 dataset_reader.type = example_reader
2021-11-14 06:45:10.045 dataset_reader.max_instances = None
2021-11-14 06:45:10.045 dataset_reader.manual_distributed_sharding = False
2021-11-14 06:45:10.045 dataset_reader.manual_multiprocess_sharding = False
2021-11-14 06:45:10.045 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-14 06:45:10.045 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-14 06:45:10.045 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-14 06:45:10.045 type = bert-base-cased
2021-11-14 06:45:10.045 dataset_reader.tokenizer.add_special_tokens = True
2021-11-14 06:45:10.045 dataset_reader.tokenizer.max_length = 128
2021-11-14 06:45:10.045 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-14 06:45:10.046 dataset_reader.text_token_indexers.type = ref
2021-11-14 06:45:10.047 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-14 06:45:10.047 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-14 06:45:10.047 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-14 06:45:10.047 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-14 06:45:10.047 type = bert-base-cased
2021-11-14 06:45:10.047 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-14 06:45:10.047 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-14 06:45:10.047 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-14 06:45:10.048 dataset_reader.to_index = 6
2021-11-14 06:45:10.048 type = from_instances
2021-11-14 06:45:10.048 Loading token dictionary from /tmp/tmpvqhxate_/vocabulary.
2021-11-14 06:45:10.049 model.type = sentence_level_classifier
2021-11-14 06:45:10.049 model.embedder.type = ref
2021-11-14 06:45:10.050 model.embedder.type = basic
2021-11-14 06:45:10.050 model.embedder.token_embedders.type = ref
2021-11-14 06:45:10.051 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-14 06:45:10.051 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-14 06:45:10.051 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-14 06:45:10.051 type = bert-base-cased
2021-11-14 06:45:10.051 model.embedder.token_embedders.tokens.max_length = None
2021-11-14 06:45:10.051 model.embedder.token_embedders.tokens.sub_module = None
2021-11-14 06:45:10.051 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-14 06:45:10.051 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-14 06:45:10.051 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-14 06:45:10.051 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-14 06:45:10.051 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-14 06:45:10.051 model.embedder.token_embedders.tokens.load_weights = True
2021-11-14 06:45:10.051 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-14 06:45:10.051 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-14 06:45:10.051 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-14 06:45:10.096 model.encoder.type = bert_pooler
2021-11-14 06:45:10.097 model.encoder.type = bert_pooler
2021-11-14 06:45:10.097 model.encoder.pretrained_model = bert-base-cased
2021-11-14 06:45:10.097 type = bert-base-cased
2021-11-14 06:45:10.097 model.encoder.override_weights_file = None
2021-11-14 06:45:10.097 model.encoder.override_weights_strip_prefix = None
2021-11-14 06:45:10.097 model.encoder.load_weights = True
2021-11-14 06:45:10.098 model.encoder.requires_grad = True
2021-11-14 06:45:10.098 model.encoder.dropout = 0.0
2021-11-14 06:45:10.098 model.encoder.transformer_kwargs = None
2021-11-14 06:45:10.294 removing temporary unarchived model dir at /tmp/tmpvqhxate_
2021-11-14 09:52:26.360 Plugin allennlp_models available
2021-11-14 09:52:26.368 Plugin allennlp_server available
2021-11-14 09:52:26.369 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-14 09:52:26.369 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpvknlmsnh
2021-11-14 09:52:28.473 dataset_reader.type = example_reader
2021-11-14 09:52:28.474 dataset_reader.max_instances = None
2021-11-14 09:52:28.474 dataset_reader.manual_distributed_sharding = False
2021-11-14 09:52:28.474 dataset_reader.manual_multiprocess_sharding = False
2021-11-14 09:52:28.474 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-14 09:52:28.474 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-14 09:52:28.474 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-14 09:52:28.474 type = bert-base-cased
2021-11-14 09:52:28.474 dataset_reader.tokenizer.add_special_tokens = True
2021-11-14 09:52:28.474 dataset_reader.tokenizer.max_length = 128
2021-11-14 09:52:28.474 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-14 09:52:28.476 dataset_reader.text_token_indexers.type = ref
2021-11-14 09:52:28.477 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-14 09:52:28.477 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-14 09:52:28.477 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-14 09:52:28.477 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-14 09:52:28.477 type = bert-base-cased
2021-11-14 09:52:28.477 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-14 09:52:28.477 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-14 09:52:28.477 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-14 09:52:28.478 dataset_reader.to_index = 6
2021-11-14 09:52:28.478 dataset_reader.type = example_reader
2021-11-14 09:52:28.479 dataset_reader.max_instances = None
2021-11-14 09:52:28.479 dataset_reader.manual_distributed_sharding = False
2021-11-14 09:52:28.479 dataset_reader.manual_multiprocess_sharding = False
2021-11-14 09:52:28.479 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-14 09:52:28.479 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-14 09:52:28.479 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-14 09:52:28.479 type = bert-base-cased
2021-11-14 09:52:28.479 dataset_reader.tokenizer.add_special_tokens = True
2021-11-14 09:52:28.479 dataset_reader.tokenizer.max_length = 128
2021-11-14 09:52:28.479 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-14 09:52:28.480 dataset_reader.text_token_indexers.type = ref
2021-11-14 09:52:28.481 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-14 09:52:28.481 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-14 09:52:28.481 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-14 09:52:28.481 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-14 09:52:28.481 type = bert-base-cased
2021-11-14 09:52:28.481 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-14 09:52:28.481 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-14 09:52:28.481 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-14 09:52:28.482 dataset_reader.to_index = 6
2021-11-14 09:52:28.482 type = from_instances
2021-11-14 09:52:28.482 Loading token dictionary from /tmp/tmpvknlmsnh/vocabulary.
2021-11-14 09:52:28.483 model.type = sentence_level_classifier
2021-11-14 09:52:28.483 model.embedder.type = ref
2021-11-14 09:52:28.483 model.embedder.type = basic
2021-11-14 09:52:28.484 model.embedder.token_embedders.type = ref
2021-11-14 09:52:28.485 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-14 09:52:28.485 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-14 09:52:28.485 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-14 09:52:28.485 type = bert-base-cased
2021-11-14 09:52:28.485 model.embedder.token_embedders.tokens.max_length = None
2021-11-14 09:52:28.485 model.embedder.token_embedders.tokens.sub_module = None
2021-11-14 09:52:28.485 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-14 09:52:28.485 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-14 09:52:28.485 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-14 09:52:28.485 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-14 09:52:28.485 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-14 09:52:28.485 model.embedder.token_embedders.tokens.load_weights = True
2021-11-14 09:52:28.485 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-14 09:52:28.485 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-14 09:52:28.485 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-14 09:52:28.536 model.encoder.type = bert_pooler
2021-11-14 09:52:28.536 model.encoder.type = bert_pooler
2021-11-14 09:52:28.536 model.encoder.pretrained_model = bert-base-cased
2021-11-14 09:52:28.536 type = bert-base-cased
2021-11-14 09:52:28.536 model.encoder.override_weights_file = None
2021-11-14 09:52:28.536 model.encoder.override_weights_strip_prefix = None
2021-11-14 09:52:28.537 model.encoder.load_weights = True
2021-11-14 09:52:28.537 model.encoder.requires_grad = True
2021-11-14 09:52:28.537 model.encoder.dropout = 0.0
2021-11-14 09:52:28.537 model.encoder.transformer_kwargs = None
2021-11-14 09:52:28.734 removing temporary unarchived model dir at /tmp/tmpvknlmsnh
2021-11-14 10:32:45.791 Plugin allennlp_models available
2021-11-14 10:32:45.800 Plugin allennlp_server available
2021-11-14 10:32:45.800 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-14 10:32:45.801 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpall9f8cx
2021-11-14 10:32:47.911 dataset_reader.type = example_reader
2021-11-14 10:32:47.912 dataset_reader.max_instances = None
2021-11-14 10:32:47.912 dataset_reader.manual_distributed_sharding = False
2021-11-14 10:32:47.912 dataset_reader.manual_multiprocess_sharding = False
2021-11-14 10:32:47.912 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-14 10:32:47.912 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-14 10:32:47.912 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-14 10:32:47.912 type = bert-base-cased
2021-11-14 10:32:47.912 dataset_reader.tokenizer.add_special_tokens = True
2021-11-14 10:32:47.912 dataset_reader.tokenizer.max_length = 128
2021-11-14 10:32:47.912 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-14 10:32:47.913 dataset_reader.text_token_indexers.type = ref
2021-11-14 10:32:47.915 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-14 10:32:47.915 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-14 10:32:47.915 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-14 10:32:47.915 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-14 10:32:47.915 type = bert-base-cased
2021-11-14 10:32:47.915 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-14 10:32:47.915 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-14 10:32:47.915 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-14 10:32:47.916 dataset_reader.to_index = 6
2021-11-14 10:32:47.916 dataset_reader.type = example_reader
2021-11-14 10:32:47.917 dataset_reader.max_instances = None
2021-11-14 10:32:47.917 dataset_reader.manual_distributed_sharding = False
2021-11-14 10:32:47.917 dataset_reader.manual_multiprocess_sharding = False
2021-11-14 10:32:47.918 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-14 10:32:47.918 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-14 10:32:47.919 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-14 10:32:47.919 type = bert-base-cased
2021-11-14 10:32:47.919 dataset_reader.tokenizer.add_special_tokens = True
2021-11-14 10:32:47.919 dataset_reader.tokenizer.max_length = 128
2021-11-14 10:32:47.920 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-14 10:32:47.922 dataset_reader.text_token_indexers.type = ref
2021-11-14 10:32:47.923 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-14 10:32:47.924 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-14 10:32:47.924 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-14 10:32:47.925 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-14 10:32:47.925 type = bert-base-cased
2021-11-14 10:32:47.925 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-14 10:32:47.925 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-14 10:32:47.926 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-14 10:32:47.927 dataset_reader.to_index = 6
2021-11-14 10:32:47.928 type = from_instances
2021-11-14 10:32:47.928 Loading token dictionary from /tmp/tmpall9f8cx/vocabulary.
2021-11-14 10:32:47.929 model.type = sentence_level_classifier
2021-11-14 10:32:47.929 model.embedder.type = ref
2021-11-14 10:32:47.931 model.embedder.type = basic
2021-11-14 10:32:47.931 model.embedder.token_embedders.type = ref
2021-11-14 10:32:47.933 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-14 10:32:47.933 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-14 10:32:47.934 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-14 10:32:47.934 type = bert-base-cased
2021-11-14 10:32:47.934 model.embedder.token_embedders.tokens.max_length = None
2021-11-14 10:32:47.935 model.embedder.token_embedders.tokens.sub_module = None
2021-11-14 10:32:47.935 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-14 10:32:47.935 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-14 10:32:47.935 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-14 10:32:47.935 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-14 10:32:47.936 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-14 10:32:47.936 model.embedder.token_embedders.tokens.load_weights = True
2021-11-14 10:32:47.936 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-14 10:32:47.936 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-14 10:32:47.936 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-14 10:32:47.988 model.encoder.type = bert_pooler
2021-11-14 10:32:47.988 model.encoder.type = bert_pooler
2021-11-14 10:32:47.989 model.encoder.pretrained_model = bert-base-cased
2021-11-14 10:32:47.989 type = bert-base-cased
2021-11-14 10:32:47.989 model.encoder.override_weights_file = None
2021-11-14 10:32:47.989 model.encoder.override_weights_strip_prefix = None
2021-11-14 10:32:47.989 model.encoder.load_weights = True
2021-11-14 10:32:47.989 model.encoder.requires_grad = True
2021-11-14 10:32:47.989 model.encoder.dropout = 0.0
2021-11-14 10:32:47.989 model.encoder.transformer_kwargs = None
2021-11-14 10:32:48.185 removing temporary unarchived model dir at /tmp/tmpall9f8cx
2021-11-14 11:08:07.603 Plugin allennlp_models available
2021-11-14 11:08:07.641 Plugin allennlp_server available
2021-11-14 11:08:07.642 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-14 11:08:07.642 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp1vimct5q
2021-11-14 11:08:09.756 dataset_reader.type = example_reader
2021-11-14 11:08:09.756 dataset_reader.max_instances = None
2021-11-14 11:08:09.756 dataset_reader.manual_distributed_sharding = False
2021-11-14 11:08:09.756 dataset_reader.manual_multiprocess_sharding = False
2021-11-14 11:08:09.757 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-14 11:08:09.757 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-14 11:08:09.757 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-14 11:08:09.757 type = bert-base-cased
2021-11-14 11:08:09.757 dataset_reader.tokenizer.add_special_tokens = True
2021-11-14 11:08:09.757 dataset_reader.tokenizer.max_length = 128
2021-11-14 11:08:09.757 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-14 11:08:09.758 dataset_reader.text_token_indexers.type = ref
2021-11-14 11:08:09.760 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-14 11:08:09.760 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-14 11:08:09.760 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-14 11:08:09.760 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-14 11:08:09.760 type = bert-base-cased
2021-11-14 11:08:09.760 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-14 11:08:09.760 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-14 11:08:09.760 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-14 11:08:09.761 dataset_reader.to_index = 6
2021-11-14 11:08:09.761 dataset_reader.type = example_reader
2021-11-14 11:08:09.761 dataset_reader.max_instances = None
2021-11-14 11:08:09.762 dataset_reader.manual_distributed_sharding = False
2021-11-14 11:08:09.762 dataset_reader.manual_multiprocess_sharding = False
2021-11-14 11:08:09.762 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-14 11:08:09.762 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-14 11:08:09.762 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-14 11:08:09.762 type = bert-base-cased
2021-11-14 11:08:09.762 dataset_reader.tokenizer.add_special_tokens = True
2021-11-14 11:08:09.762 dataset_reader.tokenizer.max_length = 128
2021-11-14 11:08:09.762 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-14 11:08:09.763 dataset_reader.text_token_indexers.type = ref
2021-11-14 11:08:09.764 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-14 11:08:09.764 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-14 11:08:09.764 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-14 11:08:09.764 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-14 11:08:09.764 type = bert-base-cased
2021-11-14 11:08:09.764 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-14 11:08:09.764 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-14 11:08:09.764 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-14 11:08:09.765 dataset_reader.to_index = 6
2021-11-14 11:08:09.765 type = from_instances
2021-11-14 11:08:09.765 Loading token dictionary from /tmp/tmp1vimct5q/vocabulary.
2021-11-14 11:08:09.766 model.type = sentence_level_classifier
2021-11-14 11:08:09.766 model.embedder.type = ref
2021-11-14 11:08:09.766 model.embedder.type = basic
2021-11-14 11:08:09.767 model.embedder.token_embedders.type = ref
2021-11-14 11:08:09.767 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-14 11:08:09.768 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-14 11:08:09.768 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-14 11:08:09.768 type = bert-base-cased
2021-11-14 11:08:09.768 model.embedder.token_embedders.tokens.max_length = None
2021-11-14 11:08:09.768 model.embedder.token_embedders.tokens.sub_module = None
2021-11-14 11:08:09.768 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-14 11:08:09.768 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-14 11:08:09.768 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-14 11:08:09.768 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-14 11:08:09.768 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-14 11:08:09.768 model.embedder.token_embedders.tokens.load_weights = True
2021-11-14 11:08:09.768 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-14 11:08:09.768 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-14 11:08:09.768 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-14 11:08:09.813 model.encoder.type = bert_pooler
2021-11-14 11:08:09.813 model.encoder.type = bert_pooler
2021-11-14 11:08:09.813 model.encoder.pretrained_model = bert-base-cased
2021-11-14 11:08:09.813 type = bert-base-cased
2021-11-14 11:08:09.813 model.encoder.override_weights_file = None
2021-11-14 11:08:09.813 model.encoder.override_weights_strip_prefix = None
2021-11-14 11:08:09.814 model.encoder.load_weights = True
2021-11-14 11:08:09.814 model.encoder.requires_grad = True
2021-11-14 11:08:09.814 model.encoder.dropout = 0.0
2021-11-14 11:08:09.814 model.encoder.transformer_kwargs = None
2021-11-14 11:08:10.012 removing temporary unarchived model dir at /tmp/tmp1vimct5q
2021-11-14 11:39:46.638 Plugin allennlp_models available
2021-11-14 11:39:46.649 Plugin allennlp_server available
2021-11-14 11:39:46.650 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-14 11:39:46.650 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpd746rqox
2021-11-14 11:39:48.779 dataset_reader.type = example_reader
2021-11-14 11:39:48.779 dataset_reader.max_instances = None
2021-11-14 11:39:48.779 dataset_reader.manual_distributed_sharding = False
2021-11-14 11:39:48.779 dataset_reader.manual_multiprocess_sharding = False
2021-11-14 11:39:48.779 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-14 11:39:48.779 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-14 11:39:48.780 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-14 11:39:48.780 type = bert-base-cased
2021-11-14 11:39:48.780 dataset_reader.tokenizer.add_special_tokens = True
2021-11-14 11:39:48.780 dataset_reader.tokenizer.max_length = 128
2021-11-14 11:39:48.780 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-14 11:39:48.781 dataset_reader.text_token_indexers.type = ref
2021-11-14 11:39:48.782 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-14 11:39:48.783 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-14 11:39:48.783 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-14 11:39:48.783 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-14 11:39:48.783 type = bert-base-cased
2021-11-14 11:39:48.783 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-14 11:39:48.783 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-14 11:39:48.783 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-14 11:39:48.784 dataset_reader.to_index = 6
2021-11-14 11:39:48.784 dataset_reader.type = example_reader
2021-11-14 11:39:48.784 dataset_reader.max_instances = None
2021-11-14 11:39:48.784 dataset_reader.manual_distributed_sharding = False
2021-11-14 11:39:48.784 dataset_reader.manual_multiprocess_sharding = False
2021-11-14 11:39:48.784 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-14 11:39:48.784 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-14 11:39:48.785 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-14 11:39:48.785 type = bert-base-cased
2021-11-14 11:39:48.785 dataset_reader.tokenizer.add_special_tokens = True
2021-11-14 11:39:48.785 dataset_reader.tokenizer.max_length = 128
2021-11-14 11:39:48.785 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-14 11:39:48.786 dataset_reader.text_token_indexers.type = ref
2021-11-14 11:39:48.786 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-14 11:39:48.786 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-14 11:39:48.787 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-14 11:39:48.787 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-14 11:39:48.787 type = bert-base-cased
2021-11-14 11:39:48.787 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-14 11:39:48.787 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-14 11:39:48.787 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-14 11:39:48.788 dataset_reader.to_index = 6
2021-11-14 11:39:48.788 type = from_instances
2021-11-14 11:39:48.788 Loading token dictionary from /tmp/tmpd746rqox/vocabulary.
2021-11-14 11:39:48.789 model.type = sentence_level_classifier
2021-11-14 11:39:48.789 model.embedder.type = ref
2021-11-14 11:39:48.789 model.embedder.type = basic
2021-11-14 11:39:48.790 model.embedder.token_embedders.type = ref
2021-11-14 11:39:48.790 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-14 11:39:48.791 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-14 11:39:48.791 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-14 11:39:48.791 type = bert-base-cased
2021-11-14 11:39:48.791 model.embedder.token_embedders.tokens.max_length = None
2021-11-14 11:39:48.791 model.embedder.token_embedders.tokens.sub_module = None
2021-11-14 11:39:48.791 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-14 11:39:48.791 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-14 11:39:48.791 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-14 11:39:48.791 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-14 11:39:48.791 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-14 11:39:48.791 model.embedder.token_embedders.tokens.load_weights = True
2021-11-14 11:39:48.791 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-14 11:39:48.791 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-14 11:39:48.791 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-14 11:39:48.835 model.encoder.type = bert_pooler
2021-11-14 11:39:48.836 model.encoder.type = bert_pooler
2021-11-14 11:39:48.836 model.encoder.pretrained_model = bert-base-cased
2021-11-14 11:39:48.836 type = bert-base-cased
2021-11-14 11:39:48.836 model.encoder.override_weights_file = None
2021-11-14 11:39:48.836 model.encoder.override_weights_strip_prefix = None
2021-11-14 11:39:48.836 model.encoder.load_weights = True
2021-11-14 11:39:48.836 model.encoder.requires_grad = True
2021-11-14 11:39:48.836 model.encoder.dropout = 0.0
2021-11-14 11:39:48.837 model.encoder.transformer_kwargs = None
2021-11-14 11:39:49.036 removing temporary unarchived model dir at /tmp/tmpd746rqox
2021-11-14 12:42:51.785 Plugin allennlp_models available
2021-11-14 12:42:51.797 Plugin allennlp_server available
2021-11-14 12:42:51.798 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-14 12:42:51.799 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp_pfw2wyp
2021-11-14 12:42:53.903 dataset_reader.type = example_reader
2021-11-14 12:42:53.903 dataset_reader.max_instances = None
2021-11-14 12:42:53.904 dataset_reader.manual_distributed_sharding = False
2021-11-14 12:42:53.904 dataset_reader.manual_multiprocess_sharding = False
2021-11-14 12:42:53.904 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-14 12:42:53.904 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-14 12:42:53.904 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-14 12:42:53.904 type = bert-base-cased
2021-11-14 12:42:53.904 dataset_reader.tokenizer.add_special_tokens = True
2021-11-14 12:42:53.904 dataset_reader.tokenizer.max_length = 128
2021-11-14 12:42:53.904 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-14 12:42:53.905 dataset_reader.text_token_indexers.type = ref
2021-11-14 12:42:53.906 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-14 12:42:53.907 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-14 12:42:53.907 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-14 12:42:53.907 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-14 12:42:53.907 type = bert-base-cased
2021-11-14 12:42:53.907 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-14 12:42:53.907 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-14 12:42:53.907 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-14 12:42:53.908 dataset_reader.to_index = 6
2021-11-14 12:42:53.908 dataset_reader.type = example_reader
2021-11-14 12:42:53.908 dataset_reader.max_instances = None
2021-11-14 12:42:53.908 dataset_reader.manual_distributed_sharding = False
2021-11-14 12:42:53.908 dataset_reader.manual_multiprocess_sharding = False
2021-11-14 12:42:53.908 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-14 12:42:53.908 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-14 12:42:53.909 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-14 12:42:53.909 type = bert-base-cased
2021-11-14 12:42:53.909 dataset_reader.tokenizer.add_special_tokens = True
2021-11-14 12:42:53.909 dataset_reader.tokenizer.max_length = 128
2021-11-14 12:42:53.909 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-14 12:42:53.910 dataset_reader.text_token_indexers.type = ref
2021-11-14 12:42:53.910 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-14 12:42:53.910 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-14 12:42:53.911 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-14 12:42:53.911 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-14 12:42:53.911 type = bert-base-cased
2021-11-14 12:42:53.911 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-14 12:42:53.911 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-14 12:42:53.911 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-14 12:42:53.912 dataset_reader.to_index = 6
2021-11-14 12:42:53.912 type = from_instances
2021-11-14 12:42:53.912 Loading token dictionary from /tmp/tmp_pfw2wyp/vocabulary.
2021-11-14 12:42:53.913 model.type = sentence_level_classifier
2021-11-14 12:42:53.914 model.embedder.type = ref
2021-11-14 12:42:53.916 model.embedder.type = basic
2021-11-14 12:42:53.916 model.embedder.token_embedders.type = ref
2021-11-14 12:42:53.918 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-14 12:42:53.918 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-14 12:42:53.919 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-14 12:42:53.919 type = bert-base-cased
2021-11-14 12:42:53.920 model.embedder.token_embedders.tokens.max_length = None
2021-11-14 12:42:53.920 model.embedder.token_embedders.tokens.sub_module = None
2021-11-14 12:42:53.920 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-14 12:42:53.920 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-14 12:42:53.920 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-14 12:42:53.921 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-14 12:42:53.921 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-14 12:42:53.921 model.embedder.token_embedders.tokens.load_weights = True
2021-11-14 12:42:53.921 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-14 12:42:53.922 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-14 12:42:53.922 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-14 12:42:53.974 model.encoder.type = bert_pooler
2021-11-14 12:42:53.974 model.encoder.type = bert_pooler
2021-11-14 12:42:53.974 model.encoder.pretrained_model = bert-base-cased
2021-11-14 12:42:53.975 type = bert-base-cased
2021-11-14 12:42:53.975 model.encoder.override_weights_file = None
2021-11-14 12:42:53.975 model.encoder.override_weights_strip_prefix = None
2021-11-14 12:42:53.975 model.encoder.load_weights = True
2021-11-14 12:42:53.975 model.encoder.requires_grad = True
2021-11-14 12:42:53.975 model.encoder.dropout = 0.0
2021-11-14 12:42:53.975 model.encoder.transformer_kwargs = None
2021-11-14 12:42:54.178 removing temporary unarchived model dir at /tmp/tmp_pfw2wyp
2021-11-14 13:28:52.689 Plugin allennlp_models available
2021-11-14 13:28:52.705 Plugin allennlp_server available
2021-11-14 13:28:52.706 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-14 13:28:52.707 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp34g0nnp3
2021-11-14 13:28:54.938 dataset_reader.type = example_reader
2021-11-14 13:28:54.938 dataset_reader.max_instances = None
2021-11-14 13:28:54.938 dataset_reader.manual_distributed_sharding = False
2021-11-14 13:28:54.938 dataset_reader.manual_multiprocess_sharding = False
2021-11-14 13:28:54.939 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-14 13:28:54.939 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-14 13:28:54.939 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-14 13:28:54.939 type = bert-base-cased
2021-11-14 13:28:54.939 dataset_reader.tokenizer.add_special_tokens = True
2021-11-14 13:28:54.939 dataset_reader.tokenizer.max_length = 128
2021-11-14 13:28:54.939 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-14 13:28:54.940 dataset_reader.text_token_indexers.type = ref
2021-11-14 13:28:54.942 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-14 13:28:54.942 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-14 13:28:54.942 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-14 13:28:54.942 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-14 13:28:54.942 type = bert-base-cased
2021-11-14 13:28:54.942 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-14 13:28:54.942 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-14 13:28:54.942 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-14 13:28:54.943 dataset_reader.to_index = 6
2021-11-14 13:28:54.943 dataset_reader.type = example_reader
2021-11-14 13:28:54.944 dataset_reader.max_instances = None
2021-11-14 13:28:54.944 dataset_reader.manual_distributed_sharding = False
2021-11-14 13:28:54.944 dataset_reader.manual_multiprocess_sharding = False
2021-11-14 13:28:54.944 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-14 13:28:54.944 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-14 13:28:54.944 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-14 13:28:54.944 type = bert-base-cased
2021-11-14 13:28:54.944 dataset_reader.tokenizer.add_special_tokens = True
2021-11-14 13:28:54.944 dataset_reader.tokenizer.max_length = 128
2021-11-14 13:28:54.944 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-14 13:28:54.945 dataset_reader.text_token_indexers.type = ref
2021-11-14 13:28:54.946 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-14 13:28:54.946 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-14 13:28:54.946 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-14 13:28:54.946 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-14 13:28:54.946 type = bert-base-cased
2021-11-14 13:28:54.946 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-14 13:28:54.946 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-14 13:28:54.946 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-14 13:28:54.947 dataset_reader.to_index = 6
2021-11-14 13:28:54.948 type = from_instances
2021-11-14 13:28:54.948 Loading token dictionary from /tmp/tmp34g0nnp3/vocabulary.
2021-11-14 13:28:54.948 model.type = sentence_level_classifier
2021-11-14 13:28:54.948 model.embedder.type = ref
2021-11-14 13:28:54.949 model.embedder.type = basic
2021-11-14 13:28:54.949 model.embedder.token_embedders.type = ref
2021-11-14 13:28:54.950 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-14 13:28:54.950 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-14 13:28:54.950 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-14 13:28:54.950 type = bert-base-cased
2021-11-14 13:28:54.950 model.embedder.token_embedders.tokens.max_length = None
2021-11-14 13:28:54.950 model.embedder.token_embedders.tokens.sub_module = None
2021-11-14 13:28:54.950 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-14 13:28:54.951 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-14 13:28:54.951 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-14 13:28:54.951 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-14 13:28:54.951 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-14 13:28:54.951 model.embedder.token_embedders.tokens.load_weights = True
2021-11-14 13:28:54.951 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-14 13:28:54.951 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-14 13:28:54.951 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-14 13:28:54.991 model.encoder.type = bert_pooler
2021-11-14 13:28:54.991 model.encoder.type = bert_pooler
2021-11-14 13:28:54.991 model.encoder.pretrained_model = bert-base-cased
2021-11-14 13:28:54.991 type = bert-base-cased
2021-11-14 13:28:54.991 model.encoder.override_weights_file = None
2021-11-14 13:28:54.991 model.encoder.override_weights_strip_prefix = None
2021-11-14 13:28:54.991 model.encoder.load_weights = True
2021-11-14 13:28:54.991 model.encoder.requires_grad = True
2021-11-14 13:28:54.991 model.encoder.dropout = 0.0
2021-11-14 13:28:54.991 model.encoder.transformer_kwargs = None
2021-11-14 13:28:55.192 removing temporary unarchived model dir at /tmp/tmp34g0nnp3
2021-11-14 21:31:37.317 Plugin allennlp_models available
2021-11-14 21:31:37.326 Plugin allennlp_server available
2021-11-14 21:31:37.327 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-14 21:31:37.327 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpjzqbqnsa
2021-11-14 21:31:39.438 dataset_reader.type = example_reader
2021-11-14 21:31:39.438 dataset_reader.max_instances = None
2021-11-14 21:31:39.439 dataset_reader.manual_distributed_sharding = False
2021-11-14 21:31:39.439 dataset_reader.manual_multiprocess_sharding = False
2021-11-14 21:31:39.439 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-14 21:31:39.439 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-14 21:31:39.439 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-14 21:31:39.439 type = bert-base-cased
2021-11-14 21:31:39.439 dataset_reader.tokenizer.add_special_tokens = True
2021-11-14 21:31:39.439 dataset_reader.tokenizer.max_length = 128
2021-11-14 21:31:39.439 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-14 21:31:39.440 dataset_reader.text_token_indexers.type = ref
2021-11-14 21:31:39.441 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-14 21:31:39.441 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-14 21:31:39.442 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-14 21:31:39.442 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-14 21:31:39.442 type = bert-base-cased
2021-11-14 21:31:39.442 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-14 21:31:39.442 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-14 21:31:39.442 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-14 21:31:39.443 dataset_reader.to_index = 6
2021-11-14 21:31:39.443 dataset_reader.type = example_reader
2021-11-14 21:31:39.443 dataset_reader.max_instances = None
2021-11-14 21:31:39.443 dataset_reader.manual_distributed_sharding = False
2021-11-14 21:31:39.443 dataset_reader.manual_multiprocess_sharding = False
2021-11-14 21:31:39.443 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-14 21:31:39.443 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-14 21:31:39.443 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-14 21:31:39.443 type = bert-base-cased
2021-11-14 21:31:39.444 dataset_reader.tokenizer.add_special_tokens = True
2021-11-14 21:31:39.444 dataset_reader.tokenizer.max_length = 128
2021-11-14 21:31:39.444 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-14 21:31:39.445 dataset_reader.text_token_indexers.type = ref
2021-11-14 21:31:39.445 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-14 21:31:39.445 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-14 21:31:39.445 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-14 21:31:39.445 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-14 21:31:39.446 type = bert-base-cased
2021-11-14 21:31:39.446 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-14 21:31:39.446 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-14 21:31:39.446 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-14 21:31:39.446 dataset_reader.to_index = 6
2021-11-14 21:31:39.447 type = from_instances
2021-11-14 21:31:39.447 Loading token dictionary from /tmp/tmpjzqbqnsa/vocabulary.
2021-11-14 21:31:39.447 model.type = sentence_level_classifier
2021-11-14 21:31:39.447 model.embedder.type = ref
2021-11-14 21:31:39.448 model.embedder.type = basic
2021-11-14 21:31:39.448 model.embedder.token_embedders.type = ref
2021-11-14 21:31:39.449 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-14 21:31:39.449 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-14 21:31:39.450 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-14 21:31:39.450 type = bert-base-cased
2021-11-14 21:31:39.450 model.embedder.token_embedders.tokens.max_length = None
2021-11-14 21:31:39.450 model.embedder.token_embedders.tokens.sub_module = None
2021-11-14 21:31:39.450 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-14 21:31:39.450 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-14 21:31:39.450 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-14 21:31:39.450 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-14 21:31:39.450 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-14 21:31:39.450 model.embedder.token_embedders.tokens.load_weights = True
2021-11-14 21:31:39.450 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-14 21:31:39.450 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-14 21:31:39.450 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-14 21:31:39.500 model.encoder.type = bert_pooler
2021-11-14 21:31:39.500 model.encoder.type = bert_pooler
2021-11-14 21:31:39.500 model.encoder.pretrained_model = bert-base-cased
2021-11-14 21:31:39.501 type = bert-base-cased
2021-11-14 21:31:39.501 model.encoder.override_weights_file = None
2021-11-14 21:31:39.501 model.encoder.override_weights_strip_prefix = None
2021-11-14 21:31:39.501 model.encoder.load_weights = True
2021-11-14 21:31:39.501 model.encoder.requires_grad = True
2021-11-14 21:31:39.501 model.encoder.dropout = 0.0
2021-11-14 21:31:39.501 model.encoder.transformer_kwargs = None
2021-11-14 21:31:39.700 removing temporary unarchived model dir at /tmp/tmpjzqbqnsa
2021-11-14 21:31:45.082 Plugin allennlp_models available
2021-11-14 21:31:45.090 Plugin allennlp_server available
2021-11-14 21:31:45.091 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-14 21:31:45.091 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp6ho98rsb
2021-11-14 21:31:47.200 dataset_reader.type = example_reader
2021-11-14 21:31:47.200 dataset_reader.max_instances = None
2021-11-14 21:31:47.200 dataset_reader.manual_distributed_sharding = False
2021-11-14 21:31:47.200 dataset_reader.manual_multiprocess_sharding = False
2021-11-14 21:31:47.200 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-14 21:31:47.200 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-14 21:31:47.200 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-14 21:31:47.200 type = bert-base-cased
2021-11-14 21:31:47.201 dataset_reader.tokenizer.add_special_tokens = True
2021-11-14 21:31:47.201 dataset_reader.tokenizer.max_length = 128
2021-11-14 21:31:47.201 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-14 21:31:47.202 dataset_reader.text_token_indexers.type = ref
2021-11-14 21:31:47.203 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-14 21:31:47.203 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-14 21:31:47.203 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-14 21:31:47.203 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-14 21:31:47.203 type = bert-base-cased
2021-11-14 21:31:47.203 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-14 21:31:47.203 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-14 21:31:47.203 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-14 21:31:47.204 dataset_reader.to_index = 6
2021-11-14 21:31:47.204 dataset_reader.type = example_reader
2021-11-14 21:31:47.204 dataset_reader.max_instances = None
2021-11-14 21:31:47.205 dataset_reader.manual_distributed_sharding = False
2021-11-14 21:31:47.205 dataset_reader.manual_multiprocess_sharding = False
2021-11-14 21:31:47.205 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-14 21:31:47.205 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-14 21:31:47.205 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-14 21:31:47.206 type = bert-base-cased
2021-11-14 21:31:47.206 dataset_reader.tokenizer.add_special_tokens = True
2021-11-14 21:31:47.206 dataset_reader.tokenizer.max_length = 128
2021-11-14 21:31:47.206 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-14 21:31:47.207 dataset_reader.text_token_indexers.type = ref
2021-11-14 21:31:47.208 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-14 21:31:47.208 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-14 21:31:47.208 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-14 21:31:47.208 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-14 21:31:47.208 type = bert-base-cased
2021-11-14 21:31:47.208 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-14 21:31:47.208 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-14 21:31:47.208 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-14 21:31:47.209 dataset_reader.to_index = 6
2021-11-14 21:31:47.209 type = from_instances
2021-11-14 21:31:47.209 Loading token dictionary from /tmp/tmp6ho98rsb/vocabulary.
2021-11-14 21:31:47.210 model.type = sentence_level_classifier
2021-11-14 21:31:47.210 model.embedder.type = ref
2021-11-14 21:31:47.211 model.embedder.type = basic
2021-11-14 21:31:47.211 model.embedder.token_embedders.type = ref
2021-11-14 21:31:47.212 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-14 21:31:47.212 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-14 21:31:47.212 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-14 21:31:47.213 type = bert-base-cased
2021-11-14 21:31:47.213 model.embedder.token_embedders.tokens.max_length = None
2021-11-14 21:31:47.213 model.embedder.token_embedders.tokens.sub_module = None
2021-11-14 21:31:47.214 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-14 21:31:47.214 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-14 21:31:47.214 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-14 21:31:47.214 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-14 21:31:47.214 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-14 21:31:47.215 model.embedder.token_embedders.tokens.load_weights = True
2021-11-14 21:31:47.215 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-14 21:31:47.215 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-14 21:31:47.215 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-14 21:31:47.268 model.encoder.type = bert_pooler
2021-11-14 21:31:47.268 model.encoder.type = bert_pooler
2021-11-14 21:31:47.268 model.encoder.pretrained_model = bert-base-cased
2021-11-14 21:31:47.268 type = bert-base-cased
2021-11-14 21:31:47.269 model.encoder.override_weights_file = None
2021-11-14 21:31:47.269 model.encoder.override_weights_strip_prefix = None
2021-11-14 21:31:47.269 model.encoder.load_weights = True
2021-11-14 21:31:47.269 model.encoder.requires_grad = True
2021-11-14 21:31:47.269 model.encoder.dropout = 0.0
2021-11-14 21:31:47.269 model.encoder.transformer_kwargs = None
2021-11-14 21:31:47.492 removing temporary unarchived model dir at /tmp/tmp6ho98rsb
2021-11-15 17:09:25.583 Plugin allennlp_models available
2021-11-15 17:09:25.589 Plugin allennlp_server available
2021-11-15 17:09:25.590 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-15 17:09:25.590 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp0n6dm1_k
2021-11-15 17:09:27.683 dataset_reader.type = example_reader
2021-11-15 17:09:27.683 dataset_reader.max_instances = None
2021-11-15 17:09:27.684 dataset_reader.manual_distributed_sharding = False
2021-11-15 17:09:27.684 dataset_reader.manual_multiprocess_sharding = False
2021-11-15 17:09:27.684 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-15 17:09:27.684 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-15 17:09:27.684 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-15 17:09:27.684 type = bert-base-cased
2021-11-15 17:09:27.684 dataset_reader.tokenizer.add_special_tokens = True
2021-11-15 17:09:27.684 dataset_reader.tokenizer.max_length = 128
2021-11-15 17:09:27.684 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-15 17:09:27.685 dataset_reader.text_token_indexers.type = ref
2021-11-15 17:09:27.686 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-15 17:09:27.687 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-15 17:09:27.687 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-15 17:09:27.687 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-15 17:09:27.687 type = bert-base-cased
2021-11-15 17:09:27.687 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-15 17:09:27.687 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-15 17:09:27.687 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-15 17:09:27.688 dataset_reader.to_index = 6
2021-11-15 17:09:27.688 dataset_reader.type = example_reader
2021-11-15 17:09:27.688 dataset_reader.max_instances = None
2021-11-15 17:09:27.688 dataset_reader.manual_distributed_sharding = False
2021-11-15 17:09:27.688 dataset_reader.manual_multiprocess_sharding = False
2021-11-15 17:09:27.689 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-15 17:09:27.689 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-15 17:09:27.689 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-15 17:09:27.689 type = bert-base-cased
2021-11-15 17:09:27.689 dataset_reader.tokenizer.add_special_tokens = True
2021-11-15 17:09:27.689 dataset_reader.tokenizer.max_length = 128
2021-11-15 17:09:27.689 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-15 17:09:27.690 dataset_reader.text_token_indexers.type = ref
2021-11-15 17:09:27.690 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-15 17:09:27.691 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-15 17:09:27.691 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-15 17:09:27.691 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-15 17:09:27.691 type = bert-base-cased
2021-11-15 17:09:27.691 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-15 17:09:27.691 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-15 17:09:27.691 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-15 17:09:27.692 dataset_reader.to_index = 6
2021-11-15 17:09:27.692 type = from_instances
2021-11-15 17:09:27.692 Loading token dictionary from /tmp/tmp0n6dm1_k/vocabulary.
2021-11-15 17:09:27.693 model.type = sentence_level_classifier
2021-11-15 17:09:27.693 model.embedder.type = ref
2021-11-15 17:09:27.694 model.embedder.type = basic
2021-11-15 17:09:27.694 model.embedder.token_embedders.type = ref
2021-11-15 17:09:27.695 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-15 17:09:27.695 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-15 17:09:27.695 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-15 17:09:27.695 type = bert-base-cased
2021-11-15 17:09:27.695 model.embedder.token_embedders.tokens.max_length = None
2021-11-15 17:09:27.695 model.embedder.token_embedders.tokens.sub_module = None
2021-11-15 17:09:27.695 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-15 17:09:27.695 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-15 17:09:27.696 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-15 17:09:27.696 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-15 17:09:27.696 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-15 17:09:27.696 model.embedder.token_embedders.tokens.load_weights = True
2021-11-15 17:09:27.696 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-15 17:09:27.696 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-15 17:09:27.696 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-15 17:09:27.745 model.encoder.type = bert_pooler
2021-11-15 17:09:27.746 model.encoder.type = bert_pooler
2021-11-15 17:09:27.746 model.encoder.pretrained_model = bert-base-cased
2021-11-15 17:09:27.746 type = bert-base-cased
2021-11-15 17:09:27.746 model.encoder.override_weights_file = None
2021-11-15 17:09:27.746 model.encoder.override_weights_strip_prefix = None
2021-11-15 17:09:27.746 model.encoder.load_weights = True
2021-11-15 17:09:27.746 model.encoder.requires_grad = True
2021-11-15 17:09:27.746 model.encoder.dropout = 0.0
2021-11-15 17:09:27.746 model.encoder.transformer_kwargs = None
2021-11-15 17:09:27.937 removing temporary unarchived model dir at /tmp/tmp0n6dm1_k
2021-11-15 17:09:54.899 Plugin allennlp_models available
2021-11-15 17:09:54.907 Plugin allennlp_server available
2021-11-15 17:09:54.908 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-15 17:09:54.908 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpi2vwiju5
2021-11-15 17:09:57.011 dataset_reader.type = example_reader
2021-11-15 17:09:57.011 dataset_reader.max_instances = None
2021-11-15 17:09:57.011 dataset_reader.manual_distributed_sharding = False
2021-11-15 17:09:57.011 dataset_reader.manual_multiprocess_sharding = False
2021-11-15 17:09:57.011 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-15 17:09:57.012 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-15 17:09:57.012 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-15 17:09:57.012 type = bert-base-cased
2021-11-15 17:09:57.012 dataset_reader.tokenizer.add_special_tokens = True
2021-11-15 17:09:57.012 dataset_reader.tokenizer.max_length = 128
2021-11-15 17:09:57.012 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-15 17:09:57.013 dataset_reader.text_token_indexers.type = ref
2021-11-15 17:09:57.014 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-15 17:09:57.014 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-15 17:09:57.014 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-15 17:09:57.014 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-15 17:09:57.015 type = bert-base-cased
2021-11-15 17:09:57.015 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-15 17:09:57.015 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-15 17:09:57.015 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-15 17:09:57.016 dataset_reader.to_index = 6
2021-11-15 17:09:57.016 dataset_reader.type = example_reader
2021-11-15 17:09:57.016 dataset_reader.max_instances = None
2021-11-15 17:09:57.016 dataset_reader.manual_distributed_sharding = False
2021-11-15 17:09:57.016 dataset_reader.manual_multiprocess_sharding = False
2021-11-15 17:09:57.016 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-15 17:09:57.016 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-15 17:09:57.016 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-15 17:09:57.017 type = bert-base-cased
2021-11-15 17:09:57.017 dataset_reader.tokenizer.add_special_tokens = True
2021-11-15 17:09:57.017 dataset_reader.tokenizer.max_length = 128
2021-11-15 17:09:57.017 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-15 17:09:57.018 dataset_reader.text_token_indexers.type = ref
2021-11-15 17:09:57.019 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-15 17:09:57.019 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-15 17:09:57.019 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-15 17:09:57.019 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-15 17:09:57.019 type = bert-base-cased
2021-11-15 17:09:57.019 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-15 17:09:57.019 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-15 17:09:57.019 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-15 17:09:57.020 dataset_reader.to_index = 6
2021-11-15 17:09:57.020 type = from_instances
2021-11-15 17:09:57.020 Loading token dictionary from /tmp/tmpi2vwiju5/vocabulary.
2021-11-15 17:09:57.021 model.type = sentence_level_classifier
2021-11-15 17:09:57.022 model.embedder.type = ref
2021-11-15 17:09:57.024 model.embedder.type = basic
2021-11-15 17:09:57.024 model.embedder.token_embedders.type = ref
2021-11-15 17:09:57.026 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-15 17:09:57.026 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-15 17:09:57.027 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-15 17:09:57.027 type = bert-base-cased
2021-11-15 17:09:57.028 model.embedder.token_embedders.tokens.max_length = None
2021-11-15 17:09:57.028 model.embedder.token_embedders.tokens.sub_module = None
2021-11-15 17:09:57.028 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-15 17:09:57.028 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-15 17:09:57.028 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-15 17:09:57.028 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-15 17:09:57.029 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-15 17:09:57.029 model.embedder.token_embedders.tokens.load_weights = True
2021-11-15 17:09:57.029 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-15 17:09:57.029 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-15 17:09:57.029 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-15 17:09:57.081 model.encoder.type = bert_pooler
2021-11-15 17:09:57.082 model.encoder.type = bert_pooler
2021-11-15 17:09:57.082 model.encoder.pretrained_model = bert-base-cased
2021-11-15 17:09:57.082 type = bert-base-cased
2021-11-15 17:09:57.082 model.encoder.override_weights_file = None
2021-11-15 17:09:57.083 model.encoder.override_weights_strip_prefix = None
2021-11-15 17:09:57.083 model.encoder.load_weights = True
2021-11-15 17:09:57.083 model.encoder.requires_grad = True
2021-11-15 17:09:57.083 model.encoder.dropout = 0.0
2021-11-15 17:09:57.083 model.encoder.transformer_kwargs = None
2021-11-15 17:09:57.277 removing temporary unarchived model dir at /tmp/tmpi2vwiju5
2021-11-15 17:10:12.044 Plugin allennlp_models available
2021-11-15 17:10:12.053 Plugin allennlp_server available
2021-11-15 17:10:12.054 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-15 17:10:12.054 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpu99y8ti_
2021-11-15 17:10:14.162 dataset_reader.type = example_reader
2021-11-15 17:10:14.162 dataset_reader.max_instances = None
2021-11-15 17:10:14.162 dataset_reader.manual_distributed_sharding = False
2021-11-15 17:10:14.162 dataset_reader.manual_multiprocess_sharding = False
2021-11-15 17:10:14.162 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-15 17:10:14.162 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-15 17:10:14.162 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-15 17:10:14.162 type = bert-base-cased
2021-11-15 17:10:14.162 dataset_reader.tokenizer.add_special_tokens = True
2021-11-15 17:10:14.162 dataset_reader.tokenizer.max_length = 128
2021-11-15 17:10:14.162 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-15 17:10:14.164 dataset_reader.text_token_indexers.type = ref
2021-11-15 17:10:14.165 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-15 17:10:14.165 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-15 17:10:14.165 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-15 17:10:14.165 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-15 17:10:14.165 type = bert-base-cased
2021-11-15 17:10:14.165 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-15 17:10:14.165 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-15 17:10:14.165 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-15 17:10:14.166 dataset_reader.to_index = 6
2021-11-15 17:10:14.166 dataset_reader.type = example_reader
2021-11-15 17:10:14.167 dataset_reader.max_instances = None
2021-11-15 17:10:14.167 dataset_reader.manual_distributed_sharding = False
2021-11-15 17:10:14.167 dataset_reader.manual_multiprocess_sharding = False
2021-11-15 17:10:14.167 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-15 17:10:14.167 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-15 17:10:14.167 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-15 17:10:14.167 type = bert-base-cased
2021-11-15 17:10:14.167 dataset_reader.tokenizer.add_special_tokens = True
2021-11-15 17:10:14.167 dataset_reader.tokenizer.max_length = 128
2021-11-15 17:10:14.167 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-15 17:10:14.168 dataset_reader.text_token_indexers.type = ref
2021-11-15 17:10:14.169 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-15 17:10:14.169 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-15 17:10:14.169 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-15 17:10:14.169 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-15 17:10:14.169 type = bert-base-cased
2021-11-15 17:10:14.169 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-15 17:10:14.169 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-15 17:10:14.169 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-15 17:10:14.170 dataset_reader.to_index = 6
2021-11-15 17:10:14.170 type = from_instances
2021-11-15 17:10:14.170 Loading token dictionary from /tmp/tmpu99y8ti_/vocabulary.
2021-11-15 17:10:14.171 model.type = sentence_level_classifier
2021-11-15 17:10:14.171 model.embedder.type = ref
2021-11-15 17:10:14.172 model.embedder.type = basic
2021-11-15 17:10:14.172 model.embedder.token_embedders.type = ref
2021-11-15 17:10:14.174 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-15 17:10:14.174 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-15 17:10:14.174 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-15 17:10:14.174 type = bert-base-cased
2021-11-15 17:10:14.174 model.embedder.token_embedders.tokens.max_length = None
2021-11-15 17:10:14.174 model.embedder.token_embedders.tokens.sub_module = None
2021-11-15 17:10:14.174 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-15 17:10:14.174 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-15 17:10:14.174 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-15 17:10:14.174 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-15 17:10:14.174 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-15 17:10:14.174 model.embedder.token_embedders.tokens.load_weights = True
2021-11-15 17:10:14.174 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-15 17:10:14.174 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-15 17:10:14.174 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-15 17:10:14.212 model.encoder.type = bert_pooler
2021-11-15 17:10:14.213 model.encoder.type = bert_pooler
2021-11-15 17:10:14.213 model.encoder.pretrained_model = bert-base-cased
2021-11-15 17:10:14.213 type = bert-base-cased
2021-11-15 17:10:14.213 model.encoder.override_weights_file = None
2021-11-15 17:10:14.213 model.encoder.override_weights_strip_prefix = None
2021-11-15 17:10:14.213 model.encoder.load_weights = True
2021-11-15 17:10:14.213 model.encoder.requires_grad = True
2021-11-15 17:10:14.213 model.encoder.dropout = 0.0
2021-11-15 17:10:14.214 model.encoder.transformer_kwargs = None
2021-11-15 17:10:14.406 removing temporary unarchived model dir at /tmp/tmpu99y8ti_
2021-11-15 17:10:27.044 Plugin allennlp_models available
2021-11-15 17:10:27.051 Plugin allennlp_server available
2021-11-15 17:10:27.052 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-15 17:10:27.053 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpfo_ypaqq
2021-11-15 17:10:29.167 dataset_reader.type = example_reader
2021-11-15 17:10:29.167 dataset_reader.max_instances = None
2021-11-15 17:10:29.167 dataset_reader.manual_distributed_sharding = False
2021-11-15 17:10:29.171 dataset_reader.manual_multiprocess_sharding = False
2021-11-15 17:10:29.171 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-15 17:10:29.171 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-15 17:10:29.172 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-15 17:10:29.172 type = bert-base-cased
2021-11-15 17:10:29.172 dataset_reader.tokenizer.add_special_tokens = True
2021-11-15 17:10:29.172 dataset_reader.tokenizer.max_length = 128
2021-11-15 17:10:29.173 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-15 17:10:29.173 dataset_reader.text_token_indexers.type = ref
2021-11-15 17:10:29.174 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-15 17:10:29.175 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-15 17:10:29.175 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-15 17:10:29.175 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-15 17:10:29.175 type = bert-base-cased
2021-11-15 17:10:29.176 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-15 17:10:29.176 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-15 17:10:29.176 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-15 17:10:29.176 dataset_reader.to_index = 6
2021-11-15 17:10:29.177 dataset_reader.type = example_reader
2021-11-15 17:10:29.177 dataset_reader.max_instances = None
2021-11-15 17:10:29.177 dataset_reader.manual_distributed_sharding = False
2021-11-15 17:10:29.177 dataset_reader.manual_multiprocess_sharding = False
2021-11-15 17:10:29.177 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-15 17:10:29.177 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-15 17:10:29.178 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-15 17:10:29.178 type = bert-base-cased
2021-11-15 17:10:29.178 dataset_reader.tokenizer.add_special_tokens = True
2021-11-15 17:10:29.178 dataset_reader.tokenizer.max_length = 128
2021-11-15 17:10:29.178 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-15 17:10:29.179 dataset_reader.text_token_indexers.type = ref
2021-11-15 17:10:29.179 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-15 17:10:29.180 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-15 17:10:29.180 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-15 17:10:29.180 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-15 17:10:29.180 type = bert-base-cased
2021-11-15 17:10:29.180 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-15 17:10:29.180 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-15 17:10:29.181 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-15 17:10:29.182 dataset_reader.to_index = 6
2021-11-15 17:10:29.183 type = from_instances
2021-11-15 17:10:29.183 Loading token dictionary from /tmp/tmpfo_ypaqq/vocabulary.
2021-11-15 17:10:29.184 model.type = sentence_level_classifier
2021-11-15 17:10:29.185 model.embedder.type = ref
2021-11-15 17:10:29.187 model.embedder.type = basic
2021-11-15 17:10:29.188 model.embedder.token_embedders.type = ref
2021-11-15 17:10:29.191 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-15 17:10:29.192 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-15 17:10:29.192 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-15 17:10:29.193 type = bert-base-cased
2021-11-15 17:10:29.193 model.embedder.token_embedders.tokens.max_length = None
2021-11-15 17:10:29.193 model.embedder.token_embedders.tokens.sub_module = None
2021-11-15 17:10:29.193 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-15 17:10:29.194 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-15 17:10:29.194 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-15 17:10:29.194 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-15 17:10:29.194 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-15 17:10:29.194 model.embedder.token_embedders.tokens.load_weights = True
2021-11-15 17:10:29.195 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-15 17:10:29.195 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-15 17:10:29.195 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-15 17:10:29.245 model.encoder.type = bert_pooler
2021-11-15 17:10:29.245 model.encoder.type = bert_pooler
2021-11-15 17:10:29.245 model.encoder.pretrained_model = bert-base-cased
2021-11-15 17:10:29.245 type = bert-base-cased
2021-11-15 17:10:29.246 model.encoder.override_weights_file = None
2021-11-15 17:10:29.246 model.encoder.override_weights_strip_prefix = None
2021-11-15 17:10:29.246 model.encoder.load_weights = True
2021-11-15 17:10:29.246 model.encoder.requires_grad = True
2021-11-15 17:10:29.246 model.encoder.dropout = 0.0
2021-11-15 17:10:29.246 model.encoder.transformer_kwargs = None
2021-11-15 17:10:29.438 removing temporary unarchived model dir at /tmp/tmpfo_ypaqq
2021-11-15 17:35:34.162 Plugin allennlp_models available
2021-11-15 17:35:34.171 Plugin allennlp_server available
2021-11-15 17:35:34.172 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-15 17:35:34.173 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp8jrnlbn7
2021-11-15 17:35:36.273 dataset_reader.type = example_reader
2021-11-15 17:35:36.273 dataset_reader.max_instances = None
2021-11-15 17:35:36.273 dataset_reader.manual_distributed_sharding = False
2021-11-15 17:35:36.274 dataset_reader.manual_multiprocess_sharding = False
2021-11-15 17:35:36.274 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-15 17:35:36.274 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-15 17:35:36.274 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-15 17:35:36.274 type = bert-base-cased
2021-11-15 17:35:36.274 dataset_reader.tokenizer.add_special_tokens = True
2021-11-15 17:35:36.274 dataset_reader.tokenizer.max_length = 128
2021-11-15 17:35:36.274 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-15 17:35:36.275 dataset_reader.text_token_indexers.type = ref
2021-11-15 17:35:36.276 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-15 17:35:36.277 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-15 17:35:36.277 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-15 17:35:36.277 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-15 17:35:36.277 type = bert-base-cased
2021-11-15 17:35:36.277 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-15 17:35:36.277 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-15 17:35:36.277 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-15 17:35:36.278 dataset_reader.to_index = 6
2021-11-15 17:35:36.278 dataset_reader.type = example_reader
2021-11-15 17:35:36.278 dataset_reader.max_instances = None
2021-11-15 17:35:36.278 dataset_reader.manual_distributed_sharding = False
2021-11-15 17:35:36.278 dataset_reader.manual_multiprocess_sharding = False
2021-11-15 17:35:36.279 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-15 17:35:36.279 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-15 17:35:36.279 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-15 17:35:36.279 type = bert-base-cased
2021-11-15 17:35:36.279 dataset_reader.tokenizer.add_special_tokens = True
2021-11-15 17:35:36.279 dataset_reader.tokenizer.max_length = 128
2021-11-15 17:35:36.279 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-15 17:35:36.280 dataset_reader.text_token_indexers.type = ref
2021-11-15 17:35:36.281 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-15 17:35:36.281 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-15 17:35:36.281 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-15 17:35:36.281 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-15 17:35:36.281 type = bert-base-cased
2021-11-15 17:35:36.281 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-15 17:35:36.281 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-15 17:35:36.281 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-15 17:35:36.282 dataset_reader.to_index = 6
2021-11-15 17:35:36.282 type = from_instances
2021-11-15 17:35:36.282 Loading token dictionary from /tmp/tmp8jrnlbn7/vocabulary.
2021-11-15 17:35:36.282 model.type = sentence_level_classifier
2021-11-15 17:35:36.283 model.embedder.type = ref
2021-11-15 17:35:36.283 model.embedder.type = basic
2021-11-15 17:35:36.284 model.embedder.token_embedders.type = ref
2021-11-15 17:35:36.284 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-15 17:35:36.285 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-15 17:35:36.285 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-15 17:35:36.285 type = bert-base-cased
2021-11-15 17:35:36.285 model.embedder.token_embedders.tokens.max_length = None
2021-11-15 17:35:36.285 model.embedder.token_embedders.tokens.sub_module = None
2021-11-15 17:35:36.285 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-15 17:35:36.285 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-15 17:35:36.285 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-15 17:35:36.285 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-15 17:35:36.285 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-15 17:35:36.285 model.embedder.token_embedders.tokens.load_weights = True
2021-11-15 17:35:36.285 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-15 17:35:36.285 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-15 17:35:36.285 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-15 17:35:36.353 model.encoder.type = bert_pooler
2021-11-15 17:35:36.354 model.encoder.type = bert_pooler
2021-11-15 17:35:36.354 model.encoder.pretrained_model = bert-base-cased
2021-11-15 17:35:36.354 type = bert-base-cased
2021-11-15 17:35:36.354 model.encoder.override_weights_file = None
2021-11-15 17:35:36.354 model.encoder.override_weights_strip_prefix = None
2021-11-15 17:35:36.354 model.encoder.load_weights = True
2021-11-15 17:35:36.354 model.encoder.requires_grad = True
2021-11-15 17:35:36.355 model.encoder.dropout = 0.0
2021-11-15 17:35:36.355 model.encoder.transformer_kwargs = None
2021-11-15 17:35:36.551 removing temporary unarchived model dir at /tmp/tmp8jrnlbn7
2021-11-16 08:50:51.728 Plugin allennlp_models available
2021-11-16 08:50:51.735 Plugin allennlp_server available
2021-11-16 08:50:51.735 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-16 08:50:51.736 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp5m2jsz02
2021-11-16 08:50:53.853 dataset_reader.type = example_reader
2021-11-16 08:50:53.853 dataset_reader.max_instances = None
2021-11-16 08:50:53.853 dataset_reader.manual_distributed_sharding = False
2021-11-16 08:50:53.853 dataset_reader.manual_multiprocess_sharding = False
2021-11-16 08:50:53.853 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-16 08:50:53.853 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-16 08:50:53.854 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-16 08:50:53.854 type = bert-base-cased
2021-11-16 08:50:53.854 dataset_reader.tokenizer.add_special_tokens = True
2021-11-16 08:50:53.854 dataset_reader.tokenizer.max_length = 128
2021-11-16 08:50:53.854 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-16 08:50:53.855 dataset_reader.text_token_indexers.type = ref
2021-11-16 08:50:53.856 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-16 08:50:53.856 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-16 08:50:53.856 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-16 08:50:53.856 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-16 08:50:53.856 type = bert-base-cased
2021-11-16 08:50:53.857 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-16 08:50:53.857 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-16 08:50:53.857 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-16 08:50:53.857 dataset_reader.to_index = 6
2021-11-16 08:50:53.858 dataset_reader.type = example_reader
2021-11-16 08:50:53.858 dataset_reader.max_instances = None
2021-11-16 08:50:53.858 dataset_reader.manual_distributed_sharding = False
2021-11-16 08:50:53.858 dataset_reader.manual_multiprocess_sharding = False
2021-11-16 08:50:53.858 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-16 08:50:53.858 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-16 08:50:53.858 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-16 08:50:53.858 type = bert-base-cased
2021-11-16 08:50:53.858 dataset_reader.tokenizer.add_special_tokens = True
2021-11-16 08:50:53.858 dataset_reader.tokenizer.max_length = 128
2021-11-16 08:50:53.858 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-16 08:50:53.859 dataset_reader.text_token_indexers.type = ref
2021-11-16 08:50:53.860 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-16 08:50:53.860 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-16 08:50:53.860 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-16 08:50:53.860 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-16 08:50:53.861 type = bert-base-cased
2021-11-16 08:50:53.861 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-16 08:50:53.861 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-16 08:50:53.861 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-16 08:50:53.862 dataset_reader.to_index = 6
2021-11-16 08:50:53.862 type = from_instances
2021-11-16 08:50:53.862 Loading token dictionary from /tmp/tmp5m2jsz02/vocabulary.
2021-11-16 08:50:53.863 model.type = sentence_level_classifier
2021-11-16 08:50:53.863 model.embedder.type = ref
2021-11-16 08:50:53.864 model.embedder.type = basic
2021-11-16 08:50:53.864 model.embedder.token_embedders.type = ref
2021-11-16 08:50:53.865 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-16 08:50:53.865 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-16 08:50:53.865 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-16 08:50:53.865 type = bert-base-cased
2021-11-16 08:50:53.865 model.embedder.token_embedders.tokens.max_length = None
2021-11-16 08:50:53.866 model.embedder.token_embedders.tokens.sub_module = None
2021-11-16 08:50:53.866 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-16 08:50:53.866 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-16 08:50:53.866 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-16 08:50:53.866 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-16 08:50:53.866 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-16 08:50:53.867 model.embedder.token_embedders.tokens.load_weights = True
2021-11-16 08:50:53.867 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-16 08:50:53.867 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-16 08:50:53.867 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-16 08:50:53.908 model.encoder.type = bert_pooler
2021-11-16 08:50:53.908 model.encoder.type = bert_pooler
2021-11-16 08:50:53.908 model.encoder.pretrained_model = bert-base-cased
2021-11-16 08:50:53.908 type = bert-base-cased
2021-11-16 08:50:53.909 model.encoder.override_weights_file = None
2021-11-16 08:50:53.909 model.encoder.override_weights_strip_prefix = None
2021-11-16 08:50:53.909 model.encoder.load_weights = True
2021-11-16 08:50:53.909 model.encoder.requires_grad = True
2021-11-16 08:50:53.909 model.encoder.dropout = 0.0
2021-11-16 08:50:53.909 model.encoder.transformer_kwargs = None
2021-11-16 08:50:54.100 removing temporary unarchived model dir at /tmp/tmp5m2jsz02
2021-11-16 12:08:38.152 Plugin allennlp_models available
2021-11-16 12:08:38.157 Plugin allennlp_server available
2021-11-16 12:08:38.158 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-16 12:08:38.158 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmp8lul_kmm
2021-11-16 12:08:40.280 dataset_reader.type = example_reader
2021-11-16 12:08:40.280 dataset_reader.max_instances = None
2021-11-16 12:08:40.280 dataset_reader.manual_distributed_sharding = False
2021-11-16 12:08:40.282 dataset_reader.manual_multiprocess_sharding = False
2021-11-16 12:08:40.282 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-16 12:08:40.282 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-16 12:08:40.282 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-16 12:08:40.282 type = bert-base-cased
2021-11-16 12:08:40.282 dataset_reader.tokenizer.add_special_tokens = True
2021-11-16 12:08:40.282 dataset_reader.tokenizer.max_length = 128
2021-11-16 12:08:40.282 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-16 12:08:40.283 dataset_reader.text_token_indexers.type = ref
2021-11-16 12:08:40.284 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-16 12:08:40.285 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-16 12:08:40.285 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-16 12:08:40.285 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-16 12:08:40.285 type = bert-base-cased
2021-11-16 12:08:40.285 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-16 12:08:40.285 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-16 12:08:40.285 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-16 12:08:40.286 dataset_reader.to_index = 6
2021-11-16 12:08:40.286 dataset_reader.type = example_reader
2021-11-16 12:08:40.286 dataset_reader.max_instances = None
2021-11-16 12:08:40.286 dataset_reader.manual_distributed_sharding = False
2021-11-16 12:08:40.286 dataset_reader.manual_multiprocess_sharding = False
2021-11-16 12:08:40.286 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-16 12:08:40.287 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-16 12:08:40.287 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-16 12:08:40.287 type = bert-base-cased
2021-11-16 12:08:40.287 dataset_reader.tokenizer.add_special_tokens = True
2021-11-16 12:08:40.287 dataset_reader.tokenizer.max_length = 128
2021-11-16 12:08:40.287 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-16 12:08:40.288 dataset_reader.text_token_indexers.type = ref
2021-11-16 12:08:40.288 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-16 12:08:40.289 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-16 12:08:40.289 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-16 12:08:40.289 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-16 12:08:40.289 type = bert-base-cased
2021-11-16 12:08:40.289 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-16 12:08:40.289 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-16 12:08:40.289 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-16 12:08:40.290 dataset_reader.to_index = 6
2021-11-16 12:08:40.290 type = from_instances
2021-11-16 12:08:40.290 Loading token dictionary from /tmp/tmp8lul_kmm/vocabulary.
2021-11-16 12:08:40.291 model.type = sentence_level_classifier
2021-11-16 12:08:40.291 model.embedder.type = ref
2021-11-16 12:08:40.291 model.embedder.type = basic
2021-11-16 12:08:40.292 model.embedder.token_embedders.type = ref
2021-11-16 12:08:40.292 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-16 12:08:40.293 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-16 12:08:40.293 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-16 12:08:40.293 type = bert-base-cased
2021-11-16 12:08:40.293 model.embedder.token_embedders.tokens.max_length = None
2021-11-16 12:08:40.293 model.embedder.token_embedders.tokens.sub_module = None
2021-11-16 12:08:40.293 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-16 12:08:40.293 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-16 12:08:40.293 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-16 12:08:40.293 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-16 12:08:40.293 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-16 12:08:40.293 model.embedder.token_embedders.tokens.load_weights = True
2021-11-16 12:08:40.293 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-16 12:08:40.293 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-16 12:08:40.293 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-16 12:08:40.339 model.encoder.type = bert_pooler
2021-11-16 12:08:40.339 model.encoder.type = bert_pooler
2021-11-16 12:08:40.339 model.encoder.pretrained_model = bert-base-cased
2021-11-16 12:08:40.339 type = bert-base-cased
2021-11-16 12:08:40.340 model.encoder.override_weights_file = None
2021-11-16 12:08:40.340 model.encoder.override_weights_strip_prefix = None
2021-11-16 12:08:40.340 model.encoder.load_weights = True
2021-11-16 12:08:40.340 model.encoder.requires_grad = True
2021-11-16 12:08:40.340 model.encoder.dropout = 0.0
2021-11-16 12:08:40.340 model.encoder.transformer_kwargs = None
2021-11-16 12:08:40.513 removing temporary unarchived model dir at /tmp/tmp8lul_kmm
2021-11-16 12:50:01.284 Plugin allennlp_models available
2021-11-16 12:50:01.289 Plugin allennlp_server available
2021-11-16 12:50:01.290 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-16 12:50:01.290 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmple_iep_s
2021-11-16 12:50:03.412 dataset_reader.type = example_reader
2021-11-16 12:50:03.412 dataset_reader.max_instances = None
2021-11-16 12:50:03.412 dataset_reader.manual_distributed_sharding = False
2021-11-16 12:50:03.412 dataset_reader.manual_multiprocess_sharding = False
2021-11-16 12:50:03.412 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-16 12:50:03.412 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-16 12:50:03.412 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-16 12:50:03.412 type = bert-base-cased
2021-11-16 12:50:03.412 dataset_reader.tokenizer.add_special_tokens = True
2021-11-16 12:50:03.413 dataset_reader.tokenizer.max_length = 128
2021-11-16 12:50:03.413 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-16 12:50:03.414 dataset_reader.text_token_indexers.type = ref
2021-11-16 12:50:03.415 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-16 12:50:03.415 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-16 12:50:03.415 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-16 12:50:03.415 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-16 12:50:03.415 type = bert-base-cased
2021-11-16 12:50:03.415 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-16 12:50:03.415 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-16 12:50:03.415 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-16 12:50:03.416 dataset_reader.to_index = 6
2021-11-16 12:50:03.417 dataset_reader.type = example_reader
2021-11-16 12:50:03.417 dataset_reader.max_instances = None
2021-11-16 12:50:03.417 dataset_reader.manual_distributed_sharding = False
2021-11-16 12:50:03.417 dataset_reader.manual_multiprocess_sharding = False
2021-11-16 12:50:03.417 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-16 12:50:03.417 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-16 12:50:03.417 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-16 12:50:03.417 type = bert-base-cased
2021-11-16 12:50:03.417 dataset_reader.tokenizer.add_special_tokens = True
2021-11-16 12:50:03.417 dataset_reader.tokenizer.max_length = 128
2021-11-16 12:50:03.418 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-16 12:50:03.418 dataset_reader.text_token_indexers.type = ref
2021-11-16 12:50:03.419 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-16 12:50:03.419 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-16 12:50:03.419 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-16 12:50:03.419 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-16 12:50:03.419 type = bert-base-cased
2021-11-16 12:50:03.419 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-16 12:50:03.419 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-16 12:50:03.419 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-16 12:50:03.420 dataset_reader.to_index = 6
2021-11-16 12:50:03.421 type = from_instances
2021-11-16 12:50:03.421 Loading token dictionary from /tmp/tmple_iep_s/vocabulary.
2021-11-16 12:50:03.421 model.type = sentence_level_classifier
2021-11-16 12:50:03.421 model.embedder.type = ref
2021-11-16 12:50:03.422 model.embedder.type = basic
2021-11-16 12:50:03.422 model.embedder.token_embedders.type = ref
2021-11-16 12:50:03.423 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-16 12:50:03.423 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-16 12:50:03.424 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-16 12:50:03.424 type = bert-base-cased
2021-11-16 12:50:03.424 model.embedder.token_embedders.tokens.max_length = None
2021-11-16 12:50:03.424 model.embedder.token_embedders.tokens.sub_module = None
2021-11-16 12:50:03.424 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-16 12:50:03.424 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-16 12:50:03.424 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-16 12:50:03.424 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-16 12:50:03.424 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-16 12:50:03.424 model.embedder.token_embedders.tokens.load_weights = True
2021-11-16 12:50:03.424 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-16 12:50:03.424 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-16 12:50:03.424 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-16 12:50:03.473 model.encoder.type = bert_pooler
2021-11-16 12:50:03.473 model.encoder.type = bert_pooler
2021-11-16 12:50:03.474 model.encoder.pretrained_model = bert-base-cased
2021-11-16 12:50:03.474 type = bert-base-cased
2021-11-16 12:50:03.474 model.encoder.override_weights_file = None
2021-11-16 12:50:03.474 model.encoder.override_weights_strip_prefix = None
2021-11-16 12:50:03.474 model.encoder.load_weights = True
2021-11-16 12:50:03.474 model.encoder.requires_grad = True
2021-11-16 12:50:03.474 model.encoder.dropout = 0.0
2021-11-16 12:50:03.474 model.encoder.transformer_kwargs = None
2021-11-16 12:50:03.665 removing temporary unarchived model dir at /tmp/tmple_iep_s
2021-11-16 13:55:43.574 Plugin allennlp_models available
2021-11-16 13:55:43.579 Plugin allennlp_server available
2021-11-16 13:55:43.580 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-16 13:55:43.580 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpp0iwltqv
2021-11-16 13:55:45.790 dataset_reader.type = example_reader
2021-11-16 13:55:45.790 dataset_reader.max_instances = None
2021-11-16 13:55:45.790 dataset_reader.manual_distributed_sharding = False
2021-11-16 13:55:45.790 dataset_reader.manual_multiprocess_sharding = False
2021-11-16 13:55:45.790 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-16 13:55:45.791 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-16 13:55:45.791 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-16 13:55:45.791 type = bert-base-cased
2021-11-16 13:55:45.791 dataset_reader.tokenizer.add_special_tokens = True
2021-11-16 13:55:45.791 dataset_reader.tokenizer.max_length = 128
2021-11-16 13:55:45.791 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-16 13:55:45.792 dataset_reader.text_token_indexers.type = ref
2021-11-16 13:55:45.793 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-16 13:55:45.794 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-16 13:55:45.794 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-16 13:55:45.794 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-16 13:55:45.794 type = bert-base-cased
2021-11-16 13:55:45.794 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-16 13:55:45.794 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-16 13:55:45.794 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-16 13:55:45.795 dataset_reader.to_index = 6
2021-11-16 13:55:45.795 dataset_reader.type = example_reader
2021-11-16 13:55:45.795 dataset_reader.max_instances = None
2021-11-16 13:55:45.795 dataset_reader.manual_distributed_sharding = False
2021-11-16 13:55:45.795 dataset_reader.manual_multiprocess_sharding = False
2021-11-16 13:55:45.796 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-16 13:55:45.796 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-16 13:55:45.796 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-16 13:55:45.796 type = bert-base-cased
2021-11-16 13:55:45.796 dataset_reader.tokenizer.add_special_tokens = True
2021-11-16 13:55:45.796 dataset_reader.tokenizer.max_length = 128
2021-11-16 13:55:45.796 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-16 13:55:45.797 dataset_reader.text_token_indexers.type = ref
2021-11-16 13:55:45.799 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-16 13:55:45.800 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-16 13:55:45.800 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-16 13:55:45.800 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-16 13:55:45.801 type = bert-base-cased
2021-11-16 13:55:45.801 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-16 13:55:45.801 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-16 13:55:45.801 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-16 13:55:45.803 dataset_reader.to_index = 6
2021-11-16 13:55:45.803 type = from_instances
2021-11-16 13:55:45.804 Loading token dictionary from /tmp/tmpp0iwltqv/vocabulary.
2021-11-16 13:55:45.805 model.type = sentence_level_classifier
2021-11-16 13:55:45.805 model.embedder.type = ref
2021-11-16 13:55:45.807 model.embedder.type = basic
2021-11-16 13:55:45.808 model.embedder.token_embedders.type = ref
2021-11-16 13:55:45.810 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-16 13:55:45.810 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-16 13:55:45.811 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-16 13:55:45.811 type = bert-base-cased
2021-11-16 13:55:45.811 model.embedder.token_embedders.tokens.max_length = None
2021-11-16 13:55:45.812 model.embedder.token_embedders.tokens.sub_module = None
2021-11-16 13:55:45.812 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-16 13:55:45.812 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-16 13:55:45.812 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-16 13:55:45.812 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-16 13:55:45.813 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-16 13:55:45.813 model.embedder.token_embedders.tokens.load_weights = True
2021-11-16 13:55:45.813 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-16 13:55:45.813 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-16 13:55:45.813 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-16 13:55:45.866 model.encoder.type = bert_pooler
2021-11-16 13:55:45.866 model.encoder.type = bert_pooler
2021-11-16 13:55:45.866 model.encoder.pretrained_model = bert-base-cased
2021-11-16 13:55:45.867 type = bert-base-cased
2021-11-16 13:55:45.867 model.encoder.override_weights_file = None
2021-11-16 13:55:45.867 model.encoder.override_weights_strip_prefix = None
2021-11-16 13:55:45.867 model.encoder.load_weights = True
2021-11-16 13:55:45.867 model.encoder.requires_grad = True
2021-11-16 13:55:45.867 model.encoder.dropout = 0.0
2021-11-16 13:55:45.867 model.encoder.transformer_kwargs = None
2021-11-16 13:55:46.062 removing temporary unarchived model dir at /tmp/tmpp0iwltqv
2021-11-16 14:48:46.744 Plugin allennlp_models available
2021-11-16 14:48:46.748 Plugin allennlp_server available
2021-11-16 14:48:46.749 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-16 14:48:46.749 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmph5pic8v5
2021-11-16 14:48:48.940 dataset_reader.type = example_reader
2021-11-16 14:48:48.940 dataset_reader.max_instances = None
2021-11-16 14:48:48.940 dataset_reader.manual_distributed_sharding = False
2021-11-16 14:48:48.941 dataset_reader.manual_multiprocess_sharding = False
2021-11-16 14:48:48.941 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-16 14:48:48.941 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-16 14:48:48.942 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-16 14:48:48.942 type = bert-base-cased
2021-11-16 14:48:48.942 dataset_reader.tokenizer.add_special_tokens = True
2021-11-16 14:48:48.942 dataset_reader.tokenizer.max_length = 128
2021-11-16 14:48:48.942 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-16 14:48:48.943 dataset_reader.text_token_indexers.type = ref
2021-11-16 14:48:48.944 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-16 14:48:48.944 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-16 14:48:48.944 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-16 14:48:48.944 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-16 14:48:48.945 type = bert-base-cased
2021-11-16 14:48:48.945 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-16 14:48:48.945 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-16 14:48:48.945 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-16 14:48:48.946 dataset_reader.to_index = 6
2021-11-16 14:48:48.946 dataset_reader.type = example_reader
2021-11-16 14:48:48.946 dataset_reader.max_instances = None
2021-11-16 14:48:48.947 dataset_reader.manual_distributed_sharding = False
2021-11-16 14:48:48.947 dataset_reader.manual_multiprocess_sharding = False
2021-11-16 14:48:48.947 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-16 14:48:48.947 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-16 14:48:48.947 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-16 14:48:48.947 type = bert-base-cased
2021-11-16 14:48:48.947 dataset_reader.tokenizer.add_special_tokens = True
2021-11-16 14:48:48.947 dataset_reader.tokenizer.max_length = 128
2021-11-16 14:48:48.947 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-16 14:48:48.948 dataset_reader.text_token_indexers.type = ref
2021-11-16 14:48:48.949 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-16 14:48:48.949 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-16 14:48:48.949 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-16 14:48:48.950 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-16 14:48:48.950 type = bert-base-cased
2021-11-16 14:48:48.950 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-16 14:48:48.950 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-16 14:48:48.950 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-16 14:48:48.950 dataset_reader.to_index = 6
2021-11-16 14:48:48.951 type = from_instances
2021-11-16 14:48:48.951 Loading token dictionary from /tmp/tmph5pic8v5/vocabulary.
2021-11-16 14:48:48.951 model.type = sentence_level_classifier
2021-11-16 14:48:48.952 model.embedder.type = ref
2021-11-16 14:48:48.952 model.embedder.type = basic
2021-11-16 14:48:48.953 model.embedder.token_embedders.type = ref
2021-11-16 14:48:48.954 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-16 14:48:48.954 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-16 14:48:48.954 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-16 14:48:48.954 type = bert-base-cased
2021-11-16 14:48:48.954 model.embedder.token_embedders.tokens.max_length = None
2021-11-16 14:48:48.954 model.embedder.token_embedders.tokens.sub_module = None
2021-11-16 14:48:48.954 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-16 14:48:48.954 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-16 14:48:48.954 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-16 14:48:48.955 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-16 14:48:48.955 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-16 14:48:48.955 model.embedder.token_embedders.tokens.load_weights = True
2021-11-16 14:48:48.955 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-16 14:48:48.955 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-16 14:48:48.955 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-16 14:48:48.995 model.encoder.type = bert_pooler
2021-11-16 14:48:48.995 model.encoder.type = bert_pooler
2021-11-16 14:48:48.995 model.encoder.pretrained_model = bert-base-cased
2021-11-16 14:48:48.995 type = bert-base-cased
2021-11-16 14:48:48.995 model.encoder.override_weights_file = None
2021-11-16 14:48:48.995 model.encoder.override_weights_strip_prefix = None
2021-11-16 14:48:48.995 model.encoder.load_weights = True
2021-11-16 14:48:48.996 model.encoder.requires_grad = True
2021-11-16 14:48:48.996 model.encoder.dropout = 0.0
2021-11-16 14:48:48.996 model.encoder.transformer_kwargs = None
2021-11-16 14:48:49.194 removing temporary unarchived model dir at /tmp/tmph5pic8v5
2021-11-16 14:57:18.511 Plugin allennlp_models available
2021-11-16 14:57:18.516 Plugin allennlp_server available
2021-11-16 14:57:18.517 loading archive file models/sent_level_bert_ce_6levels.tar.gz
2021-11-16 14:57:18.517 extracting archive file models/sent_level_bert_ce_6levels.tar.gz to temp dir /tmp/tmpk7mrr67c
2021-11-16 14:57:20.717 dataset_reader.type = example_reader
2021-11-16 14:57:20.717 dataset_reader.max_instances = None
2021-11-16 14:57:20.718 dataset_reader.manual_distributed_sharding = False
2021-11-16 14:57:20.718 dataset_reader.manual_multiprocess_sharding = False
2021-11-16 14:57:20.718 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-16 14:57:20.718 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-16 14:57:20.719 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-16 14:57:20.719 type = bert-base-cased
2021-11-16 14:57:20.719 dataset_reader.tokenizer.add_special_tokens = True
2021-11-16 14:57:20.719 dataset_reader.tokenizer.max_length = 128
2021-11-16 14:57:20.719 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-16 14:57:20.720 dataset_reader.text_token_indexers.type = ref
2021-11-16 14:57:20.721 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-16 14:57:20.721 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-16 14:57:20.722 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-16 14:57:20.722 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-16 14:57:20.722 type = bert-base-cased
2021-11-16 14:57:20.722 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-16 14:57:20.722 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-16 14:57:20.722 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-16 14:57:20.723 dataset_reader.to_index = 6
2021-11-16 14:57:20.723 dataset_reader.type = example_reader
2021-11-16 14:57:20.724 dataset_reader.max_instances = None
2021-11-16 14:57:20.724 dataset_reader.manual_distributed_sharding = False
2021-11-16 14:57:20.724 dataset_reader.manual_multiprocess_sharding = False
2021-11-16 14:57:20.724 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-16 14:57:20.724 dataset_reader.tokenizer.type = pretrained_transformer
2021-11-16 14:57:20.725 dataset_reader.tokenizer.model_name = bert-base-cased
2021-11-16 14:57:20.725 type = bert-base-cased
2021-11-16 14:57:20.726 dataset_reader.tokenizer.add_special_tokens = True
2021-11-16 14:57:20.726 dataset_reader.tokenizer.max_length = 128
2021-11-16 14:57:20.726 dataset_reader.tokenizer.tokenizer_kwargs = None
2021-11-16 14:57:20.728 dataset_reader.text_token_indexers.type = ref
2021-11-16 14:57:20.730 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-16 14:57:20.730 dataset_reader.text_token_indexers.tokens.type = pretrained_transformer
2021-11-16 14:57:20.731 dataset_reader.text_token_indexers.tokens.token_min_padding_length = 0
2021-11-16 14:57:20.731 dataset_reader.text_token_indexers.tokens.model_name = bert-base-cased
2021-11-16 14:57:20.731 type = bert-base-cased
2021-11-16 14:57:20.731 dataset_reader.text_token_indexers.tokens.namespace = tags
2021-11-16 14:57:20.732 dataset_reader.text_token_indexers.tokens.max_length = None
2021-11-16 14:57:20.732 dataset_reader.text_token_indexers.tokens.tokenizer_kwargs = None
2021-11-16 14:57:20.734 dataset_reader.to_index = 6
2021-11-16 14:57:20.734 type = from_instances
2021-11-16 14:57:20.734 Loading token dictionary from /tmp/tmpk7mrr67c/vocabulary.
2021-11-16 14:57:20.735 model.type = sentence_level_classifier
2021-11-16 14:57:20.736 model.embedder.type = ref
2021-11-16 14:57:20.739 model.embedder.type = basic
2021-11-16 14:57:20.739 model.embedder.token_embedders.type = ref
2021-11-16 14:57:20.741 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-16 14:57:20.742 model.embedder.token_embedders.tokens.type = pretrained_transformer
2021-11-16 14:57:20.742 model.embedder.token_embedders.tokens.model_name = bert-base-cased
2021-11-16 14:57:20.743 type = bert-base-cased
2021-11-16 14:57:20.743 model.embedder.token_embedders.tokens.max_length = None
2021-11-16 14:57:20.743 model.embedder.token_embedders.tokens.sub_module = None
2021-11-16 14:57:20.743 model.embedder.token_embedders.tokens.train_parameters = True
2021-11-16 14:57:20.743 model.embedder.token_embedders.tokens.eval_mode = False
2021-11-16 14:57:20.744 model.embedder.token_embedders.tokens.last_layer_only = True
2021-11-16 14:57:20.744 model.embedder.token_embedders.tokens.override_weights_file = None
2021-11-16 14:57:20.744 model.embedder.token_embedders.tokens.override_weights_strip_prefix = None
2021-11-16 14:57:20.744 model.embedder.token_embedders.tokens.load_weights = True
2021-11-16 14:57:20.744 model.embedder.token_embedders.tokens.gradient_checkpointing = None
2021-11-16 14:57:20.744 model.embedder.token_embedders.tokens.tokenizer_kwargs = None
2021-11-16 14:57:20.745 model.embedder.token_embedders.tokens.transformer_kwargs = None
2021-11-16 14:57:20.794 model.encoder.type = bert_pooler
2021-11-16 14:57:20.794 model.encoder.type = bert_pooler
2021-11-16 14:57:20.795 model.encoder.pretrained_model = bert-base-cased
2021-11-16 14:57:20.795 type = bert-base-cased
2021-11-16 14:57:20.795 model.encoder.override_weights_file = None
2021-11-16 14:57:20.795 model.encoder.override_weights_strip_prefix = None
2021-11-16 14:57:20.796 model.encoder.load_weights = True
2021-11-16 14:57:20.796 model.encoder.requires_grad = True
2021-11-16 14:57:20.796 model.encoder.dropout = 0.0
2021-11-16 14:57:20.796 model.encoder.transformer_kwargs = None
2021-11-16 14:57:21.002 removing temporary unarchived model dir at /tmp/tmpk7mrr67c
